{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_5_RNN_Part1_Srinath.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9b1ec548",
        "0c34550c",
        "c715ac7b",
        "Fc9OZbtI7oPF",
        "NUD_v_L21y2A",
        "gWJ6KpLz60sM",
        "7PH1icKQ8fmG",
        "jnxcVXiV7EZy",
        "k7TIRqNmFSXS",
        "n0RPyzhHW4I-",
        "PTCrk77NoxTN",
        "g6t9f4Ve7ejM",
        "c7MTBwdjMp3Y",
        "fiTvrH56GhH5",
        "VYP3UFTgN5gq",
        "P2o29F7hN7mI",
        "yQgyOYrorhSn",
        "MJADKoSYw3LY",
        "ziUGUkYSXeGH",
        "SRMF4TOko9wg",
        "MQklZAuL708I"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mannam95/Deep_Learning_Programming/blob/main/Assignment5/Assignment_5_RNN_Part1_Srinath.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6aef94d"
      },
      "source": [
        "# Team Assignment\n",
        "\n",
        "\n",
        "1.   Srinath Mannam (229750)\n",
        "2.   Meghana Rao (234907)\n",
        "3.   Govind Shukla (235192)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b1ec548"
      },
      "source": [
        "# import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85eb7211"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numba import cuda\n",
        "import sys\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "import pydot\n",
        "import graphviz\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from datetime import datetime\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c34550c"
      },
      "source": [
        "# Change the directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94b2626e"
      },
      "source": [
        "working_directory = '/content/drive/My Drive/Colab Notebooks/OVGU/Deep_Learning/05_Assignment'\n",
        "def colabDrive():\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "    if os.getcwd() !=  working_directory:\n",
        "      os.chdir(working_directory)\n",
        "    print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8540ed4"
      },
      "source": [
        "#colabDrive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c715ac7b"
      },
      "source": [
        "# Clears GPU memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "591268b2"
      },
      "source": [
        "def clearGPUMemory():\n",
        "    from numba import cuda \n",
        "    device = cuda.get_current_device()\n",
        "    device.reset()\n",
        "    !nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "caee9b53"
      },
      "source": [
        "#clearGPUMemory()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6mjm447GMHX"
      },
      "source": [
        "# Prepare the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dv0CvhD7Yga"
      },
      "source": [
        "## Load the dataset and remove infrequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVE7aMvC4iXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27f0bb8-aa47-49c0-d59c-392739fcc22b"
      },
      "source": [
        "num_words = 40000\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=num_words)\n",
        "\n",
        "print('Loaded dataset with {} training samples, {} test samples'.format(len(train_sequences), len(test_sequences)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "Loaded dataset with 25000 training samples, 25000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc9OZbtI7oPF"
      },
      "source": [
        "## Have some look at train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c07u7Z7s4opk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5e68a3-5416-4bc2-b275-48431ff46105"
      },
      "source": [
        "# look at some sequences. words have been replaced with arbitrary index mappings\n",
        "# 1 is a special \"beginning of sequence\" marker\n",
        "# infrequent words have been replaced by the index 2\n",
        "# actual words start with index 4, 3 is never used (???)\n",
        "train_sequences[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFCZvjGjuy1v",
        "outputId": "3da0c7d4-6df0-4625-e4f1-deb365e51aed"
      },
      "source": [
        "print('---review---')\n",
        "print(train_sequences[6])\n",
        "print('---label---')\n",
        "print(train_labels[6])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---review---\n",
            "[1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 10626, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 16393, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 32677, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 26441, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 33740, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
            "---label---\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUD_v_L21y2A"
      },
      "source": [
        "## to restore words, load the word-to-index mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYX6F3AX5hpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc0f3b2-8f02-49b2-9514-00dc4fe55865"
      },
      "source": [
        "word2id = tf.keras.datasets.imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "print('---review with words---')\n",
        "print([id2word.get(i, ' ') for i in train_sequences[6]])\n",
        "print('---label---')\n",
        "print(train_sequences[6])\n",
        "print('---label---')\n",
        "print(train_labels[6])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "---review with words---\n",
            "['the', 'boiled', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'murdering', 'naschy', 'br', 'villain', 'council', 'suggestion', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'echoed', 'concentrates', 'concept', 'issue', 'skeptical', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'rocketed', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', \"captain's\", 'starship', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'originals', 'things', 'is', 'far', 'this', 'make', 'mistakes', \"kevin's\", 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'dose', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n",
            "---label---\n",
            "[1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 10626, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 16393, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 32677, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 26441, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 33740, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
            "---label---\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWJ6KpLz60sM"
      },
      "source": [
        "## Maximum review length and minimum review length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6LZSJS_62Ye",
        "outputId": "6db4abf1-17dd-4438-ee6e-a1ad1b8b1672"
      },
      "source": [
        "print('Maximum review length: {}'.format(len(max((train_sequences + test_sequences), key=len))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum review length: 2697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XMcFfgQ7H-O",
        "outputId": "bc43841f-9656-4622-8450-e1792b1b0b8d"
      },
      "source": [
        "print('Minimum review length: {}'.format(len(min((train_sequences + test_sequences), key=len))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum review length: 70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PH1icKQ8fmG"
      },
      "source": [
        "## Overview over the sequence length in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpWf-SfA8d0d",
        "outputId": "0012f020-ea65-4dd7-a168-31fc1cf0db87"
      },
      "source": [
        "sequence_lengths = [len(sequence) for sequence in train_sequences]\n",
        "max_len = max(sequence_lengths)\n",
        "max_len\n",
        "\n",
        "plt.hist(sequence_lengths, bins=80)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnxcVXiV7EZy"
      },
      "source": [
        "## Pad Sequences  to some length because the dataset should be in rectangular for feeding to tensors\n",
        "\n",
        "*  all sequences above maxlen will be truncated to that length\n",
        "*  note: pad_sequences has \"pre\" and \"post\" options for both padding and truncation. one may be better than the other!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDFYpy4Q8LH9"
      },
      "source": [
        "max_words = 500\n",
        "train_sequences = sequence.pad_sequences(train_sequences, maxlen=max_words, padding = \"post\", truncating = \"pre\")\n",
        "test_sequences = sequence.pad_sequences(test_sequences, maxlen=max_words, padding = \"post\", truncating = \"pre\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sxPAMPF9GRf"
      },
      "source": [
        "## Load the dataset into tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1YiFTf89KzS"
      },
      "source": [
        "train_labels = train_labels.reshape(-1).astype(np.int32)\n",
        "test_labels = test_labels.reshape(-1).astype(np.int32)\n",
        "# train_data = tf.data.Dataset.from_tensor_slices((train_sequences, train_labels))\n",
        "# test_data = tf.data.Dataset.from_tensor_slices((test_sequences, test_labels))\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences, train_labels)).shuffle(25000).batch(128, drop_remainder=True)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sequences, test_labels)).batch(128, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epD2fyBM-CKA",
        "outputId": "6eb5a6f4-65d8-4e5f-d172-008a3551786a"
      },
      "source": [
        "print(\"Training Dataset Size: \",train_sequences.shape)\n",
        "print(\"Test Dataset Size: \",test_sequences.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Size:  (25000, 500)\n",
            "Test Dataset Size:  (25000, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiB5DU5SAKLG"
      },
      "source": [
        "# Models Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7TIRqNmFSXS"
      },
      "source": [
        "## Model1 Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12k45HYkAJY8"
      },
      "source": [
        "modelConfig1 = {\n",
        "    \"epochs\" : 10,\n",
        "    \"learning_rate\" : 0.1,\n",
        "\n",
        "    ##Input to hidden(U)\n",
        "    \"W_Inp_Hid\":  tf.Variable(tf.random.uniform([20000, 64], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "    ##Initial Previous hidden(ht-1)\n",
        "    \"Init_Prev_Hidden\":  tf.Variable(np.zeros([1,64], dtype=np.float32)),\n",
        "    ##Hidden to hidden(W)\n",
        "    \"W_Hid_Hid\":  tf.Variable(tf.random.uniform([64, 64], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "    ##bias(b)\n",
        "    \"b\": tf.Variable(np.zeros((1, 64), dtype=np.float32)),\n",
        "    ##Hidden to hidden(W)\n",
        "    \"W_Hid_Out\":  tf.Variable(tf.random.uniform([64, 2], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "    ##bias(c)\n",
        "    \"c\": tf.Variable(np.zeros((128, 2), dtype=np.float32))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0RPyzhHW4I-"
      },
      "source": [
        "## Model2 Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ljDP3y7W4JV"
      },
      "source": [
        "modelConfig2 = {\n",
        "    \"epochs\" : 40,\n",
        "    \"learning_rate\" : 0.001,\n",
        "\n",
        "    ##Input to hidden(U)\n",
        "    \"W_Inp_Hid\":  tf.Variable(tf.random.uniform([num_words, 128], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "    ##Initial Previous hidden(ht-1)\n",
        "    \"Init_Prev_Hidden\":  tf.Variable(np.zeros([1,128], dtype=np.float32)),\n",
        "    ##Hidden to hidden(W)\n",
        "    \"W_Hid_Hid\":  tf.Variable(tf.random.uniform([128, 128], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "    ##bias(b)\n",
        "    \"b\": tf.Variable(np.zeros((1, 128), dtype=np.float32)),\n",
        "    ##Hidden to hidden(V)\n",
        "    \"W_Hid_Out\":  tf.Variable(tf.random.uniform([128, 2], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "    ##bias(c)\n",
        "    \"c\": tf.Variable(np.zeros((128, 2), dtype=np.float32))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTCrk77NoxTN"
      },
      "source": [
        "## Model3 Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as2q_giUoxTW"
      },
      "source": [
        "modelConfig3 = {\n",
        "    \"epochs\" : 20,\n",
        "    \"learning_rate\" : 0.001,\n",
        "\n",
        "    ##Input to hidden(U)\n",
        "    \"W_Inp_Hid\":  tf.Variable(tf.random.uniform([num_words, 128], minval=-0.2, maxval=0.2, dtype=np.float32)),\n",
        "    ##Initial Previous hidden(ht-1)\n",
        "    \"Init_Prev_Hidden\":  tf.Variable(np.zeros([1,128], dtype=np.float32)),\n",
        "    ##Hidden to hidden(W)\n",
        "    \"W_Hid_Hid\":  tf.Variable(tf.random.uniform([128, 128], minval=-0.2, maxval=0.2, dtype=np.float32)),\n",
        "    ##bias(b)\n",
        "    \"b\": tf.Variable(np.zeros((1, 128), dtype=np.float32)),\n",
        "    ##Hidden to hidden(V)\n",
        "    \"W_Hid_Out\":  tf.Variable(tf.random.uniform([128, 2], minval=-0.2, maxval=0.2, dtype=np.float32)),\n",
        "    ##bias(c)\n",
        "    \"c\": tf.Variable(np.zeros((128, 2), dtype=np.float32))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6t9f4Ve7ejM"
      },
      "source": [
        "## Model4 Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcfnX68w7ejT"
      },
      "source": [
        "modelConfig4 = {\n",
        "    \"epochs\" : 100,\n",
        "    \"learning_rate\" : 0.001,\n",
        "\n",
        "    ##Input to hidden(U)\n",
        "    \"W_Inp_Hid\":  tf.Variable(tf.random.uniform([num_words, 128], minval=-0.25, maxval=0.25, dtype=np.float32)),\n",
        "    ##Initial Previous hidden(ht-1)\n",
        "    \"Init_Prev_Hidden\":  tf.Variable(np.zeros([1,128], dtype=np.float32)),\n",
        "    ##Hidden to hidden(W)\n",
        "    \"W_Hid_Hid\":  tf.Variable(tf.random.uniform([128, 128], minval=-0.25, maxval=0.25, dtype=np.float32)),\n",
        "    ##bias(b)\n",
        "    \"b\": tf.Variable(np.zeros((1, 128), dtype=np.float32)),\n",
        "    ##Hidden to hidden(V)\n",
        "    \"W_Hid_Out\":  tf.Variable(tf.random.uniform([128, 2], minval=-0.25, maxval=0.25, dtype=np.float32)),\n",
        "    ##bias(c)\n",
        "    \"c\": tf.Variable(np.zeros((128, 2), dtype=np.float32))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7MTBwdjMp3Y"
      },
      "source": [
        "## Model5 Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f05e_IhhMp3Z"
      },
      "source": [
        "modelConfig5 = {\n",
        "    \"epochs\" : 20,\n",
        "    \"learning_rate\" : 0.001,\n",
        "\n",
        "    ##Input to hidden(U)\n",
        "    \"W_Inp_Hid\":  tf.Variable(tf.random.uniform([num_words, 128], minval=-0.2, maxval=0.2, dtype=np.float32)),\n",
        "    ##Initial Previous hidden(ht-1)\n",
        "    \"Init_Prev_Hidden\":  tf.Variable(np.zeros([1,128], dtype=np.float32)),\n",
        "    ##Hidden to hidden(W)\n",
        "    \"W_Hid_Hid\":  tf.Variable(tf.random.uniform([128, 128], minval=-0.2, maxval=0.2, dtype=np.float32)),\n",
        "    ##bias(b)\n",
        "    \"b\": tf.Variable(np.zeros((1, 128), dtype=np.float32)),\n",
        "    ##Hidden to hidden(V)\n",
        "    \"W_Hid_Out\":  tf.Variable(tf.random.uniform([128, 2], minval=-0.2, maxval=0.2, dtype=np.float32)),\n",
        "    ##bias(c)\n",
        "    \"c\": tf.Variable(np.zeros((128, 2), dtype=np.float32))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiTvrH56GhH5"
      },
      "source": [
        "# Generic Function for training, testing\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYP3UFTgN5gq"
      },
      "source": [
        "## RNN Loop Cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JheLUJCjN4Eb"
      },
      "source": [
        "def rnn_loop(sequences, modelConfig):\n",
        "    old_state = modelConfig['Init_Prev_Hidden']\n",
        "    #seq_onehot = tf.one_hot(sequences, depth=num_words)\n",
        "\n",
        "    for step in range(max_words):\n",
        "        #x_t = seq_onehot[:,step]\n",
        "        x_t = sequences[:, step]\n",
        "        x_t = tf.one_hot(x_t, depth=num_words)\n",
        "        at = modelConfig['b'] + tf.matmul(old_state, modelConfig['W_Hid_Hid']) + tf.matmul(x_t, modelConfig['W_Inp_Hid'])\n",
        "        new_state = tf.nn.tanh(at)\n",
        "        old_state = new_state\n",
        "\n",
        "    o_t = modelConfig['c'] + tf.matmul(new_state, modelConfig['W_Hid_Out'])\n",
        "\n",
        "    return o_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2o29F7hN7mI"
      },
      "source": [
        "## Training and Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEw-Xlx3_OsU"
      },
      "source": [
        "def train_loop(modelConfig):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=modelConfig['learning_rate'])\n",
        "    for epoch in range(modelConfig['epochs']):\n",
        "      print(\"Epoch No: \", epoch)\n",
        "      for step, (sequence_batch, label_batch) in enumerate(train_data):\n",
        "          # label_batch = tf.reshape(label_batch, [1])\n",
        "          with tf.GradientTape() as tape:\n",
        "              logits = rnn_loop(sequence_batch, modelConfig)\n",
        "              xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                  logits=logits, labels=label_batch))\n",
        "\n",
        "          ## Gradients calculation and update weights\n",
        "          grads = tape.gradient(xent, [modelConfig['W_Inp_Hid'], modelConfig['W_Hid_Hid'], modelConfig['b'], modelConfig['W_Hid_Out'], modelConfig['c']])\n",
        "          optimizer.apply_gradients(zip(grads, [modelConfig['W_Inp_Hid'], modelConfig['W_Hid_Hid'], modelConfig['b'], modelConfig['W_Hid_Out'], modelConfig['c']]))\n",
        "\n",
        "\n",
        "          if not step % 50:\n",
        "              print(\"Steps Completed: \", step)\n",
        "              preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "              acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch), tf.float32))\n",
        "              print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "          \n",
        "      print(\"-------------\\n\")\n",
        "\n",
        "    #Testing\n",
        "    print(\"Working on test dataset\")\n",
        "    testAcc = []\n",
        "    for step, (sequence_batch, label_batch) in enumerate(test_data):\n",
        "        if not step % 50:\n",
        "          print(\"Steps Completed: \", step)\n",
        "        logits = rnn_loop(sequence_batch, modelConfig)\n",
        "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch), tf.float32))\n",
        "        testAcc.append(acc)\n",
        "\n",
        "    print(\"Final Test Accuracy Average: {} \".format(sum(testAcc)/len(testAcc)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQgyOYrorhSn"
      },
      "source": [
        "# Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNdg7FvCrlzm"
      },
      "source": [
        "## Q1\n",
        "\n",
        "Question:\n",
        "\n",
        "\n",
        "\n",
        "*   In the notebook, this is done in a rather crude way: All sequences are padded to the length of the longest sequence in the dataset.\n",
        "*   Why is this wasteful? Can you think of a smarter padding scheme that is more efficient? Consider the fact that RNNs can work on arbitrary sequence lengths, and that training minibatches are pretty much independent of each other\n",
        "\n",
        "Answer: \n",
        "\n",
        "\n",
        "\n",
        "*   Because as per the histogram plot in the above plots, we can see that most of the sequence lengths are between 0-500 and if we pad to the max length then most of the neurons are initialised with dummy values which is wasteful for computation.\n",
        "*   Alternative way is to pad at batch level by taking the average length of sequences in that particular batch or truncate all sequences to some length like 500 and pad.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kc9VYx7s-99"
      },
      "source": [
        "## Q2\n",
        "\n",
        "Question: Between truncating long sequences and removing them, which option do you think is better? Why?\n",
        "\n",
        "Answer: Post trunctaing could be better, but here tried different configs with both pre and post truncating but the results are same.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqWBsWgwtSOW"
      },
      "source": [
        "## Q3\n",
        "\n",
        "Question: Can you think of a way to avoid the one-hot vectors completely? Even if you cannot implement it, a conceptual idea is fine.\n",
        "\n",
        "Answer: Embeddings.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2SIjpvvtggm"
      },
      "source": [
        "## Q4\n",
        "\n",
        "Question: How can it be that we can choose how many outputs we have, i.e. how can both be correct? Are there differences between both choices as well as (dis)advantages relative to each other?\n",
        "\n",
        "Answer: \n",
        "\n",
        "\n",
        "\n",
        "*   Use softmax if the output neurons size is 2 and whichever wins will be the target\n",
        "*   Use sigmoid if the output neuron size is 1 and get a value, based on the value classify it as 0 or 1 ( by threshold of 0.5)\n",
        "*   Both almost gave same results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NQi7YVAuRka"
      },
      "source": [
        "## Q5\n",
        "\n",
        "Question: All sequences start with the same special “beginning of sequence” token (coded by index 1). Given this fact, is there a point in learning an initial state? Why (not)?\n",
        "\n",
        "Answer: Same information might be travelled from the initial state to the next states for every sequence which might not be helpful for classification purpose.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95iiW9ZquR5w"
      },
      "source": [
        "## Q6\n",
        "\n",
        "Question: pad_sequences allows for pre or post padding. Try both to see the difference. Which option do you think is better? Recall that we use the final time step output from our model.\n",
        "\n",
        "Answer: By Naive assumption post padding is correct as you might start by not learning anything at beginning if it is pre-pad, but in the different configs performed there is no much difference w.r.t accuracy.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPCEnYY5uSLU"
      },
      "source": [
        "## Q7\n",
        "\n",
        "Question: Can you think of a way to prevent the RNN from computing new states on padded time steps? One idea might be to “pass through” the previous state in case the current time step is padding. Note that, within a batch, some sequences might be padded for a given time step while others are not.\n",
        "\n",
        "Answer: \n",
        "\n",
        "\n",
        "\n",
        "*   May be instead of setting max lenth sequence at global level for the entire dataset. We can set the max length of the sequence at batch level since RNN can accept different length of time steps. By taking the average length in the current batch sequences and then apply truncation, padding followed by. But here we need to compute the tensors at batch level which will be time consuming.\n",
        "*   The above description might not prevent padding at all, we can go by sequence by sequence then no need to pad. Now here it is too much time consuming.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qar5fh1qvWCr"
      },
      "source": [
        "## Q8\n",
        "\n",
        "Question: What could be the advantage of using methods like the above? What are disadvantages? Can you think of other methods to incorporate the full output sequence instead of just the final step?\n",
        "\n",
        "Answer: We can have output units at each time step and before sending it to the loss function take average of all the output units and send them.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8NysXi27_Gg"
      },
      "source": [
        "# Different Models Running\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJADKoSYw3LY"
      },
      "source": [
        "## Model1\n",
        "\n",
        "1.   With 10 Epochs\n",
        "2.   The test accuracy is 49.6%\n",
        "3.   Batch Size of 128\n",
        "4.   Hidden layer neurons are 64\n",
        "5.   Max corpus length 20000\n",
        "6.   Max words 300, padding post, truncating post"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH36czdew3Lf",
        "outputId": "09df6c38-70b4-4217-b64b-b89126896a38"
      },
      "source": [
        "train_loop(modelConfig1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch No:  0\n",
            "Steps Completed:  0\n",
            "Loss: 0.696199357509613 Accuracy: 0.4609375\n",
            "Steps Completed:  50\n",
            "Loss: 0.9726694822311401 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.7283741235733032 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.746300995349884 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  1\n",
            "Steps Completed:  0\n",
            "Loss: 0.8708627223968506 Accuracy: 0.5703125\n",
            "Steps Completed:  50\n",
            "Loss: 0.7043036818504333 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.8352829217910767 Accuracy: 0.484375\n",
            "Steps Completed:  150\n",
            "Loss: 0.7988328337669373 Accuracy: 0.5390625\n",
            "-------------\n",
            "\n",
            "Epoch No:  2\n",
            "Steps Completed:  0\n",
            "Loss: 0.6984706521034241 Accuracy: 0.5859375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6942040920257568 Accuracy: 0.59375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6530992388725281 Accuracy: 0.609375\n",
            "Steps Completed:  150\n",
            "Loss: 0.8190151453018188 Accuracy: 0.46875\n",
            "-------------\n",
            "\n",
            "Epoch No:  3\n",
            "Steps Completed:  0\n",
            "Loss: 0.7579739093780518 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.7478058934211731 Accuracy: 0.4765625\n",
            "Steps Completed:  100\n",
            "Loss: 0.7769534587860107 Accuracy: 0.4765625\n",
            "Steps Completed:  150\n",
            "Loss: 0.7120305299758911 Accuracy: 0.4921875\n",
            "-------------\n",
            "\n",
            "Epoch No:  4\n",
            "Steps Completed:  0\n",
            "Loss: 0.8354402780532837 Accuracy: 0.4375\n",
            "Steps Completed:  50\n",
            "Loss: 0.7821717262268066 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.7646347284317017 Accuracy: 0.5\n",
            "Steps Completed:  150\n",
            "Loss: 0.7192972302436829 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  5\n",
            "Steps Completed:  0\n",
            "Loss: 0.7207278609275818 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.7831155061721802 Accuracy: 0.4609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.883648693561554 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7884370684623718 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  6\n",
            "Steps Completed:  0\n",
            "Loss: 0.7322415113449097 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.7204431891441345 Accuracy: 0.6015625\n",
            "Steps Completed:  100\n",
            "Loss: 0.7217613458633423 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7292568683624268 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  7\n",
            "Steps Completed:  0\n",
            "Loss: 0.7257785797119141 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.7618244290351868 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.7485902905464172 Accuracy: 0.421875\n",
            "Steps Completed:  150\n",
            "Loss: 0.7179251909255981 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  8\n",
            "Steps Completed:  0\n",
            "Loss: 0.7179141640663147 Accuracy: 0.453125\n",
            "Steps Completed:  50\n",
            "Loss: 0.8544270992279053 Accuracy: 0.4609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.9200974702835083 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7902131080627441 Accuracy: 0.5078125\n",
            "-------------\n",
            "\n",
            "Epoch No:  9\n",
            "Steps Completed:  0\n",
            "Loss: 0.8069676756858826 Accuracy: 0.5703125\n",
            "Steps Completed:  50\n",
            "Loss: 0.8043767213821411 Accuracy: 0.4609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.7284603118896484 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.7533402442932129 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Working on test dataset\n",
            "Steps Completed:  0\n",
            "Steps Completed:  50\n",
            "Steps Completed:  100\n",
            "Steps Completed:  150\n",
            "Final Test Accuracy Average: 0.49655449390411377 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziUGUkYSXeGH"
      },
      "source": [
        "## Model2\n",
        "\n",
        "1.   With 40 Epochs\n",
        "2.   The test accuracy is 50.6%\n",
        "3.   Batch Size of 128\n",
        "4.   Hidden layer neurons are 128\n",
        "5.   Max corpus length 40000\n",
        "6.   Max words 500, padding post, truncating pre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qkM4eVXXeGb",
        "outputId": "03498d2c-716c-4c19-c28a-f671949fd86a"
      },
      "source": [
        "train_loop(modelConfig2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch No:  0\n",
            "Steps Completed:  0\n",
            "Loss: 0.6928707957267761 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6919717788696289 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6927862763404846 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6956644058227539 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  1\n",
            "Steps Completed:  0\n",
            "Loss: 0.6871331334114075 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6924787759780884 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6678862571716309 Accuracy: 0.578125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6944794654846191 Accuracy: 0.5078125\n",
            "-------------\n",
            "\n",
            "Epoch No:  2\n",
            "Steps Completed:  0\n",
            "Loss: 0.6982626914978027 Accuracy: 0.453125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6907624006271362 Accuracy: 0.6015625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6890923976898193 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.681374192237854 Accuracy: 0.59375\n",
            "-------------\n",
            "\n",
            "Epoch No:  3\n",
            "Steps Completed:  0\n",
            "Loss: 0.6838294267654419 Accuracy: 0.4921875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6668428778648376 Accuracy: 0.59375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6884939074516296 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6923825740814209 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  4\n",
            "Steps Completed:  0\n",
            "Loss: 0.6853148937225342 Accuracy: 0.546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6748105883598328 Accuracy: 0.5859375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6739037036895752 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6957507729530334 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  5\n",
            "Steps Completed:  0\n",
            "Loss: 0.685388445854187 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6888481378555298 Accuracy: 0.4921875\n",
            "Steps Completed:  100\n",
            "Loss: 0.7017629146575928 Accuracy: 0.4296875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6903291344642639 Accuracy: 0.5234375\n",
            "-------------\n",
            "\n",
            "Epoch No:  6\n",
            "Steps Completed:  0\n",
            "Loss: 0.6853963732719421 Accuracy: 0.6328125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6989051699638367 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6801783442497253 Accuracy: 0.5703125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7012248039245605 Accuracy: 0.453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  7\n",
            "Steps Completed:  0\n",
            "Loss: 0.6868335008621216 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6699327230453491 Accuracy: 0.6015625\n",
            "Steps Completed:  100\n",
            "Loss: 0.684109091758728 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7213274240493774 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  8\n",
            "Steps Completed:  0\n",
            "Loss: 0.6706337332725525 Accuracy: 0.5859375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6822420358657837 Accuracy: 0.546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6683303117752075 Accuracy: 0.5703125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6870102286338806 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  9\n",
            "Steps Completed:  0\n",
            "Loss: 0.6819988489151001 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6813089847564697 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6834003925323486 Accuracy: 0.484375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6858251094818115 Accuracy: 0.5390625\n",
            "-------------\n",
            "\n",
            "Epoch No:  10\n",
            "Steps Completed:  0\n",
            "Loss: 0.6824917197227478 Accuracy: 0.546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6784583330154419 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.7553969621658325 Accuracy: 0.4453125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6870009303092957 Accuracy: 0.59375\n",
            "-------------\n",
            "\n",
            "Epoch No:  11\n",
            "Steps Completed:  0\n",
            "Loss: 0.677443265914917 Accuracy: 0.5078125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6986886262893677 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6865496039390564 Accuracy: 0.578125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6931723356246948 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  12\n",
            "Steps Completed:  0\n",
            "Loss: 0.7095809578895569 Accuracy: 0.4375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6887379288673401 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.684390664100647 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.7124885320663452 Accuracy: 0.421875\n",
            "-------------\n",
            "\n",
            "Epoch No:  13\n",
            "Steps Completed:  0\n",
            "Loss: 0.6898264288902283 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.69621741771698 Accuracy: 0.4765625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6994284987449646 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6946438550949097 Accuracy: 0.4609375\n",
            "-------------\n",
            "\n",
            "Epoch No:  14\n",
            "Steps Completed:  0\n",
            "Loss: 0.6826863288879395 Accuracy: 0.6015625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6906579732894897 Accuracy: 0.5625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6928120851516724 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6931532621383667 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  15\n",
            "Steps Completed:  0\n",
            "Loss: 0.6961991786956787 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.6867450475692749 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6894733905792236 Accuracy: 0.5234375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6946952939033508 Accuracy: 0.4453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  16\n",
            "Steps Completed:  0\n",
            "Loss: 0.6948097944259644 Accuracy: 0.5078125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6832908391952515 Accuracy: 0.5703125\n",
            "Steps Completed:  100\n",
            "Loss: 0.689587414264679 Accuracy: 0.546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6864959597587585 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  17\n",
            "Steps Completed:  0\n",
            "Loss: 0.691623330116272 Accuracy: 0.546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6845155954360962 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6868915557861328 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6948813199996948 Accuracy: 0.5390625\n",
            "-------------\n",
            "\n",
            "Epoch No:  18\n",
            "Steps Completed:  0\n",
            "Loss: 0.6784287095069885 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6913138628005981 Accuracy: 0.46875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6802389621734619 Accuracy: 0.5625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6977211236953735 Accuracy: 0.4375\n",
            "-------------\n",
            "\n",
            "Epoch No:  19\n",
            "Steps Completed:  0\n",
            "Loss: 0.6905947923660278 Accuracy: 0.4765625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6790931224822998 Accuracy: 0.5625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6726071834564209 Accuracy: 0.5546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6818754076957703 Accuracy: 0.46875\n",
            "-------------\n",
            "\n",
            "Epoch No:  20\n",
            "Steps Completed:  0\n",
            "Loss: 0.6729390621185303 Accuracy: 0.5625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6875730752944946 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6913844347000122 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7047351598739624 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  21\n",
            "Steps Completed:  0\n",
            "Loss: 0.7465717792510986 Accuracy: 0.46875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6931878328323364 Accuracy: 0.4453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6960141062736511 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6788547039031982 Accuracy: 0.5234375\n",
            "-------------\n",
            "\n",
            "Epoch No:  22\n",
            "Steps Completed:  0\n",
            "Loss: 0.6835840940475464 Accuracy: 0.4765625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6918814182281494 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.674383282661438 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6833890676498413 Accuracy: 0.5546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  23\n",
            "Steps Completed:  0\n",
            "Loss: 0.6829054951667786 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6787573099136353 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6659526228904724 Accuracy: 0.5703125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6814069747924805 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  24\n",
            "Steps Completed:  0\n",
            "Loss: 0.6754723787307739 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6624303460121155 Accuracy: 0.578125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6658598780632019 Accuracy: 0.6015625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6677289605140686 Accuracy: 0.5703125\n",
            "-------------\n",
            "\n",
            "Epoch No:  25\n",
            "Steps Completed:  0\n",
            "Loss: 0.6835052967071533 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.68401038646698 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6464418768882751 Accuracy: 0.5703125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6591694355010986 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  26\n",
            "Steps Completed:  0\n",
            "Loss: 0.6885026097297668 Accuracy: 0.546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6645923256874084 Accuracy: 0.59375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6904908418655396 Accuracy: 0.4765625\n",
            "Steps Completed:  150\n",
            "Loss: 0.7065625190734863 Accuracy: 0.4296875\n",
            "-------------\n",
            "\n",
            "Epoch No:  27\n",
            "Steps Completed:  0\n",
            "Loss: 0.6667781472206116 Accuracy: 0.5078125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6751317977905273 Accuracy: 0.546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6979342699050903 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6604052782058716 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  28\n",
            "Steps Completed:  0\n",
            "Loss: 0.6712331771850586 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6647506356239319 Accuracy: 0.546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.649803638458252 Accuracy: 0.5703125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6438575387001038 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  29\n",
            "Steps Completed:  0\n",
            "Loss: 0.6617770791053772 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6736299991607666 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6465813517570496 Accuracy: 0.5625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6420191526412964 Accuracy: 0.578125\n",
            "-------------\n",
            "\n",
            "Epoch No:  30\n",
            "Steps Completed:  0\n",
            "Loss: 0.6594953536987305 Accuracy: 0.4765625\n",
            "Steps Completed:  50\n",
            "Loss: 0.7210909724235535 Accuracy: 0.4921875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6681708097457886 Accuracy: 0.59375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6662375330924988 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  31\n",
            "Steps Completed:  0\n",
            "Loss: 0.679766833782196 Accuracy: 0.4453125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6584535837173462 Accuracy: 0.46875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6443846821784973 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6711093187332153 Accuracy: 0.4453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  32\n",
            "Steps Completed:  0\n",
            "Loss: 0.647388756275177 Accuracy: 0.5859375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6239281296730042 Accuracy: 0.640625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6755654215812683 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6697190999984741 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  33\n",
            "Steps Completed:  0\n",
            "Loss: 0.6635428071022034 Accuracy: 0.4296875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6818914413452148 Accuracy: 0.4765625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6319710612297058 Accuracy: 0.5859375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6669565439224243 Accuracy: 0.5625\n",
            "-------------\n",
            "\n",
            "Epoch No:  34\n",
            "Steps Completed:  0\n",
            "Loss: 0.6625196933746338 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6722698211669922 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6689714193344116 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.638032078742981 Accuracy: 0.6015625\n",
            "-------------\n",
            "\n",
            "Epoch No:  35\n",
            "Steps Completed:  0\n",
            "Loss: 0.6636316776275635 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6827977895736694 Accuracy: 0.421875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6475929617881775 Accuracy: 0.59375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6807490587234497 Accuracy: 0.453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  36\n",
            "Steps Completed:  0\n",
            "Loss: 0.651878833770752 Accuracy: 0.5625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6550564765930176 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6342585682868958 Accuracy: 0.546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6570444107055664 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  37\n",
            "Steps Completed:  0\n",
            "Loss: 0.6281876564025879 Accuracy: 0.6015625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6329957246780396 Accuracy: 0.609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6584104299545288 Accuracy: 0.609375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6473525762557983 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  38\n",
            "Steps Completed:  0\n",
            "Loss: 0.6493752002716064 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6487276554107666 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6538717746734619 Accuracy: 0.5859375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6758322715759277 Accuracy: 0.421875\n",
            "-------------\n",
            "\n",
            "Epoch No:  39\n",
            "Steps Completed:  0\n",
            "Loss: 0.6392800807952881 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6439899802207947 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6158491373062134 Accuracy: 0.578125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6568650603294373 Accuracy: 0.578125\n",
            "-------------\n",
            "\n",
            "Working on test dataset\n",
            "Steps Completed:  0\n",
            "Steps Completed:  50\n",
            "Steps Completed:  100\n",
            "Steps Completed:  150\n",
            "Final Test Accuracy Average: 0.5069711804389954 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRMF4TOko9wg"
      },
      "source": [
        "## Model3\n",
        "\n",
        "1.   With 20 Epochs\n",
        "2.   The test accuracy is 50.2%\n",
        "3.   Batch Size of 128\n",
        "4.   Hidden layer neurons are 128\n",
        "5.   Max corpus length 40000\n",
        "6.   Max words 500, padding post, truncating pre\n",
        "7.   Weights are initialised from -0.2 to 0.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59-w4jNeo9wq",
        "outputId": "af2174c1-99c0-4028-c988-2bbdc5e0504a"
      },
      "source": [
        "train_loop(modelConfig3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch No:  0\n",
            "Steps Completed:  0\n",
            "Loss: 0.7625614404678345 Accuracy: 0.5078125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6968785524368286 Accuracy: 0.453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6926401853561401 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6837975382804871 Accuracy: 0.625\n",
            "-------------\n",
            "\n",
            "Epoch No:  1\n",
            "Steps Completed:  0\n",
            "Loss: 0.6936752200126648 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6903246641159058 Accuracy: 0.5234375\n",
            "Steps Completed:  100\n",
            "Loss: 0.7023606896400452 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.7186465263366699 Accuracy: 0.4296875\n",
            "-------------\n",
            "\n",
            "Epoch No:  2\n",
            "Steps Completed:  0\n",
            "Loss: 0.6824567914009094 Accuracy: 0.59375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6933882236480713 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6863447427749634 Accuracy: 0.5234375\n",
            "Steps Completed:  150\n",
            "Loss: 0.704568088054657 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  3\n",
            "Steps Completed:  0\n",
            "Loss: 0.6739521026611328 Accuracy: 0.59375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6993414163589478 Accuracy: 0.453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6897538900375366 Accuracy: 0.546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6805823445320129 Accuracy: 0.5390625\n",
            "-------------\n",
            "\n",
            "Epoch No:  4\n",
            "Steps Completed:  0\n",
            "Loss: 0.677291989326477 Accuracy: 0.5703125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6752027273178101 Accuracy: 0.5625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6859322190284729 Accuracy: 0.5390625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6864591240882874 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  5\n",
            "Steps Completed:  0\n",
            "Loss: 0.673235297203064 Accuracy: 0.5859375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6851481199264526 Accuracy: 0.5390625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6957435607910156 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.7103928327560425 Accuracy: 0.4609375\n",
            "-------------\n",
            "\n",
            "Epoch No:  6\n",
            "Steps Completed:  0\n",
            "Loss: 0.7121118903160095 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6705667972564697 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6906747817993164 Accuracy: 0.5546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.675923764705658 Accuracy: 0.6015625\n",
            "-------------\n",
            "\n",
            "Epoch No:  7\n",
            "Steps Completed:  0\n",
            "Loss: 0.6867736577987671 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.680898129940033 Accuracy: 0.5234375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6889313459396362 Accuracy: 0.5234375\n",
            "Steps Completed:  150\n",
            "Loss: 0.676171600818634 Accuracy: 0.59375\n",
            "-------------\n",
            "\n",
            "Epoch No:  8\n",
            "Steps Completed:  0\n",
            "Loss: 0.7025389671325684 Accuracy: 0.4609375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6933951377868652 Accuracy: 0.453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6684936881065369 Accuracy: 0.5390625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6826215982437134 Accuracy: 0.4921875\n",
            "-------------\n",
            "\n",
            "Epoch No:  9\n",
            "Steps Completed:  0\n",
            "Loss: 0.6852145791053772 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.6777186393737793 Accuracy: 0.546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6874328851699829 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6543234586715698 Accuracy: 0.6328125\n",
            "-------------\n",
            "\n",
            "Epoch No:  10\n",
            "Steps Completed:  0\n",
            "Loss: 0.6762974262237549 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6762714385986328 Accuracy: 0.5390625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6954240798950195 Accuracy: 0.484375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6721462607383728 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  11\n",
            "Steps Completed:  0\n",
            "Loss: 0.6870684623718262 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6725794076919556 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6747759580612183 Accuracy: 0.4765625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6917638778686523 Accuracy: 0.4921875\n",
            "-------------\n",
            "\n",
            "Epoch No:  12\n",
            "Steps Completed:  0\n",
            "Loss: 0.675301194190979 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.6682358384132385 Accuracy: 0.4453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6821600198745728 Accuracy: 0.5\n",
            "Steps Completed:  150\n",
            "Loss: 0.6828330159187317 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  13\n",
            "Steps Completed:  0\n",
            "Loss: 0.7015039920806885 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.6782524585723877 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6873793601989746 Accuracy: 0.453125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6748563051223755 Accuracy: 0.5390625\n",
            "-------------\n",
            "\n",
            "Epoch No:  14\n",
            "Steps Completed:  0\n",
            "Loss: 0.6826949119567871 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6759935021400452 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6735442280769348 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6587771773338318 Accuracy: 0.5625\n",
            "-------------\n",
            "\n",
            "Epoch No:  15\n",
            "Steps Completed:  0\n",
            "Loss: 0.672133207321167 Accuracy: 0.5625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6847619414329529 Accuracy: 0.5390625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6804699897766113 Accuracy: 0.5625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6817440390586853 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  16\n",
            "Steps Completed:  0\n",
            "Loss: 0.6844315528869629 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6756361126899719 Accuracy: 0.4609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6991997361183167 Accuracy: 0.4296875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6702701449394226 Accuracy: 0.5390625\n",
            "-------------\n",
            "\n",
            "Epoch No:  17\n",
            "Steps Completed:  0\n",
            "Loss: 0.6631523370742798 Accuracy: 0.609375\n",
            "Steps Completed:  50\n",
            "Loss: 0.659622311592102 Accuracy: 0.5703125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6686543226242065 Accuracy: 0.578125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6638755798339844 Accuracy: 0.59375\n",
            "-------------\n",
            "\n",
            "Epoch No:  18\n",
            "Steps Completed:  0\n",
            "Loss: 0.6886003017425537 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.718749463558197 Accuracy: 0.453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.656849205493927 Accuracy: 0.6171875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6784762144088745 Accuracy: 0.4765625\n",
            "-------------\n",
            "\n",
            "Epoch No:  19\n",
            "Steps Completed:  0\n",
            "Loss: 0.6307874917984009 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6595213413238525 Accuracy: 0.59375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6747157573699951 Accuracy: 0.546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6575079560279846 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Working on test dataset\n",
            "Steps Completed:  0\n",
            "Steps Completed:  50\n",
            "Steps Completed:  100\n",
            "Steps Completed:  150\n",
            "Final Test Accuracy Average: 0.5024038553237915 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQklZAuL708I"
      },
      "source": [
        "## Model4\n",
        "\n",
        "1.   With 100 Epochs\n",
        "2.   The test accuracy is 49.7%\n",
        "3.   Batch Size of 128\n",
        "4.   Hidden layer neurons are 128\n",
        "5.   Max corpus length 40000\n",
        "6.   Max words 500, padding post, truncating pre\n",
        "7.   Weights are initialised from -0.25 to 0.25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7jO7Aag708I",
        "outputId": "c6adb987-69c0-43ff-b88a-23c6b270a674"
      },
      "source": [
        "train_loop(modelConfig4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch No:  0\n",
            "Steps Completed:  0\n",
            "Loss: 0.9379345178604126 Accuracy: 0.5078125\n",
            "Steps Completed:  50\n",
            "Loss: 0.7528995275497437 Accuracy: 0.4765625\n",
            "Steps Completed:  100\n",
            "Loss: 0.7489109039306641 Accuracy: 0.4453125\n",
            "Steps Completed:  150\n",
            "Loss: 0.708798885345459 Accuracy: 0.5078125\n",
            "-------------\n",
            "\n",
            "Epoch No:  1\n",
            "Steps Completed:  0\n",
            "Loss: 0.7285313606262207 Accuracy: 0.4453125\n",
            "Steps Completed:  50\n",
            "Loss: 0.7069002389907837 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.7075903415679932 Accuracy: 0.453125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6950585842132568 Accuracy: 0.5625\n",
            "-------------\n",
            "\n",
            "Epoch No:  2\n",
            "Steps Completed:  0\n",
            "Loss: 0.6881455183029175 Accuracy: 0.5546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.7292505502700806 Accuracy: 0.4609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.7038137316703796 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.7163032293319702 Accuracy: 0.4296875\n",
            "-------------\n",
            "\n",
            "Epoch No:  3\n",
            "Steps Completed:  0\n",
            "Loss: 0.7139977812767029 Accuracy: 0.4453125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6910511255264282 Accuracy: 0.5625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6945555210113525 Accuracy: 0.5234375\n",
            "Steps Completed:  150\n",
            "Loss: 0.7335977554321289 Accuracy: 0.46875\n",
            "-------------\n",
            "\n",
            "Epoch No:  4\n",
            "Steps Completed:  0\n",
            "Loss: 0.6985917687416077 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.7095882892608643 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6968992948532104 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.7216708660125732 Accuracy: 0.40625\n",
            "-------------\n",
            "\n",
            "Epoch No:  5\n",
            "Steps Completed:  0\n",
            "Loss: 0.7016093730926514 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.7026324272155762 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6985684633255005 Accuracy: 0.5234375\n",
            "Steps Completed:  150\n",
            "Loss: 0.7032193541526794 Accuracy: 0.4765625\n",
            "-------------\n",
            "\n",
            "Epoch No:  6\n",
            "Steps Completed:  0\n",
            "Loss: 0.6840882301330566 Accuracy: 0.59375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6906039714813232 Accuracy: 0.515625\n",
            "Steps Completed:  100\n",
            "Loss: 0.7143468856811523 Accuracy: 0.4453125\n",
            "Steps Completed:  150\n",
            "Loss: 0.716762363910675 Accuracy: 0.40625\n",
            "-------------\n",
            "\n",
            "Epoch No:  7\n",
            "Steps Completed:  0\n",
            "Loss: 0.6971864700317383 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6780627965927124 Accuracy: 0.578125\n",
            "Steps Completed:  100\n",
            "Loss: 0.7076585292816162 Accuracy: 0.4609375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6784905195236206 Accuracy: 0.5703125\n",
            "-------------\n",
            "\n",
            "Epoch No:  8\n",
            "Steps Completed:  0\n",
            "Loss: 0.7011947631835938 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.702670156955719 Accuracy: 0.4609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6853640675544739 Accuracy: 0.5390625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6859043836593628 Accuracy: 0.5859375\n",
            "-------------\n",
            "\n",
            "Epoch No:  9\n",
            "Steps Completed:  0\n",
            "Loss: 0.6928945779800415 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.7021245956420898 Accuracy: 0.46875\n",
            "Steps Completed:  100\n",
            "Loss: 0.681656539440155 Accuracy: 0.5546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.7066316604614258 Accuracy: 0.40625\n",
            "-------------\n",
            "\n",
            "Epoch No:  10\n",
            "Steps Completed:  0\n",
            "Loss: 0.6857814788818359 Accuracy: 0.4921875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6868358850479126 Accuracy: 0.5234375\n",
            "Steps Completed:  100\n",
            "Loss: 0.702682375907898 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6975157260894775 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  11\n",
            "Steps Completed:  0\n",
            "Loss: 0.6901827454566956 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6959105730056763 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6735565662384033 Accuracy: 0.6171875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6884457468986511 Accuracy: 0.59375\n",
            "-------------\n",
            "\n",
            "Epoch No:  12\n",
            "Steps Completed:  0\n",
            "Loss: 0.6835297346115112 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6920701861381531 Accuracy: 0.515625\n",
            "Steps Completed:  100\n",
            "Loss: 0.7036477327346802 Accuracy: 0.4609375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6918869018554688 Accuracy: 0.4375\n",
            "-------------\n",
            "\n",
            "Epoch No:  13\n",
            "Steps Completed:  0\n",
            "Loss: 0.6976842880249023 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6991612911224365 Accuracy: 0.5234375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6993080377578735 Accuracy: 0.484375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6981499195098877 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  14\n",
            "Steps Completed:  0\n",
            "Loss: 0.6897212266921997 Accuracy: 0.5078125\n",
            "Steps Completed:  50\n",
            "Loss: 0.682508647441864 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6986027956008911 Accuracy: 0.5390625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6948510408401489 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  15\n",
            "Steps Completed:  0\n",
            "Loss: 0.694309413433075 Accuracy: 0.5859375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6962026357650757 Accuracy: 0.5390625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6907017230987549 Accuracy: 0.5546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6949433088302612 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  16\n",
            "Steps Completed:  0\n",
            "Loss: 0.6802579164505005 Accuracy: 0.5703125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6902791857719421 Accuracy: 0.5234375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6724987626075745 Accuracy: 0.640625\n",
            "Steps Completed:  150\n",
            "Loss: 0.69333416223526 Accuracy: 0.4921875\n",
            "-------------\n",
            "\n",
            "Epoch No:  17\n",
            "Steps Completed:  0\n",
            "Loss: 0.6922481060028076 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6948513984680176 Accuracy: 0.515625\n",
            "Steps Completed:  100\n",
            "Loss: 0.7075827121734619 Accuracy: 0.4453125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6806974411010742 Accuracy: 0.5625\n",
            "-------------\n",
            "\n",
            "Epoch No:  18\n",
            "Steps Completed:  0\n",
            "Loss: 0.6886409521102905 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.7058639526367188 Accuracy: 0.4375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6988346576690674 Accuracy: 0.484375\n",
            "Steps Completed:  150\n",
            "Loss: 0.7098070383071899 Accuracy: 0.4296875\n",
            "-------------\n",
            "\n",
            "Epoch No:  19\n",
            "Steps Completed:  0\n",
            "Loss: 0.6945329308509827 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.7046222686767578 Accuracy: 0.4609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6915643215179443 Accuracy: 0.5234375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6852923035621643 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  20\n",
            "Steps Completed:  0\n",
            "Loss: 0.6946179866790771 Accuracy: 0.4609375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6888822317123413 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6908847093582153 Accuracy: 0.609375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6843912601470947 Accuracy: 0.5625\n",
            "-------------\n",
            "\n",
            "Epoch No:  21\n",
            "Steps Completed:  0\n",
            "Loss: 0.6960546374320984 Accuracy: 0.4921875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6877603530883789 Accuracy: 0.5625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6994805335998535 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6956133842468262 Accuracy: 0.5078125\n",
            "-------------\n",
            "\n",
            "Epoch No:  22\n",
            "Steps Completed:  0\n",
            "Loss: 0.7024973630905151 Accuracy: 0.4765625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6994068026542664 Accuracy: 0.4765625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6812493801116943 Accuracy: 0.5859375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6994832158088684 Accuracy: 0.4453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  23\n",
            "Steps Completed:  0\n",
            "Loss: 0.6835413575172424 Accuracy: 0.5625\n",
            "Steps Completed:  50\n",
            "Loss: 0.7036714553833008 Accuracy: 0.4140625\n",
            "Steps Completed:  100\n",
            "Loss: 0.7017648816108704 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.694540798664093 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  24\n",
            "Steps Completed:  0\n",
            "Loss: 0.6939878463745117 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6916195750236511 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6875455379486084 Accuracy: 0.5859375\n",
            "Steps Completed:  150\n",
            "Loss: 0.7097967863082886 Accuracy: 0.421875\n",
            "-------------\n",
            "\n",
            "Epoch No:  25\n",
            "Steps Completed:  0\n",
            "Loss: 0.6908830404281616 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6745242476463318 Accuracy: 0.5703125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6911303997039795 Accuracy: 0.5546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6937152147293091 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  26\n",
            "Steps Completed:  0\n",
            "Loss: 0.6875163316726685 Accuracy: 0.4921875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6956771612167358 Accuracy: 0.46875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6950634121894836 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6956088542938232 Accuracy: 0.4765625\n",
            "-------------\n",
            "\n",
            "Epoch No:  27\n",
            "Steps Completed:  0\n",
            "Loss: 0.6947484016418457 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.684215784072876 Accuracy: 0.5859375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6923868060112 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6960034370422363 Accuracy: 0.4921875\n",
            "-------------\n",
            "\n",
            "Epoch No:  28\n",
            "Steps Completed:  0\n",
            "Loss: 0.6866137385368347 Accuracy: 0.5078125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6917430758476257 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.7036676406860352 Accuracy: 0.4609375\n",
            "Steps Completed:  150\n",
            "Loss: 0.7007942199707031 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  29\n",
            "Steps Completed:  0\n",
            "Loss: 0.6991889476776123 Accuracy: 0.4921875\n",
            "Steps Completed:  50\n",
            "Loss: 0.697627067565918 Accuracy: 0.453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.7018071413040161 Accuracy: 0.484375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6954675316810608 Accuracy: 0.453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  30\n",
            "Steps Completed:  0\n",
            "Loss: 0.7124249339103699 Accuracy: 0.4453125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6900474429130554 Accuracy: 0.5234375\n",
            "Steps Completed:  100\n",
            "Loss: 0.7089411020278931 Accuracy: 0.4296875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6983650326728821 Accuracy: 0.4453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  31\n",
            "Steps Completed:  0\n",
            "Loss: 0.7054787278175354 Accuracy: 0.4765625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6968727707862854 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.690680205821991 Accuracy: 0.5234375\n",
            "Steps Completed:  150\n",
            "Loss: 0.7035455703735352 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  32\n",
            "Steps Completed:  0\n",
            "Loss: 0.682779848575592 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.7035571932792664 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6946925520896912 Accuracy: 0.4609375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6892170906066895 Accuracy: 0.5546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  33\n",
            "Steps Completed:  0\n",
            "Loss: 0.700275182723999 Accuracy: 0.4921875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6918569803237915 Accuracy: 0.4921875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6905896067619324 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.705958366394043 Accuracy: 0.46875\n",
            "-------------\n",
            "\n",
            "Epoch No:  34\n",
            "Steps Completed:  0\n",
            "Loss: 0.6881002187728882 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.7079037427902222 Accuracy: 0.4296875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6879026293754578 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6954739689826965 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  35\n",
            "Steps Completed:  0\n",
            "Loss: 0.689326822757721 Accuracy: 0.546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6904453039169312 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6868520975112915 Accuracy: 0.5390625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6916102766990662 Accuracy: 0.5078125\n",
            "-------------\n",
            "\n",
            "Epoch No:  36\n",
            "Steps Completed:  0\n",
            "Loss: 0.6971461772918701 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.7011510729789734 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.7029867172241211 Accuracy: 0.5\n",
            "Steps Completed:  150\n",
            "Loss: 0.6777658462524414 Accuracy: 0.6484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  37\n",
            "Steps Completed:  0\n",
            "Loss: 0.6800680756568909 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6913120746612549 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6877003312110901 Accuracy: 0.5625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6820822954177856 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  38\n",
            "Steps Completed:  0\n",
            "Loss: 0.6953164339065552 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.7046899795532227 Accuracy: 0.515625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6855063438415527 Accuracy: 0.5859375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6937294006347656 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  39\n",
            "Steps Completed:  0\n",
            "Loss: 0.6880174875259399 Accuracy: 0.5546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6835116147994995 Accuracy: 0.5390625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6931769847869873 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6943718791007996 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  40\n",
            "Steps Completed:  0\n",
            "Loss: 0.6772433519363403 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6898925304412842 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6840788125991821 Accuracy: 0.5625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6831028461456299 Accuracy: 0.5546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  41\n",
            "Steps Completed:  0\n",
            "Loss: 0.6951839923858643 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6907733678817749 Accuracy: 0.5234375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6898741722106934 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6841578483581543 Accuracy: 0.5546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  42\n",
            "Steps Completed:  0\n",
            "Loss: 0.7002272009849548 Accuracy: 0.4375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6920338869094849 Accuracy: 0.4921875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6895804405212402 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7028952836990356 Accuracy: 0.4921875\n",
            "-------------\n",
            "\n",
            "Epoch No:  43\n",
            "Steps Completed:  0\n",
            "Loss: 0.6949291825294495 Accuracy: 0.5078125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6948702335357666 Accuracy: 0.4921875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6909849643707275 Accuracy: 0.5390625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6898472309112549 Accuracy: 0.5703125\n",
            "-------------\n",
            "\n",
            "Epoch No:  44\n",
            "Steps Completed:  0\n",
            "Loss: 0.6893211603164673 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6951988339424133 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6909527778625488 Accuracy: 0.5234375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6950579285621643 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  45\n",
            "Steps Completed:  0\n",
            "Loss: 0.6937776803970337 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.700744092464447 Accuracy: 0.4921875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6958920955657959 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6918211579322815 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  46\n",
            "Steps Completed:  0\n",
            "Loss: 0.6953476071357727 Accuracy: 0.46875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6936414837837219 Accuracy: 0.5234375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6865565776824951 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6988587379455566 Accuracy: 0.453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  47\n",
            "Steps Completed:  0\n",
            "Loss: 0.6904264688491821 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.691726565361023 Accuracy: 0.4765625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6849240660667419 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6774353981018066 Accuracy: 0.578125\n",
            "-------------\n",
            "\n",
            "Epoch No:  48\n",
            "Steps Completed:  0\n",
            "Loss: 0.7021849155426025 Accuracy: 0.4765625\n",
            "Steps Completed:  50\n",
            "Loss: 0.70400470495224 Accuracy: 0.453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6960097551345825 Accuracy: 0.4609375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6950921416282654 Accuracy: 0.453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  49\n",
            "Steps Completed:  0\n",
            "Loss: 0.6865829825401306 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6944171190261841 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6917741894721985 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6899468898773193 Accuracy: 0.6015625\n",
            "-------------\n",
            "\n",
            "Epoch No:  50\n",
            "Steps Completed:  0\n",
            "Loss: 0.6935852766036987 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.7080819606781006 Accuracy: 0.46875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6883276700973511 Accuracy: 0.5390625\n",
            "Steps Completed:  150\n",
            "Loss: 0.69040846824646 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  51\n",
            "Steps Completed:  0\n",
            "Loss: 0.6985540986061096 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6870971918106079 Accuracy: 0.5625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6980165839195251 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6942493915557861 Accuracy: 0.5078125\n",
            "-------------\n",
            "\n",
            "Epoch No:  52\n",
            "Steps Completed:  0\n",
            "Loss: 0.7137645483016968 Accuracy: 0.453125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6904553771018982 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.7132386565208435 Accuracy: 0.4296875\n",
            "Steps Completed:  150\n",
            "Loss: 0.693482518196106 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  53\n",
            "Steps Completed:  0\n",
            "Loss: 0.6960129737854004 Accuracy: 0.4765625\n",
            "Steps Completed:  50\n",
            "Loss: 0.687314510345459 Accuracy: 0.5234375\n",
            "Steps Completed:  100\n",
            "Loss: 0.692963719367981 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6923534870147705 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  54\n",
            "Steps Completed:  0\n",
            "Loss: 0.6908698081970215 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6996574997901917 Accuracy: 0.4609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6970837116241455 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6855144500732422 Accuracy: 0.625\n",
            "-------------\n",
            "\n",
            "Epoch No:  55\n",
            "Steps Completed:  0\n",
            "Loss: 0.697573184967041 Accuracy: 0.4765625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6925652027130127 Accuracy: 0.4921875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6958435773849487 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6958515644073486 Accuracy: 0.5078125\n",
            "-------------\n",
            "\n",
            "Epoch No:  56\n",
            "Steps Completed:  0\n",
            "Loss: 0.6913926601409912 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6883348822593689 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.692802906036377 Accuracy: 0.5390625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6844459176063538 Accuracy: 0.5859375\n",
            "-------------\n",
            "\n",
            "Epoch No:  57\n",
            "Steps Completed:  0\n",
            "Loss: 0.6946923732757568 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6932122111320496 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6961221694946289 Accuracy: 0.453125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6983771324157715 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  58\n",
            "Steps Completed:  0\n",
            "Loss: 0.6774745583534241 Accuracy: 0.625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6885979771614075 Accuracy: 0.546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6787269115447998 Accuracy: 0.5859375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6945367455482483 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  59\n",
            "Steps Completed:  0\n",
            "Loss: 0.6863586902618408 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6891274452209473 Accuracy: 0.546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6989977955818176 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6933767795562744 Accuracy: 0.5078125\n",
            "-------------\n",
            "\n",
            "Epoch No:  60\n",
            "Steps Completed:  0\n",
            "Loss: 0.6967484951019287 Accuracy: 0.4765625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6944462060928345 Accuracy: 0.4453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6893905401229858 Accuracy: 0.5625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6926371455192566 Accuracy: 0.4921875\n",
            "-------------\n",
            "\n",
            "Epoch No:  61\n",
            "Steps Completed:  0\n",
            "Loss: 0.697702169418335 Accuracy: 0.4921875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6910524964332581 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6878820061683655 Accuracy: 0.5703125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6983124613761902 Accuracy: 0.5078125\n",
            "-------------\n",
            "\n",
            "Epoch No:  62\n",
            "Steps Completed:  0\n",
            "Loss: 0.7012178301811218 Accuracy: 0.3984375\n",
            "Steps Completed:  50\n",
            "Loss: 0.7202036380767822 Accuracy: 0.40625\n",
            "Steps Completed:  100\n",
            "Loss: 0.69758141040802 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6843767166137695 Accuracy: 0.578125\n",
            "-------------\n",
            "\n",
            "Epoch No:  63\n",
            "Steps Completed:  0\n",
            "Loss: 0.688734769821167 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6898624897003174 Accuracy: 0.546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6896288394927979 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6933841109275818 Accuracy: 0.4453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  64\n",
            "Steps Completed:  0\n",
            "Loss: 0.690803050994873 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6915023922920227 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6914386749267578 Accuracy: 0.5234375\n",
            "Steps Completed:  150\n",
            "Loss: 0.7062530517578125 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  65\n",
            "Steps Completed:  0\n",
            "Loss: 0.693132758140564 Accuracy: 0.453125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6889512538909912 Accuracy: 0.515625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6932230591773987 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6927525997161865 Accuracy: 0.5234375\n",
            "-------------\n",
            "\n",
            "Epoch No:  66\n",
            "Steps Completed:  0\n",
            "Loss: 0.7028514742851257 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.696805477142334 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.703081488609314 Accuracy: 0.453125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6882190704345703 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  67\n",
            "Steps Completed:  0\n",
            "Loss: 0.6928081512451172 Accuracy: 0.453125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6945589184761047 Accuracy: 0.515625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6901540756225586 Accuracy: 0.5\n",
            "Steps Completed:  150\n",
            "Loss: 0.6961519122123718 Accuracy: 0.4375\n",
            "-------------\n",
            "\n",
            "Epoch No:  68\n",
            "Steps Completed:  0\n",
            "Loss: 0.6940038800239563 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6885340213775635 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.7017188668251038 Accuracy: 0.484375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6911463737487793 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  69\n",
            "Steps Completed:  0\n",
            "Loss: 0.6960552334785461 Accuracy: 0.46875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6867094039916992 Accuracy: 0.5390625\n",
            "Steps Completed:  100\n",
            "Loss: 0.700667679309845 Accuracy: 0.453125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6844843029975891 Accuracy: 0.5625\n",
            "-------------\n",
            "\n",
            "Epoch No:  70\n",
            "Steps Completed:  0\n",
            "Loss: 0.7074867486953735 Accuracy: 0.421875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6965352892875671 Accuracy: 0.4765625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6851693987846375 Accuracy: 0.5703125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6838423013687134 Accuracy: 0.578125\n",
            "-------------\n",
            "\n",
            "Epoch No:  71\n",
            "Steps Completed:  0\n",
            "Loss: 0.6909527778625488 Accuracy: 0.546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6842232346534729 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6896386742591858 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6995478272438049 Accuracy: 0.4609375\n",
            "-------------\n",
            "\n",
            "Epoch No:  72\n",
            "Steps Completed:  0\n",
            "Loss: 0.6961113214492798 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6977008581161499 Accuracy: 0.4765625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6866332292556763 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6883366107940674 Accuracy: 0.5234375\n",
            "-------------\n",
            "\n",
            "Epoch No:  73\n",
            "Steps Completed:  0\n",
            "Loss: 0.6915984749794006 Accuracy: 0.4921875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6841832399368286 Accuracy: 0.5390625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6900010108947754 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6948351860046387 Accuracy: 0.4921875\n",
            "-------------\n",
            "\n",
            "Epoch No:  74\n",
            "Steps Completed:  0\n",
            "Loss: 0.6883159875869751 Accuracy: 0.546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6923080086708069 Accuracy: 0.5234375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6871705055236816 Accuracy: 0.546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6944559216499329 Accuracy: 0.4921875\n",
            "-------------\n",
            "\n",
            "Epoch No:  75\n",
            "Steps Completed:  0\n",
            "Loss: 0.7057439088821411 Accuracy: 0.4609375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6883644461631775 Accuracy: 0.5390625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6907681822776794 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6875447034835815 Accuracy: 0.5234375\n",
            "-------------\n",
            "\n",
            "Epoch No:  76\n",
            "Steps Completed:  0\n",
            "Loss: 0.688080906867981 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6909486055374146 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6874415278434753 Accuracy: 0.4765625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6847264170646667 Accuracy: 0.6171875\n",
            "-------------\n",
            "\n",
            "Epoch No:  77\n",
            "Steps Completed:  0\n",
            "Loss: 0.690838634967804 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6855545043945312 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6843754649162292 Accuracy: 0.59375\n",
            "Steps Completed:  150\n",
            "Loss: 0.688650369644165 Accuracy: 0.5234375\n",
            "-------------\n",
            "\n",
            "Epoch No:  78\n",
            "Steps Completed:  0\n",
            "Loss: 0.685858964920044 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6900817155838013 Accuracy: 0.515625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6893719434738159 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6907944083213806 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  79\n",
            "Steps Completed:  0\n",
            "Loss: 0.696949303150177 Accuracy: 0.4921875\n",
            "Steps Completed:  50\n",
            "Loss: 0.7069469094276428 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.7027479410171509 Accuracy: 0.4375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6851626038551331 Accuracy: 0.5546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  80\n",
            "Steps Completed:  0\n",
            "Loss: 0.6846474409103394 Accuracy: 0.6015625\n",
            "Steps Completed:  50\n",
            "Loss: 0.7021902203559875 Accuracy: 0.4765625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6817588210105896 Accuracy: 0.6015625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6911925077438354 Accuracy: 0.4921875\n",
            "-------------\n",
            "\n",
            "Epoch No:  81\n",
            "Steps Completed:  0\n",
            "Loss: 0.6976573467254639 Accuracy: 0.4921875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6891670227050781 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6928369402885437 Accuracy: 0.484375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6807817816734314 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  82\n",
            "Steps Completed:  0\n",
            "Loss: 0.6958284378051758 Accuracy: 0.4453125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6899018287658691 Accuracy: 0.5390625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6903696060180664 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6872297525405884 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  83\n",
            "Steps Completed:  0\n",
            "Loss: 0.6878855228424072 Accuracy: 0.546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6791856288909912 Accuracy: 0.578125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6950492262840271 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6994863748550415 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  84\n",
            "Steps Completed:  0\n",
            "Loss: 0.6937249898910522 Accuracy: 0.5078125\n",
            "Steps Completed:  50\n",
            "Loss: 0.701651394367218 Accuracy: 0.4453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6811025142669678 Accuracy: 0.5390625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6838409900665283 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  85\n",
            "Steps Completed:  0\n",
            "Loss: 0.6925652027130127 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6934225559234619 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6853122711181641 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6938265562057495 Accuracy: 0.4765625\n",
            "-------------\n",
            "\n",
            "Epoch No:  86\n",
            "Steps Completed:  0\n",
            "Loss: 0.6920806169509888 Accuracy: 0.4921875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6904038786888123 Accuracy: 0.5390625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6883439421653748 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6858640909194946 Accuracy: 0.578125\n",
            "-------------\n",
            "\n",
            "Epoch No:  87\n",
            "Steps Completed:  0\n",
            "Loss: 0.6781519651412964 Accuracy: 0.6015625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6992368102073669 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6974385380744934 Accuracy: 0.5234375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6967306137084961 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  88\n",
            "Steps Completed:  0\n",
            "Loss: 0.710138738155365 Accuracy: 0.4375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6924188733100891 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.692714512348175 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6819762587547302 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  89\n",
            "Steps Completed:  0\n",
            "Loss: 0.6904175281524658 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6910759210586548 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6906630992889404 Accuracy: 0.5\n",
            "Steps Completed:  150\n",
            "Loss: 0.6817437410354614 Accuracy: 0.5625\n",
            "-------------\n",
            "\n",
            "Epoch No:  90\n",
            "Steps Completed:  0\n",
            "Loss: 0.6933242082595825 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6984535455703735 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6891614198684692 Accuracy: 0.59375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6875807642936707 Accuracy: 0.5546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  91\n",
            "Steps Completed:  0\n",
            "Loss: 0.6879278421401978 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6893001794815063 Accuracy: 0.5234375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6775494813919067 Accuracy: 0.59375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6929941177368164 Accuracy: 0.5234375\n",
            "-------------\n",
            "\n",
            "Epoch No:  92\n",
            "Steps Completed:  0\n",
            "Loss: 0.6973048448562622 Accuracy: 0.4765625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6614792943000793 Accuracy: 0.6484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6914324760437012 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6853008270263672 Accuracy: 0.5546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  93\n",
            "Steps Completed:  0\n",
            "Loss: 0.6820119619369507 Accuracy: 0.5546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6917790174484253 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6766656637191772 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6811744570732117 Accuracy: 0.5078125\n",
            "-------------\n",
            "\n",
            "Epoch No:  94\n",
            "Steps Completed:  0\n",
            "Loss: 0.6957314014434814 Accuracy: 0.4921875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6894363164901733 Accuracy: 0.5625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6842094659805298 Accuracy: 0.5\n",
            "Steps Completed:  150\n",
            "Loss: 0.7002492547035217 Accuracy: 0.4296875\n",
            "-------------\n",
            "\n",
            "Epoch No:  95\n",
            "Steps Completed:  0\n",
            "Loss: 0.6941852569580078 Accuracy: 0.46875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6897416114807129 Accuracy: 0.4453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.708289384841919 Accuracy: 0.5\n",
            "Steps Completed:  150\n",
            "Loss: 0.689771831035614 Accuracy: 0.5859375\n",
            "-------------\n",
            "\n",
            "Epoch No:  96\n",
            "Steps Completed:  0\n",
            "Loss: 0.6866633892059326 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6932149529457092 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6885606050491333 Accuracy: 0.546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6877998113632202 Accuracy: 0.5546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  97\n",
            "Steps Completed:  0\n",
            "Loss: 0.6883612275123596 Accuracy: 0.46875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6801347136497498 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.7190037965774536 Accuracy: 0.4140625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6814723014831543 Accuracy: 0.578125\n",
            "-------------\n",
            "\n",
            "Epoch No:  98\n",
            "Steps Completed:  0\n",
            "Loss: 0.704007089138031 Accuracy: 0.4609375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6753185391426086 Accuracy: 0.5703125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6965448260307312 Accuracy: 0.4765625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6887755393981934 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  99\n",
            "Steps Completed:  0\n",
            "Loss: 0.7012900114059448 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6856820583343506 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6915480494499207 Accuracy: 0.5\n",
            "Steps Completed:  150\n",
            "Loss: 0.6889926791191101 Accuracy: 0.5390625\n",
            "-------------\n",
            "\n",
            "Working on test dataset\n",
            "Steps Completed:  0\n",
            "Steps Completed:  50\n",
            "Steps Completed:  100\n",
            "Steps Completed:  150\n",
            "Final Test Accuracy Average: 0.4972756505012512 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL-IqJwTQOuz"
      },
      "source": [
        "## Model5\n",
        "\n",
        "1.   With 20 Epochs\n",
        "2.   The test accuracy is 50.3%\n",
        "3.   Batch Size of 128\n",
        "4.   Hidden layer neurons are 128\n",
        "5.   Max corpus length 40000\n",
        "6.   Max words 500, padding post, truncating pre\n",
        "7.   Weights are initialised from -0.2 to 0.2\n",
        "8.   Hinge loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f22DfYoAQOu0",
        "outputId": "44d706f4-aea9-4dfb-c7be-f7281970c966"
      },
      "source": [
        "train_loop(modelConfig5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch No:  0\n",
            "Steps Completed:  0\n",
            "Loss: 0.9987021684646606 Accuracy: 0.5625\n",
            "Steps Completed:  50\n",
            "Loss: 1.0434925556182861 Accuracy: 0.421875\n",
            "Steps Completed:  100\n",
            "Loss: 1.0033537149429321 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.9796609282493591 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  1\n",
            "Steps Completed:  0\n",
            "Loss: 1.0047553777694702 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.975113570690155 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.9954187870025635 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.9687178134918213 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  2\n",
            "Steps Completed:  0\n",
            "Loss: 0.893357515335083 Accuracy: 0.625\n",
            "Steps Completed:  50\n",
            "Loss: 0.8132309913635254 Accuracy: 0.6015625\n",
            "Steps Completed:  100\n",
            "Loss: 1.0085259675979614 Accuracy: 0.4765625\n",
            "Steps Completed:  150\n",
            "Loss: 0.9174610376358032 Accuracy: 0.5546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  3\n",
            "Steps Completed:  0\n",
            "Loss: 0.7928885817527771 Accuracy: 0.6484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.7690962553024292 Accuracy: 0.6328125\n",
            "Steps Completed:  100\n",
            "Loss: 0.9221796989440918 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7722242474555969 Accuracy: 0.6484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  4\n",
            "Steps Completed:  0\n",
            "Loss: 0.7695585489273071 Accuracy: 0.625\n",
            "Steps Completed:  50\n",
            "Loss: 0.702038586139679 Accuracy: 0.7265625\n",
            "Steps Completed:  100\n",
            "Loss: 0.738619327545166 Accuracy: 0.6484375\n",
            "Steps Completed:  150\n",
            "Loss: 0.8448368906974792 Accuracy: 0.5703125\n",
            "-------------\n",
            "\n",
            "Epoch No:  5\n",
            "Steps Completed:  0\n",
            "Loss: 0.869552731513977 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.7717002630233765 Accuracy: 0.6328125\n",
            "Steps Completed:  100\n",
            "Loss: 0.8105411529541016 Accuracy: 0.6015625\n",
            "Steps Completed:  150\n",
            "Loss: 0.7469059228897095 Accuracy: 0.6328125\n",
            "-------------\n",
            "\n",
            "Epoch No:  6\n",
            "Steps Completed:  0\n",
            "Loss: 0.8069871068000793 Accuracy: 0.5546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.8002569675445557 Accuracy: 0.59375\n",
            "Steps Completed:  100\n",
            "Loss: 0.7415741086006165 Accuracy: 0.625\n",
            "Steps Completed:  150\n",
            "Loss: 0.7975336909294128 Accuracy: 0.59375\n",
            "-------------\n",
            "\n",
            "Epoch No:  7\n",
            "Steps Completed:  0\n",
            "Loss: 0.9799197912216187 Accuracy: 0.609375\n",
            "Steps Completed:  50\n",
            "Loss: 0.8060302138328552 Accuracy: 0.625\n",
            "Steps Completed:  100\n",
            "Loss: 0.8176098465919495 Accuracy: 0.59375\n",
            "Steps Completed:  150\n",
            "Loss: 0.7606325745582581 Accuracy: 0.65625\n",
            "-------------\n",
            "\n",
            "Epoch No:  8\n",
            "Steps Completed:  0\n",
            "Loss: 0.7690266966819763 Accuracy: 0.6640625\n",
            "Steps Completed:  50\n",
            "Loss: 0.8177210092544556 Accuracy: 0.6015625\n",
            "Steps Completed:  100\n",
            "Loss: 0.7517725825309753 Accuracy: 0.6328125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6953790783882141 Accuracy: 0.671875\n",
            "-------------\n",
            "\n",
            "Epoch No:  9\n",
            "Steps Completed:  0\n",
            "Loss: 0.826698362827301 Accuracy: 0.5859375\n",
            "Steps Completed:  50\n",
            "Loss: 0.7664206624031067 Accuracy: 0.609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.7249821424484253 Accuracy: 0.671875\n",
            "Steps Completed:  150\n",
            "Loss: 0.8319603800773621 Accuracy: 0.5703125\n",
            "-------------\n",
            "\n",
            "Epoch No:  10\n",
            "Steps Completed:  0\n",
            "Loss: 0.7997782230377197 Accuracy: 0.5859375\n",
            "Steps Completed:  50\n",
            "Loss: 0.8081527948379517 Accuracy: 0.578125\n",
            "Steps Completed:  100\n",
            "Loss: 0.8243492245674133 Accuracy: 0.5546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.9712522625923157 Accuracy: 0.5703125\n",
            "-------------\n",
            "\n",
            "Epoch No:  11\n",
            "Steps Completed:  0\n",
            "Loss: 0.9823821783065796 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.9298020005226135 Accuracy: 0.5703125\n",
            "Steps Completed:  100\n",
            "Loss: 0.8799211978912354 Accuracy: 0.6015625\n",
            "Steps Completed:  150\n",
            "Loss: 0.8577402830123901 Accuracy: 0.5859375\n",
            "-------------\n",
            "\n",
            "Epoch No:  12\n",
            "Steps Completed:  0\n",
            "Loss: 0.7805129289627075 Accuracy: 0.6328125\n",
            "Steps Completed:  50\n",
            "Loss: 0.8596682548522949 Accuracy: 0.578125\n",
            "Steps Completed:  100\n",
            "Loss: 0.8722939491271973 Accuracy: 0.5703125\n",
            "Steps Completed:  150\n",
            "Loss: 0.9075540900230408 Accuracy: 0.5625\n",
            "-------------\n",
            "\n",
            "Epoch No:  13\n",
            "Steps Completed:  0\n",
            "Loss: 0.8925358057022095 Accuracy: 0.5859375\n",
            "Steps Completed:  50\n",
            "Loss: 0.7897994518280029 Accuracy: 0.6171875\n",
            "Steps Completed:  100\n",
            "Loss: 0.8179154396057129 Accuracy: 0.578125\n",
            "Steps Completed:  150\n",
            "Loss: 0.8480281829833984 Accuracy: 0.6015625\n",
            "-------------\n",
            "\n",
            "Epoch No:  14\n",
            "Steps Completed:  0\n",
            "Loss: 0.7644496560096741 Accuracy: 0.6328125\n",
            "Steps Completed:  50\n",
            "Loss: 0.8300867080688477 Accuracy: 0.5703125\n",
            "Steps Completed:  100\n",
            "Loss: 0.7137907147407532 Accuracy: 0.6640625\n",
            "Steps Completed:  150\n",
            "Loss: 0.8202506303787231 Accuracy: 0.59375\n",
            "-------------\n",
            "\n",
            "Epoch No:  15\n",
            "Steps Completed:  0\n",
            "Loss: 0.7488176822662354 Accuracy: 0.625\n",
            "Steps Completed:  50\n",
            "Loss: 0.7849636077880859 Accuracy: 0.6015625\n",
            "Steps Completed:  100\n",
            "Loss: 0.7892409563064575 Accuracy: 0.6015625\n",
            "Steps Completed:  150\n",
            "Loss: 0.7967185378074646 Accuracy: 0.609375\n",
            "-------------\n",
            "\n",
            "Epoch No:  16\n",
            "Steps Completed:  0\n",
            "Loss: 0.7308281660079956 Accuracy: 0.6640625\n",
            "Steps Completed:  50\n",
            "Loss: 0.7532223463058472 Accuracy: 0.6171875\n",
            "Steps Completed:  100\n",
            "Loss: 0.7975736856460571 Accuracy: 0.6171875\n",
            "Steps Completed:  150\n",
            "Loss: 0.7684744596481323 Accuracy: 0.609375\n",
            "-------------\n",
            "\n",
            "Epoch No:  17\n",
            "Steps Completed:  0\n",
            "Loss: 0.7018880248069763 Accuracy: 0.6875\n",
            "Steps Completed:  50\n",
            "Loss: 0.7769281268119812 Accuracy: 0.6171875\n",
            "Steps Completed:  100\n",
            "Loss: 0.7777866721153259 Accuracy: 0.5625\n",
            "Steps Completed:  150\n",
            "Loss: 0.7221190929412842 Accuracy: 0.6640625\n",
            "-------------\n",
            "\n",
            "Epoch No:  18\n",
            "Steps Completed:  0\n",
            "Loss: 0.78399658203125 Accuracy: 0.609375\n",
            "Steps Completed:  50\n",
            "Loss: 0.7709033489227295 Accuracy: 0.6015625\n",
            "Steps Completed:  100\n",
            "Loss: 0.7711292505264282 Accuracy: 0.6171875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6956968307495117 Accuracy: 0.703125\n",
            "-------------\n",
            "\n",
            "Epoch No:  19\n",
            "Steps Completed:  0\n",
            "Loss: 0.7791162729263306 Accuracy: 0.6328125\n",
            "Steps Completed:  50\n",
            "Loss: 0.7670352458953857 Accuracy: 0.625\n",
            "Steps Completed:  100\n",
            "Loss: 0.8309760093688965 Accuracy: 0.578125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7428493499755859 Accuracy: 0.640625\n",
            "-------------\n",
            "\n",
            "Working on test dataset\n",
            "Steps Completed:  0\n",
            "Steps Completed:  50\n",
            "Steps Completed:  100\n",
            "Steps Completed:  150\n",
            "Final Test Accuracy Average: 0.5030849575996399 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW6u4DcOG2aZ"
      },
      "source": [
        "# References\n",
        "\n",
        "\n",
        "1.   https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e\n",
        "2.   https://www.deeplearningbook.org/contents/rnn.html\n",
        "3.   https://ovgu-ailab.github.io/idl2021/assignments/5/rnns_part1.ipynb\n",
        "4.   https://keras.io/api/losses/hinge_losses/\n",
        "\n"
      ]
    }
  ]
}