{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_5_RNN_Part1_Srinath.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9b1ec548",
        "0c34550c",
        "c715ac7b",
        "Fc9OZbtI7oPF",
        "NUD_v_L21y2A",
        "gWJ6KpLz60sM",
        "7PH1icKQ8fmG",
        "jnxcVXiV7EZy",
        "k7TIRqNmFSXS",
        "n0RPyzhHW4I-",
        "PTCrk77NoxTN",
        "fiTvrH56GhH5",
        "VYP3UFTgN5gq",
        "P2o29F7hN7mI",
        "MJADKoSYw3LY",
        "ziUGUkYSXeGH",
        "SRMF4TOko9wg"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mannam95/Deep_Learning_Programming/blob/main/Assignment5/Assignment_5_RNN_Part1_Srinath.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6aef94d"
      },
      "source": [
        "# Team Assignment\n",
        "\n",
        "\n",
        "1.   Srinath Mannam (229750)\n",
        "2.   Meghana Rao (234907)\n",
        "3.   Govind Shukla (235192)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b1ec548"
      },
      "source": [
        "# import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85eb7211"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numba import cuda\n",
        "import sys\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "import pydot\n",
        "import graphviz\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from datetime import datetime\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c34550c"
      },
      "source": [
        "# Change the directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94b2626e"
      },
      "source": [
        "working_directory = '/content/drive/My Drive/Colab Notebooks/OVGU/Deep_Learning/05_Assignment'\n",
        "def colabDrive():\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "    if os.getcwd() !=  working_directory:\n",
        "      os.chdir(working_directory)\n",
        "    print(os.getcwd())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8540ed4"
      },
      "source": [
        "#colabDrive()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c715ac7b"
      },
      "source": [
        "# Clears GPU memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "591268b2"
      },
      "source": [
        "def clearGPUMemory():\n",
        "    from numba import cuda \n",
        "    device = cuda.get_current_device()\n",
        "    device.reset()\n",
        "    !nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "caee9b53"
      },
      "source": [
        "#clearGPUMemory()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6mjm447GMHX"
      },
      "source": [
        "# Prepare the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dv0CvhD7Yga"
      },
      "source": [
        "## Load the dataset and remove infrequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVE7aMvC4iXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27f0bb8-aa47-49c0-d59c-392739fcc22b"
      },
      "source": [
        "num_words = 40000\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=num_words)\n",
        "\n",
        "print('Loaded dataset with {} training samples, {} test samples'.format(len(train_sequences), len(test_sequences)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "Loaded dataset with 25000 training samples, 25000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc9OZbtI7oPF"
      },
      "source": [
        "## Have some look at train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c07u7Z7s4opk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5e68a3-5416-4bc2-b275-48431ff46105"
      },
      "source": [
        "# look at some sequences. words have been replaced with arbitrary index mappings\n",
        "# 1 is a special \"beginning of sequence\" marker\n",
        "# infrequent words have been replaced by the index 2\n",
        "# actual words start with index 4, 3 is never used (???)\n",
        "train_sequences[:3]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFCZvjGjuy1v",
        "outputId": "3da0c7d4-6df0-4625-e4f1-deb365e51aed"
      },
      "source": [
        "print('---review---')\n",
        "print(train_sequences[6])\n",
        "print('---label---')\n",
        "print(train_labels[6])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---review---\n",
            "[1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 10626, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 16393, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 32677, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 26441, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 33740, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
            "---label---\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUD_v_L21y2A"
      },
      "source": [
        "## to restore words, load the word-to-index mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYX6F3AX5hpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc0f3b2-8f02-49b2-9514-00dc4fe55865"
      },
      "source": [
        "word2id = tf.keras.datasets.imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "print('---review with words---')\n",
        "print([id2word.get(i, ' ') for i in train_sequences[6]])\n",
        "print('---label---')\n",
        "print(train_sequences[6])\n",
        "print('---label---')\n",
        "print(train_labels[6])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "---review with words---\n",
            "['the', 'boiled', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'murdering', 'naschy', 'br', 'villain', 'council', 'suggestion', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'echoed', 'concentrates', 'concept', 'issue', 'skeptical', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'rocketed', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', \"captain's\", 'starship', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'originals', 'things', 'is', 'far', 'this', 'make', 'mistakes', \"kevin's\", 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'dose', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n",
            "---label---\n",
            "[1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 10626, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 16393, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 32677, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 26441, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 33740, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
            "---label---\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWJ6KpLz60sM"
      },
      "source": [
        "## Maximum review length and minimum review length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6LZSJS_62Ye",
        "outputId": "6db4abf1-17dd-4438-ee6e-a1ad1b8b1672"
      },
      "source": [
        "print('Maximum review length: {}'.format(len(max((train_sequences + test_sequences), key=len))))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum review length: 2697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XMcFfgQ7H-O",
        "outputId": "bc43841f-9656-4622-8450-e1792b1b0b8d"
      },
      "source": [
        "print('Minimum review length: {}'.format(len(min((train_sequences + test_sequences), key=len))))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum review length: 70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PH1icKQ8fmG"
      },
      "source": [
        "## Overview over the sequence length in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bpWf-SfA8d0d",
        "outputId": "0012f020-ea65-4dd7-a168-31fc1cf0db87"
      },
      "source": [
        "sequence_lengths = [len(sequence) for sequence in train_sequences]\n",
        "max_len = max(sequence_lengths)\n",
        "max_len\n",
        "\n",
        "plt.hist(sequence_lengths, bins=80)\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnxcVXiV7EZy"
      },
      "source": [
        "## Pad Sequences  to some length because the dataset should be in rectangular for feeding to tensors\n",
        "\n",
        "*  all sequences above maxlen will be truncated to that length\n",
        "*  note: pad_sequences has \"pre\" and \"post\" options for both padding and truncation. one may be better than the other!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDFYpy4Q8LH9"
      },
      "source": [
        "max_words = 500\n",
        "train_sequences = sequence.pad_sequences(train_sequences, maxlen=max_words, padding = \"post\", truncating = \"pre\")\n",
        "test_sequences = sequence.pad_sequences(test_sequences, maxlen=max_words, padding = \"post\", truncating = \"pre\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sxPAMPF9GRf"
      },
      "source": [
        "## Load the dataset into tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1YiFTf89KzS"
      },
      "source": [
        "train_labels = train_labels.reshape(-1).astype(np.int32)\n",
        "test_labels = test_labels.reshape(-1).astype(np.int32)\n",
        "# train_data = tf.data.Dataset.from_tensor_slices((train_sequences, train_labels))\n",
        "# test_data = tf.data.Dataset.from_tensor_slices((test_sequences, test_labels))\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences, train_labels)).shuffle(25000).batch(128, drop_remainder=True)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sequences, test_labels)).batch(128, drop_remainder=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epD2fyBM-CKA",
        "outputId": "6eb5a6f4-65d8-4e5f-d172-008a3551786a"
      },
      "source": [
        "print(\"Training Dataset Size: \",train_sequences.shape)\n",
        "print(\"Test Dataset Size: \",test_sequences.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Size:  (25000, 500)\n",
            "Test Dataset Size:  (25000, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiB5DU5SAKLG"
      },
      "source": [
        "# Models Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7TIRqNmFSXS"
      },
      "source": [
        "## Model1 Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12k45HYkAJY8"
      },
      "source": [
        "modelConfig1 = {\n",
        "    \"epochs\" : 10,\n",
        "    \"learning_rate\" : 0.1,\n",
        "\n",
        "    ##Input to hidden(U)\n",
        "    \"W_Inp_Hid\":  tf.Variable(tf.random.uniform([20000, 64], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "    ##Initial Previous hidden(ht-1)\n",
        "    \"Init_Prev_Hidden\":  tf.Variable(np.zeros([1,64], dtype=np.float32)),\n",
        "    ##Hidden to hidden(W)\n",
        "    \"W_Hid_Hid\":  tf.Variable(tf.random.uniform([64, 64], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "    ##bias(b)\n",
        "    \"b\": tf.Variable(np.zeros((1, 64), dtype=np.float32)),\n",
        "    ##Hidden to hidden(W)\n",
        "    \"W_Hid_Out\":  tf.Variable(tf.random.uniform([64, 2], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "    ##bias(c)\n",
        "    \"c\": tf.Variable(np.zeros((128, 2), dtype=np.float32))\n",
        "}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0RPyzhHW4I-"
      },
      "source": [
        "## Model2 Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ljDP3y7W4JV"
      },
      "source": [
        "modelConfig2 = {\n",
        "    \"epochs\" : 40,\n",
        "    \"learning_rate\" : 0.001,\n",
        "\n",
        "    ##Input to hidden(U)\n",
        "    \"W_Inp_Hid\":  tf.Variable(tf.random.uniform([num_words, 128], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "    ##Initial Previous hidden(ht-1)\n",
        "    \"Init_Prev_Hidden\":  tf.Variable(np.zeros([1,128], dtype=np.float32)),\n",
        "    ##Hidden to hidden(W)\n",
        "    \"W_Hid_Hid\":  tf.Variable(tf.random.uniform([128, 128], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "    ##bias(b)\n",
        "    \"b\": tf.Variable(np.zeros((1, 128), dtype=np.float32)),\n",
        "    ##Hidden to hidden(V)\n",
        "    \"W_Hid_Out\":  tf.Variable(tf.random.uniform([128, 2], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "    ##bias(c)\n",
        "    \"c\": tf.Variable(np.zeros((128, 2), dtype=np.float32))\n",
        "}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTCrk77NoxTN"
      },
      "source": [
        "## Model3 Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as2q_giUoxTW"
      },
      "source": [
        "modelConfig3 = {\n",
        "    \"epochs\" : 20,\n",
        "    \"learning_rate\" : 0.001,\n",
        "\n",
        "    ##Input to hidden(U)\n",
        "    \"W_Inp_Hid\":  tf.Variable(tf.random.uniform([num_words, 128], minval=-0.2, maxval=0.2, dtype=np.float32)),\n",
        "    ##Initial Previous hidden(ht-1)\n",
        "    \"Init_Prev_Hidden\":  tf.Variable(np.zeros([1,128], dtype=np.float32)),\n",
        "    ##Hidden to hidden(W)\n",
        "    \"W_Hid_Hid\":  tf.Variable(tf.random.uniform([128, 128], minval=-0.2, maxval=0.2, dtype=np.float32)),\n",
        "    ##bias(b)\n",
        "    \"b\": tf.Variable(np.zeros((1, 128), dtype=np.float32)),\n",
        "    ##Hidden to hidden(V)\n",
        "    \"W_Hid_Out\":  tf.Variable(tf.random.uniform([128, 2], minval=-0.2, maxval=0.2, dtype=np.float32)),\n",
        "    ##bias(c)\n",
        "    \"c\": tf.Variable(np.zeros((128, 2), dtype=np.float32))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiTvrH56GhH5"
      },
      "source": [
        "# Generic Function for training, testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYP3UFTgN5gq"
      },
      "source": [
        "## RNN Loop Cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JheLUJCjN4Eb"
      },
      "source": [
        "def rnn_loop(sequences, modelConfig):\n",
        "    old_state = modelConfig['Init_Prev_Hidden']\n",
        "    #seq_onehot = tf.one_hot(sequences, depth=num_words)\n",
        "\n",
        "    for step in range(max_words):\n",
        "        #x_t = seq_onehot[:,step]\n",
        "        x_t = sequences[:, step]\n",
        "        x_t = tf.one_hot(x_t, depth=num_words)\n",
        "        at = modelConfig['b'] + tf.matmul(old_state, modelConfig['W_Hid_Hid']) + tf.matmul(x_t, modelConfig['W_Inp_Hid'])\n",
        "        new_state = tf.nn.tanh(at)\n",
        "        old_state = new_state\n",
        "\n",
        "    o_t = modelConfig['c'] + tf.matmul(new_state, modelConfig['W_Hid_Out'])\n",
        "\n",
        "    return o_t"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2o29F7hN7mI"
      },
      "source": [
        "## Training and Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEw-Xlx3_OsU"
      },
      "source": [
        "def train_loop(modelConfig):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=modelConfig['learning_rate'])\n",
        "    for epoch in range(modelConfig['epochs']):\n",
        "      print(\"Epoch No: \", epoch)\n",
        "      for step, (sequence_batch, label_batch) in enumerate(train_data):\n",
        "          # label_batch = tf.reshape(label_batch, [1])\n",
        "          with tf.GradientTape() as tape:\n",
        "              logits = rnn_loop(sequence_batch, modelConfig)\n",
        "              xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                  logits=logits, labels=label_batch))\n",
        "\n",
        "          ## Gradients calculation and update weights\n",
        "          grads = tape.gradient(xent, [modelConfig['W_Inp_Hid'], modelConfig['W_Hid_Hid'], modelConfig['b'], modelConfig['W_Hid_Out'], modelConfig['c']])\n",
        "          optimizer.apply_gradients(zip(grads, [modelConfig['W_Inp_Hid'], modelConfig['W_Hid_Hid'], modelConfig['b'], modelConfig['W_Hid_Out'], modelConfig['c']]))\n",
        "\n",
        "\n",
        "          if not step % 50:\n",
        "              print(\"Steps Completed: \", step)\n",
        "              preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "              acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch), tf.float32))\n",
        "              print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "          \n",
        "      print(\"-------------\\n\")\n",
        "\n",
        "    #Testing\n",
        "    print(\"Working on test dataset\")\n",
        "    testAcc = []\n",
        "    for step, (sequence_batch, label_batch) in enumerate(test_data):\n",
        "        if not step % 50:\n",
        "          print(\"Steps Completed: \", step)\n",
        "        logits = rnn_loop(sequence_batch, modelConfig)\n",
        "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch), tf.float32))\n",
        "        testAcc.append(acc)\n",
        "\n",
        "    print(\"Final Test Accuracy Average: {} \".format(sum(testAcc)/len(testAcc)))\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8NysXi27_Gg"
      },
      "source": [
        "# Different Models Running\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJADKoSYw3LY"
      },
      "source": [
        "## Model1\n",
        "\n",
        "1.   With 10 Epochs\n",
        "2.   The test accuracy is 49.6%\n",
        "3.   Batch Size of 128\n",
        "4.   Hidden layer neurons are 64\n",
        "5.   Max corpus length 20000\n",
        "6.   Max words 300, padding post, truncating post"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH36czdew3Lf",
        "outputId": "09df6c38-70b4-4217-b64b-b89126896a38"
      },
      "source": [
        "train_loop(modelConfig1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch No:  0\n",
            "Steps Completed:  0\n",
            "Loss: 0.696199357509613 Accuracy: 0.4609375\n",
            "Steps Completed:  50\n",
            "Loss: 0.9726694822311401 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.7283741235733032 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.746300995349884 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  1\n",
            "Steps Completed:  0\n",
            "Loss: 0.8708627223968506 Accuracy: 0.5703125\n",
            "Steps Completed:  50\n",
            "Loss: 0.7043036818504333 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.8352829217910767 Accuracy: 0.484375\n",
            "Steps Completed:  150\n",
            "Loss: 0.7988328337669373 Accuracy: 0.5390625\n",
            "-------------\n",
            "\n",
            "Epoch No:  2\n",
            "Steps Completed:  0\n",
            "Loss: 0.6984706521034241 Accuracy: 0.5859375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6942040920257568 Accuracy: 0.59375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6530992388725281 Accuracy: 0.609375\n",
            "Steps Completed:  150\n",
            "Loss: 0.8190151453018188 Accuracy: 0.46875\n",
            "-------------\n",
            "\n",
            "Epoch No:  3\n",
            "Steps Completed:  0\n",
            "Loss: 0.7579739093780518 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.7478058934211731 Accuracy: 0.4765625\n",
            "Steps Completed:  100\n",
            "Loss: 0.7769534587860107 Accuracy: 0.4765625\n",
            "Steps Completed:  150\n",
            "Loss: 0.7120305299758911 Accuracy: 0.4921875\n",
            "-------------\n",
            "\n",
            "Epoch No:  4\n",
            "Steps Completed:  0\n",
            "Loss: 0.8354402780532837 Accuracy: 0.4375\n",
            "Steps Completed:  50\n",
            "Loss: 0.7821717262268066 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.7646347284317017 Accuracy: 0.5\n",
            "Steps Completed:  150\n",
            "Loss: 0.7192972302436829 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  5\n",
            "Steps Completed:  0\n",
            "Loss: 0.7207278609275818 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.7831155061721802 Accuracy: 0.4609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.883648693561554 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7884370684623718 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  6\n",
            "Steps Completed:  0\n",
            "Loss: 0.7322415113449097 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.7204431891441345 Accuracy: 0.6015625\n",
            "Steps Completed:  100\n",
            "Loss: 0.7217613458633423 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7292568683624268 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  7\n",
            "Steps Completed:  0\n",
            "Loss: 0.7257785797119141 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.7618244290351868 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.7485902905464172 Accuracy: 0.421875\n",
            "Steps Completed:  150\n",
            "Loss: 0.7179251909255981 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  8\n",
            "Steps Completed:  0\n",
            "Loss: 0.7179141640663147 Accuracy: 0.453125\n",
            "Steps Completed:  50\n",
            "Loss: 0.8544270992279053 Accuracy: 0.4609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.9200974702835083 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7902131080627441 Accuracy: 0.5078125\n",
            "-------------\n",
            "\n",
            "Epoch No:  9\n",
            "Steps Completed:  0\n",
            "Loss: 0.8069676756858826 Accuracy: 0.5703125\n",
            "Steps Completed:  50\n",
            "Loss: 0.8043767213821411 Accuracy: 0.4609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.7284603118896484 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.7533402442932129 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Working on test dataset\n",
            "Steps Completed:  0\n",
            "Steps Completed:  50\n",
            "Steps Completed:  100\n",
            "Steps Completed:  150\n",
            "Final Test Accuracy Average: 0.49655449390411377 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziUGUkYSXeGH"
      },
      "source": [
        "## Model2\n",
        "\n",
        "1.   With 40 Epochs\n",
        "2.   The test accuracy is 50.6%\n",
        "3.   Batch Size of 128\n",
        "4.   Hidden layer neurons are 128\n",
        "5.   Max corpus length 40000\n",
        "6.   Max words 500, padding post, truncating pre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qkM4eVXXeGb",
        "outputId": "03498d2c-716c-4c19-c28a-f671949fd86a"
      },
      "source": [
        "train_loop(modelConfig2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch No:  0\n",
            "Steps Completed:  0\n",
            "Loss: 0.6928707957267761 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6919717788696289 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6927862763404846 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6956644058227539 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  1\n",
            "Steps Completed:  0\n",
            "Loss: 0.6871331334114075 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6924787759780884 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6678862571716309 Accuracy: 0.578125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6944794654846191 Accuracy: 0.5078125\n",
            "-------------\n",
            "\n",
            "Epoch No:  2\n",
            "Steps Completed:  0\n",
            "Loss: 0.6982626914978027 Accuracy: 0.453125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6907624006271362 Accuracy: 0.6015625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6890923976898193 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.681374192237854 Accuracy: 0.59375\n",
            "-------------\n",
            "\n",
            "Epoch No:  3\n",
            "Steps Completed:  0\n",
            "Loss: 0.6838294267654419 Accuracy: 0.4921875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6668428778648376 Accuracy: 0.59375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6884939074516296 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6923825740814209 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  4\n",
            "Steps Completed:  0\n",
            "Loss: 0.6853148937225342 Accuracy: 0.546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6748105883598328 Accuracy: 0.5859375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6739037036895752 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6957507729530334 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  5\n",
            "Steps Completed:  0\n",
            "Loss: 0.685388445854187 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6888481378555298 Accuracy: 0.4921875\n",
            "Steps Completed:  100\n",
            "Loss: 0.7017629146575928 Accuracy: 0.4296875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6903291344642639 Accuracy: 0.5234375\n",
            "-------------\n",
            "\n",
            "Epoch No:  6\n",
            "Steps Completed:  0\n",
            "Loss: 0.6853963732719421 Accuracy: 0.6328125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6989051699638367 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6801783442497253 Accuracy: 0.5703125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7012248039245605 Accuracy: 0.453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  7\n",
            "Steps Completed:  0\n",
            "Loss: 0.6868335008621216 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6699327230453491 Accuracy: 0.6015625\n",
            "Steps Completed:  100\n",
            "Loss: 0.684109091758728 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7213274240493774 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  8\n",
            "Steps Completed:  0\n",
            "Loss: 0.6706337332725525 Accuracy: 0.5859375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6822420358657837 Accuracy: 0.546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6683303117752075 Accuracy: 0.5703125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6870102286338806 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  9\n",
            "Steps Completed:  0\n",
            "Loss: 0.6819988489151001 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6813089847564697 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6834003925323486 Accuracy: 0.484375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6858251094818115 Accuracy: 0.5390625\n",
            "-------------\n",
            "\n",
            "Epoch No:  10\n",
            "Steps Completed:  0\n",
            "Loss: 0.6824917197227478 Accuracy: 0.546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6784583330154419 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.7553969621658325 Accuracy: 0.4453125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6870009303092957 Accuracy: 0.59375\n",
            "-------------\n",
            "\n",
            "Epoch No:  11\n",
            "Steps Completed:  0\n",
            "Loss: 0.677443265914917 Accuracy: 0.5078125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6986886262893677 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6865496039390564 Accuracy: 0.578125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6931723356246948 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  12\n",
            "Steps Completed:  0\n",
            "Loss: 0.7095809578895569 Accuracy: 0.4375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6887379288673401 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.684390664100647 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.7124885320663452 Accuracy: 0.421875\n",
            "-------------\n",
            "\n",
            "Epoch No:  13\n",
            "Steps Completed:  0\n",
            "Loss: 0.6898264288902283 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.69621741771698 Accuracy: 0.4765625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6994284987449646 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6946438550949097 Accuracy: 0.4609375\n",
            "-------------\n",
            "\n",
            "Epoch No:  14\n",
            "Steps Completed:  0\n",
            "Loss: 0.6826863288879395 Accuracy: 0.6015625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6906579732894897 Accuracy: 0.5625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6928120851516724 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6931532621383667 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  15\n",
            "Steps Completed:  0\n",
            "Loss: 0.6961991786956787 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.6867450475692749 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6894733905792236 Accuracy: 0.5234375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6946952939033508 Accuracy: 0.4453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  16\n",
            "Steps Completed:  0\n",
            "Loss: 0.6948097944259644 Accuracy: 0.5078125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6832908391952515 Accuracy: 0.5703125\n",
            "Steps Completed:  100\n",
            "Loss: 0.689587414264679 Accuracy: 0.546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6864959597587585 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  17\n",
            "Steps Completed:  0\n",
            "Loss: 0.691623330116272 Accuracy: 0.546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6845155954360962 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6868915557861328 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6948813199996948 Accuracy: 0.5390625\n",
            "-------------\n",
            "\n",
            "Epoch No:  18\n",
            "Steps Completed:  0\n",
            "Loss: 0.6784287095069885 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6913138628005981 Accuracy: 0.46875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6802389621734619 Accuracy: 0.5625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6977211236953735 Accuracy: 0.4375\n",
            "-------------\n",
            "\n",
            "Epoch No:  19\n",
            "Steps Completed:  0\n",
            "Loss: 0.6905947923660278 Accuracy: 0.4765625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6790931224822998 Accuracy: 0.5625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6726071834564209 Accuracy: 0.5546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6818754076957703 Accuracy: 0.46875\n",
            "-------------\n",
            "\n",
            "Epoch No:  20\n",
            "Steps Completed:  0\n",
            "Loss: 0.6729390621185303 Accuracy: 0.5625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6875730752944946 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6913844347000122 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.7047351598739624 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  21\n",
            "Steps Completed:  0\n",
            "Loss: 0.7465717792510986 Accuracy: 0.46875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6931878328323364 Accuracy: 0.4453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6960141062736511 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6788547039031982 Accuracy: 0.5234375\n",
            "-------------\n",
            "\n",
            "Epoch No:  22\n",
            "Steps Completed:  0\n",
            "Loss: 0.6835840940475464 Accuracy: 0.4765625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6918814182281494 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.674383282661438 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6833890676498413 Accuracy: 0.5546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  23\n",
            "Steps Completed:  0\n",
            "Loss: 0.6829054951667786 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6787573099136353 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6659526228904724 Accuracy: 0.5703125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6814069747924805 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  24\n",
            "Steps Completed:  0\n",
            "Loss: 0.6754723787307739 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6624303460121155 Accuracy: 0.578125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6658598780632019 Accuracy: 0.6015625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6677289605140686 Accuracy: 0.5703125\n",
            "-------------\n",
            "\n",
            "Epoch No:  25\n",
            "Steps Completed:  0\n",
            "Loss: 0.6835052967071533 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.68401038646698 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6464418768882751 Accuracy: 0.5703125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6591694355010986 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  26\n",
            "Steps Completed:  0\n",
            "Loss: 0.6885026097297668 Accuracy: 0.546875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6645923256874084 Accuracy: 0.59375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6904908418655396 Accuracy: 0.4765625\n",
            "Steps Completed:  150\n",
            "Loss: 0.7065625190734863 Accuracy: 0.4296875\n",
            "-------------\n",
            "\n",
            "Epoch No:  27\n",
            "Steps Completed:  0\n",
            "Loss: 0.6667781472206116 Accuracy: 0.5078125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6751317977905273 Accuracy: 0.546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6979342699050903 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6604052782058716 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  28\n",
            "Steps Completed:  0\n",
            "Loss: 0.6712331771850586 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6647506356239319 Accuracy: 0.546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.649803638458252 Accuracy: 0.5703125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6438575387001038 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  29\n",
            "Steps Completed:  0\n",
            "Loss: 0.6617770791053772 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6736299991607666 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6465813517570496 Accuracy: 0.5625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6420191526412964 Accuracy: 0.578125\n",
            "-------------\n",
            "\n",
            "Epoch No:  30\n",
            "Steps Completed:  0\n",
            "Loss: 0.6594953536987305 Accuracy: 0.4765625\n",
            "Steps Completed:  50\n",
            "Loss: 0.7210909724235535 Accuracy: 0.4921875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6681708097457886 Accuracy: 0.59375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6662375330924988 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  31\n",
            "Steps Completed:  0\n",
            "Loss: 0.679766833782196 Accuracy: 0.4453125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6584535837173462 Accuracy: 0.46875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6443846821784973 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6711093187332153 Accuracy: 0.4453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  32\n",
            "Steps Completed:  0\n",
            "Loss: 0.647388756275177 Accuracy: 0.5859375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6239281296730042 Accuracy: 0.640625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6755654215812683 Accuracy: 0.5078125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6697190999984741 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  33\n",
            "Steps Completed:  0\n",
            "Loss: 0.6635428071022034 Accuracy: 0.4296875\n",
            "Steps Completed:  50\n",
            "Loss: 0.6818914413452148 Accuracy: 0.4765625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6319710612297058 Accuracy: 0.5859375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6669565439224243 Accuracy: 0.5625\n",
            "-------------\n",
            "\n",
            "Epoch No:  34\n",
            "Steps Completed:  0\n",
            "Loss: 0.6625196933746338 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6722698211669922 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6689714193344116 Accuracy: 0.4921875\n",
            "Steps Completed:  150\n",
            "Loss: 0.638032078742981 Accuracy: 0.6015625\n",
            "-------------\n",
            "\n",
            "Epoch No:  35\n",
            "Steps Completed:  0\n",
            "Loss: 0.6636316776275635 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6827977895736694 Accuracy: 0.421875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6475929617881775 Accuracy: 0.59375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6807490587234497 Accuracy: 0.453125\n",
            "-------------\n",
            "\n",
            "Epoch No:  36\n",
            "Steps Completed:  0\n",
            "Loss: 0.651878833770752 Accuracy: 0.5625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6550564765930176 Accuracy: 0.5\n",
            "Steps Completed:  100\n",
            "Loss: 0.6342585682868958 Accuracy: 0.546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6570444107055664 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  37\n",
            "Steps Completed:  0\n",
            "Loss: 0.6281876564025879 Accuracy: 0.6015625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6329957246780396 Accuracy: 0.609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6584104299545288 Accuracy: 0.609375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6473525762557983 Accuracy: 0.515625\n",
            "-------------\n",
            "\n",
            "Epoch No:  38\n",
            "Steps Completed:  0\n",
            "Loss: 0.6493752002716064 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6487276554107666 Accuracy: 0.484375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6538717746734619 Accuracy: 0.5859375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6758322715759277 Accuracy: 0.421875\n",
            "-------------\n",
            "\n",
            "Epoch No:  39\n",
            "Steps Completed:  0\n",
            "Loss: 0.6392800807952881 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6439899802207947 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6158491373062134 Accuracy: 0.578125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6568650603294373 Accuracy: 0.578125\n",
            "-------------\n",
            "\n",
            "Working on test dataset\n",
            "Steps Completed:  0\n",
            "Steps Completed:  50\n",
            "Steps Completed:  100\n",
            "Steps Completed:  150\n",
            "Final Test Accuracy Average: 0.5069711804389954 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRMF4TOko9wg"
      },
      "source": [
        "## Model3\n",
        "\n",
        "1.   With 20 Epochs\n",
        "2.   The test accuracy is 50.2%\n",
        "3.   Batch Size of 128\n",
        "4.   Hidden layer neurons are 128\n",
        "5.   Max corpus length 40000\n",
        "6.   Max words 500, padding post, truncating pre\n",
        "7.   Weights are initialised from -0.2 to 0.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59-w4jNeo9wq",
        "outputId": "af2174c1-99c0-4028-c988-2bbdc5e0504a"
      },
      "source": [
        "train_loop(modelConfig3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch No:  0\n",
            "Steps Completed:  0\n",
            "Loss: 0.7625614404678345 Accuracy: 0.5078125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6968785524368286 Accuracy: 0.453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6926401853561401 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6837975382804871 Accuracy: 0.625\n",
            "-------------\n",
            "\n",
            "Epoch No:  1\n",
            "Steps Completed:  0\n",
            "Loss: 0.6936752200126648 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6903246641159058 Accuracy: 0.5234375\n",
            "Steps Completed:  100\n",
            "Loss: 0.7023606896400452 Accuracy: 0.46875\n",
            "Steps Completed:  150\n",
            "Loss: 0.7186465263366699 Accuracy: 0.4296875\n",
            "-------------\n",
            "\n",
            "Epoch No:  2\n",
            "Steps Completed:  0\n",
            "Loss: 0.6824567914009094 Accuracy: 0.59375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6933882236480713 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6863447427749634 Accuracy: 0.5234375\n",
            "Steps Completed:  150\n",
            "Loss: 0.704568088054657 Accuracy: 0.484375\n",
            "-------------\n",
            "\n",
            "Epoch No:  3\n",
            "Steps Completed:  0\n",
            "Loss: 0.6739521026611328 Accuracy: 0.59375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6993414163589478 Accuracy: 0.453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6897538900375366 Accuracy: 0.546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6805823445320129 Accuracy: 0.5390625\n",
            "-------------\n",
            "\n",
            "Epoch No:  4\n",
            "Steps Completed:  0\n",
            "Loss: 0.677291989326477 Accuracy: 0.5703125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6752027273178101 Accuracy: 0.5625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6859322190284729 Accuracy: 0.5390625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6864591240882874 Accuracy: 0.546875\n",
            "-------------\n",
            "\n",
            "Epoch No:  5\n",
            "Steps Completed:  0\n",
            "Loss: 0.673235297203064 Accuracy: 0.5859375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6851481199264526 Accuracy: 0.5390625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6957435607910156 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.7103928327560425 Accuracy: 0.4609375\n",
            "-------------\n",
            "\n",
            "Epoch No:  6\n",
            "Steps Completed:  0\n",
            "Loss: 0.7121118903160095 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6705667972564697 Accuracy: 0.5546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6906747817993164 Accuracy: 0.5546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.675923764705658 Accuracy: 0.6015625\n",
            "-------------\n",
            "\n",
            "Epoch No:  7\n",
            "Steps Completed:  0\n",
            "Loss: 0.6867736577987671 Accuracy: 0.484375\n",
            "Steps Completed:  50\n",
            "Loss: 0.680898129940033 Accuracy: 0.5234375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6889313459396362 Accuracy: 0.5234375\n",
            "Steps Completed:  150\n",
            "Loss: 0.676171600818634 Accuracy: 0.59375\n",
            "-------------\n",
            "\n",
            "Epoch No:  8\n",
            "Steps Completed:  0\n",
            "Loss: 0.7025389671325684 Accuracy: 0.4609375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6933951377868652 Accuracy: 0.453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6684936881065369 Accuracy: 0.5390625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6826215982437134 Accuracy: 0.4921875\n",
            "-------------\n",
            "\n",
            "Epoch No:  9\n",
            "Steps Completed:  0\n",
            "Loss: 0.6852145791053772 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.6777186393737793 Accuracy: 0.546875\n",
            "Steps Completed:  100\n",
            "Loss: 0.6874328851699829 Accuracy: 0.515625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6543234586715698 Accuracy: 0.6328125\n",
            "-------------\n",
            "\n",
            "Epoch No:  10\n",
            "Steps Completed:  0\n",
            "Loss: 0.6762974262237549 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6762714385986328 Accuracy: 0.5390625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6954240798950195 Accuracy: 0.484375\n",
            "Steps Completed:  150\n",
            "Loss: 0.6721462607383728 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  11\n",
            "Steps Completed:  0\n",
            "Loss: 0.6870684623718262 Accuracy: 0.5234375\n",
            "Steps Completed:  50\n",
            "Loss: 0.6725794076919556 Accuracy: 0.53125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6747759580612183 Accuracy: 0.4765625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6917638778686523 Accuracy: 0.4921875\n",
            "-------------\n",
            "\n",
            "Epoch No:  12\n",
            "Steps Completed:  0\n",
            "Loss: 0.675301194190979 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.6682358384132385 Accuracy: 0.4453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6821600198745728 Accuracy: 0.5\n",
            "Steps Completed:  150\n",
            "Loss: 0.6828330159187317 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Epoch No:  13\n",
            "Steps Completed:  0\n",
            "Loss: 0.7015039920806885 Accuracy: 0.5\n",
            "Steps Completed:  50\n",
            "Loss: 0.6782524585723877 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6873793601989746 Accuracy: 0.453125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6748563051223755 Accuracy: 0.5390625\n",
            "-------------\n",
            "\n",
            "Epoch No:  14\n",
            "Steps Completed:  0\n",
            "Loss: 0.6826949119567871 Accuracy: 0.515625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6759935021400452 Accuracy: 0.5078125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6735442280769348 Accuracy: 0.53125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6587771773338318 Accuracy: 0.5625\n",
            "-------------\n",
            "\n",
            "Epoch No:  15\n",
            "Steps Completed:  0\n",
            "Loss: 0.672133207321167 Accuracy: 0.5625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6847619414329529 Accuracy: 0.5390625\n",
            "Steps Completed:  100\n",
            "Loss: 0.6804699897766113 Accuracy: 0.5625\n",
            "Steps Completed:  150\n",
            "Loss: 0.6817440390586853 Accuracy: 0.5\n",
            "-------------\n",
            "\n",
            "Epoch No:  16\n",
            "Steps Completed:  0\n",
            "Loss: 0.6844315528869629 Accuracy: 0.5390625\n",
            "Steps Completed:  50\n",
            "Loss: 0.6756361126899719 Accuracy: 0.4609375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6991997361183167 Accuracy: 0.4296875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6702701449394226 Accuracy: 0.5390625\n",
            "-------------\n",
            "\n",
            "Epoch No:  17\n",
            "Steps Completed:  0\n",
            "Loss: 0.6631523370742798 Accuracy: 0.609375\n",
            "Steps Completed:  50\n",
            "Loss: 0.659622311592102 Accuracy: 0.5703125\n",
            "Steps Completed:  100\n",
            "Loss: 0.6686543226242065 Accuracy: 0.578125\n",
            "Steps Completed:  150\n",
            "Loss: 0.6638755798339844 Accuracy: 0.59375\n",
            "-------------\n",
            "\n",
            "Epoch No:  18\n",
            "Steps Completed:  0\n",
            "Loss: 0.6886003017425537 Accuracy: 0.53125\n",
            "Steps Completed:  50\n",
            "Loss: 0.718749463558197 Accuracy: 0.453125\n",
            "Steps Completed:  100\n",
            "Loss: 0.656849205493927 Accuracy: 0.6171875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6784762144088745 Accuracy: 0.4765625\n",
            "-------------\n",
            "\n",
            "Epoch No:  19\n",
            "Steps Completed:  0\n",
            "Loss: 0.6307874917984009 Accuracy: 0.578125\n",
            "Steps Completed:  50\n",
            "Loss: 0.6595213413238525 Accuracy: 0.59375\n",
            "Steps Completed:  100\n",
            "Loss: 0.6747157573699951 Accuracy: 0.546875\n",
            "Steps Completed:  150\n",
            "Loss: 0.6575079560279846 Accuracy: 0.53125\n",
            "-------------\n",
            "\n",
            "Working on test dataset\n",
            "Steps Completed:  0\n",
            "Steps Completed:  50\n",
            "Steps Completed:  100\n",
            "Steps Completed:  150\n",
            "Final Test Accuracy Average: 0.5024038553237915 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQgyOYrorhSn"
      },
      "source": [
        "# Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNdg7FvCrlzm"
      },
      "source": [
        "# Q1\n",
        "\n",
        "Question:\n",
        "\n",
        "\n",
        "\n",
        "*   In the notebook, this is done in a rather crude way: All sequences are padded to the length of the longest sequence in the dataset.\n",
        "*   Why is this wasteful? Can you think of a smarter padding scheme that is more efficient? Consider the fact that RNNs can work on arbitrary sequence lengths, and that training minibatches are pretty much independent of each other\n",
        "\n",
        "Answer: \n",
        "\n",
        "\n",
        "\n",
        "*   Because as per the histogram plot in the above plots, we can see that most of the sequence lengths are between 0-500 and if we pad to the max length then most of the neurons are initialised with dummy values which is wasteful for computation.\n",
        "*   Alternative way is to pad at batch level by taking the average length of sequences in that particular batch or truncate all sequences to some length like 500 and pad.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kc9VYx7s-99"
      },
      "source": [
        "# Q2\n",
        "\n",
        "Question: Between truncating long sequences and removing them, which option do you think is better? Why?\n",
        "\n",
        "Answer: Pre truncating and post-padding is bettter.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqWBsWgwtSOW"
      },
      "source": [
        "# Q3\n",
        "\n",
        "Question: Can you think of a way to avoid the one-hot vectors completely? Even if you cannot implement it, a conceptual idea is fine.\n",
        "\n",
        "Answer: Word embeddings.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2SIjpvvtggm"
      },
      "source": [
        "# Q4\n",
        "\n",
        "Question: How can it be that we can choose how many outputs we have, i.e. how can both be correct? Are there differences between both choices as well as (dis)advantages relative to each other?\n",
        "\n",
        "Answer: Word embeddings.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NQi7YVAuRka"
      },
      "source": [
        "# Q5\n",
        "\n",
        "Question: All sequences start with the same special “beginning of sequence” token (coded by index 1). Given this fact, is there a point in learning an initial state? Why (not)?\n",
        "\n",
        "Answer: Word embeddings.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95iiW9ZquR5w"
      },
      "source": [
        "# Q6\n",
        "\n",
        "Question: pad_sequences allows for pre or post padding. Try both to see the difference. Which option do you think is better? Recall that we use the final time step output from our model.\n",
        "\n",
        "Answer: Word embeddings.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPCEnYY5uSLU"
      },
      "source": [
        "# Q7\n",
        "\n",
        "Question: Can you think of a way to prevent the RNN from computing new states on padded time steps? One idea might be to “pass through” the previous state in case the current time step is padding. Note that, within a batch, some sequences might be padded for a given time step while others are not.\n",
        "\n",
        "Answer: Word embeddings.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW6u4DcOG2aZ"
      },
      "source": [
        "# References\n",
        "\n",
        "\n",
        "1.   https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e\n",
        "2.   List item\n",
        "\n"
      ]
    }
  ]
}