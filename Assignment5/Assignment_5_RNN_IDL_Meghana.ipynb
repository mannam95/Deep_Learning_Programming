{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_5_RNN_IDL_Meghana.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mannam95/Deep_Learning_Programming/blob/main/Assignment5/Assignment_5_RNN_IDL_Meghana.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCiXbg2NTH8O"
      },
      "source": [
        "#Team Assignment\n",
        "\n",
        "\n",
        "1.   Srinath Mannam (229750)\n",
        "2.   Meghana Rao (234907)\n",
        "3.   Govind Shukla (235192)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOGo_uKlQT6J"
      },
      "source": [
        "#### Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4P-6H7NMdmc"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H-hSh5rQXwf"
      },
      "source": [
        "#### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hptG5KRgNIfX"
      },
      "source": [
        "vocab_size = 40000\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words= vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WooanbofNv8g",
        "outputId": "98d53cc4-7212-4506-f113-4c807941bf50"
      },
      "source": [
        "print(train_sequences[:4])\n",
        "print(train_labels[:4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
            " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
            " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
            " list([1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 15305, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 2, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 33929, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 23005, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 2, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 18435, 192, 5, 1219, 3890, 19, 20523, 217, 4122, 1710, 537, 20341, 1236, 5, 736, 10, 10, 61, 403, 9, 2, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 18665, 594, 7, 5168, 94, 9096, 3987, 15242, 11, 28280, 4, 538, 7, 1795, 246, 2, 9, 10161, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 10197, 19, 49, 7, 4, 1885, 13699, 1118, 25, 80, 126, 842, 10, 10, 2, 18223, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 16376, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 25837, 21, 29, 9, 2841, 23, 4, 1010, 26747, 793, 6, 13699, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 16376, 8, 124, 4, 882, 4, 882, 496, 27, 33029, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 11120, 1311, 8, 4, 23643, 7, 31, 7, 29851, 91, 22793, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 11876, 1775, 3353, 12716, 1846, 4, 11286, 7, 154, 5, 4, 518, 53, 13243, 11286, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 18223, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 33058, 4, 22793, 10359, 9, 242, 4, 91, 1202, 11377, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 18223, 13, 188, 1076, 3222, 19, 4, 13465, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 18223, 13, 33195, 14, 280, 13, 219, 4, 2, 431, 758, 859, 4, 953, 1052, 12283, 7, 5991, 5, 94, 40, 25, 238, 60, 35410, 4, 15812, 804, 27767, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 22148, 23, 12, 1685, 195, 25, 238, 60, 796, 13713, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574])]\n",
            "[1 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjbWaH8QQelv"
      },
      "source": [
        "#### Index and Word mappings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQnYvXqEOG_q"
      },
      "source": [
        "word_to_index = tf.keras.datasets.imdb.get_word_index()\n",
        "index_to_word = dict((index, word) for (word, index) in word_to_index.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "T8fLusDiOTsp",
        "outputId": "5b0efbe9-8278-494a-939d-7c24cf96de7a"
      },
      "source": [
        "sequence_lengths = [len(sequence) for sequence in train_sequences]\n",
        "max_len = max(sequence_lengths)\n",
        "min_len = min(sequence_lengths)\n",
        "print(\"Max sequence length:\", max_len)\n",
        "print(\"Min sequence length:\",min_len)\n",
        "\n",
        "plt.hist(sequence_lengths, bins=100)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sequence length: 2494\n",
            "Min sequence length: 11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3mr1rYDQz8N"
      },
      "source": [
        "#### Padding sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw4NlLihQplT"
      },
      "source": [
        "max_words_length = 300\n",
        "\n",
        "train_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_words_length, padding = \"pre\", truncating = \"pre\")\n",
        "test_sequences = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=max_words_length, padding = \"pre\", truncating = \"pre\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOSpvDmzQ7uV"
      },
      "source": [
        "#### Train and Test data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhSA8fTSQy5x"
      },
      "source": [
        "train_labels = train_labels.reshape(-1).astype(np.int32)\n",
        "test_labels = test_labels.reshape(-1).astype(np.int32)\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences, train_labels)).shuffle(25000).batch(128)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sequences, test_labels)).batch(128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWx3O8HNRALJ"
      },
      "source": [
        "#### Equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjZiVGp4TbGz"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlkGXM4eTx62"
      },
      "source": [
        "#### **RNN Loop**\n",
        "###### *x(t)_size: batch_size x (1 x vocab_size) -> eg: 128 x (1 x 20000)*\n",
        "\n",
        "###### U_input_to_hidden:  vocab_size x no_of_hidden_units -> eg: 20000 x 64\n",
        "###### W_hidden_to_hidden: no_of_hidden_units x no_of_hidden_units -> eg: 64 x 64\n",
        "###### b_bias: 1 x no_of_hidden_units -> eg: 1 x 64\n",
        "\n",
        "###### *a(t)_size = h(t)_size = 1 x  no_of_hidden_units -> eg: 1 x 64*\n",
        "\n",
        "###### V_hidden_to_output: no_of_hidden_units x no_of_output_units -> eg: 64 x 2\n",
        "###### c_bias: 1 x no_of_output_units -> eg: 1 x 2\n",
        "\n",
        "###### *o(t)_size: 1 x no_of_output_units -> eg : 1 x 2*\n",
        "###### *y(t)_size: 1*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNjXJ_YPkzWi"
      },
      "source": [
        "def rnn_loop(sequences, model):\n",
        "  old_state = model.prev_h\n",
        "  #sequences_one_hot = tf.one_hot(sequences, depth = vocab_size)\n",
        "  for step in range(max_words_length):\n",
        "    #x_t = sequences_one_hot[:,step]\n",
        "    x_t = sequences[:, step]\n",
        "    x_t = tf.one_hot(x_t, depth=vocab_size)\n",
        "    a_t = model.b + tf.matmul(old_state, model.W_h_h) + tf.matmul(x_t, model.U_x_h)\n",
        "    new_state = tf.nn.tanh(a_t)\n",
        "    old_state = new_state\n",
        "\n",
        "  o_t = model.c + tf.matmul(new_state, model.V_h_o)\n",
        "\n",
        "  return o_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_4CJ090R87O"
      },
      "source": [
        "#### Model Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSCYd73NboKN"
      },
      "source": [
        "def start_model_train(train_data, model):\n",
        "  print(\"-----------Model train-------------\")\n",
        "  optimizer = model.optimizer\n",
        "  U_x_h, W_h_h, b, V_h_o, c = model.U_x_h, model.W_h_h, model.b, model.V_h_o, model.c\n",
        "  for num_of_epochs in range(model.epochs):\n",
        "    print(\"Epoch:\", num_of_epochs)\n",
        "    for step, (sequence_batch, label_batch) in enumerate(train_data):\n",
        "      with tf.GradientTape() as tape:\n",
        "        logits = rnn_loop(sequence_batch, model)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label_batch))\n",
        "     \n",
        "      grads = tape.gradient(xent, [U_x_h, W_h_h, b, V_h_o, c])\n",
        "      optimizer.apply_gradients(zip(grads, [U_x_h, W_h_h, b, V_h_o, c]))\n",
        "     \n",
        "      if not step % 50:\n",
        "        print(\"Step: \", step)\n",
        "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch), tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "  \n",
        "  return U_x_h, W_h_h, b, V_h_o, c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAgxmYjESAO8"
      },
      "source": [
        "#### Model Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzc2L4OwhpzB"
      },
      "source": [
        "def start_model_test(test_data, model, U_x_h, W_h_h, b, V_h_o, c):\n",
        "  print(\"-----------Model test-------------\")\n",
        "  test_accuracy = []\n",
        "  for step, (sequence_batch, label_batch) in enumerate(test_data):\n",
        "    logits = rnn_loop(sequence_batch, model)\n",
        "    preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "    acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch), tf.float32))\n",
        "    test_accuracy.append(acc)\n",
        "\n",
        "  print(\"Final Test Accuracy: {}\".format(sum(test_accuracy)/len(test_accuracy)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS7SD6wBSVzm"
      },
      "source": [
        "class Model_1:\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "     super().__init__(**kwargs)\n",
        "     self.U_x_h = tf.Variable(tf.random.uniform([vocab_size, 64],minval=-0.1,maxval=0.1, dtype=np.float32))\n",
        "     self.W_h_h = tf.Variable(tf.random.uniform([64, 64], minval=-0.1, maxval=0.1, dtype=np.float32))\n",
        "     self.b = tf.Variable(tf.zeros([1, 64],tf.float32))\n",
        "     self.V_h_o = tf.Variable(tf.random.uniform([64, 2],minval=-0.1,maxval=0.1, dtype=np.float32))\n",
        "     self.c = tf.Variable(tf.zeros([1, 2],tf.float32))\n",
        "\n",
        "     self.prev_h = tf.Variable(tf.zeros([1, 64],tf.float32))\n",
        "\n",
        "     self.optimizer = tf.keras.optimizers.Adam()\n",
        "     self.epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF71rvav0QxZ"
      },
      "source": [
        "class Model_2:\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "     super().__init__(**kwargs)\n",
        "     self.U_x_h = tf.Variable(tf.random.uniform([vocab_size, 64], minval=-0.1, maxval=0.1, dtype=np.float32))\n",
        "     self.W_h_h = tf.Variable(tf.random.uniform([64, 64], minval=-0.1, maxval=0.1, dtype=np.float32))\n",
        "     self.b = tf.Variable(tf.zeros([1, 64],tf.float32))\n",
        "     self.V_h_o = tf.Variable(tf.random.uniform([64, 1], minval=-0.1, maxval=0.1, dtype=np.float32))\n",
        "     self.c = tf.Variable(tf.zeros([1, 1],tf.float32))\n",
        "\n",
        "     self.prev_h = tf.Variable(tf.zeros([1, 64],tf.float32))\n",
        "\n",
        "     self.optimizer = tf.keras.optimizers.Adam()\n",
        "     self.epochs = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaydjtqBAeIr"
      },
      "source": [
        "class Model_3:\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "     super().__init__(**kwargs)\n",
        "     self.U_x_h = tf.Variable(tf.random.uniform([vocab_size, 128],minval=-0.1,maxval=0.1, dtype=np.float32))\n",
        "     self.W_h_h = tf.Variable(tf.random.uniform([128, 128], minval=-0.1, maxval=0.1, dtype=np.float32))\n",
        "     self.b = tf.Variable(tf.zeros([1, 128],tf.float32))\n",
        "     self.V_h_o = tf.Variable(tf.random.uniform([128, 2],minval=-0.1,maxval=0.1, dtype=np.float32))\n",
        "     self.c = tf.Variable(tf.zeros([1, 2],tf.float32))\n",
        "\n",
        "     self.prev_h = tf.Variable(tf.zeros([1, 128],tf.float32))\n",
        "\n",
        "     self.optimizer = tf.keras.optimizers.Adam()\n",
        "     self.epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmh8coohOKQX"
      },
      "source": [
        "### **Model 1**\n",
        "##### - Hidden units size: 64\n",
        "##### - Output unit size: 2\n",
        "##### - Output activation function: Softmax\n",
        "##### - Epochs: 10\n",
        "##### - Vocab_size: 20000\n",
        "##### - Max words length: 300\n",
        "##### - Padding: post\n",
        "##### - One hot encoding for every batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUrOKvc3pA-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5645d4d-5f89-4cfc-8700-faeaa1928e5d"
      },
      "source": [
        "model_1 = Model_1()\n",
        "U_x_h, W_h_h, b, V_h_o, c = start_model_train(train_data, model_1)\n",
        "start_model_test(test_data, model_1, U_x_h, W_h_h, b, V_h_o, c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------Model train-------------\n",
            "Epoch: 0\n",
            "Step:  0\n",
            "Loss: 0.696983814239502 Accuracy: 0.4609375\n",
            "Step:  50\n",
            "Loss: 0.6933837532997131 Accuracy: 0.4765625\n",
            "Step:  100\n",
            "Loss: 0.6923089027404785 Accuracy: 0.5\n",
            "Step:  150\n",
            "Loss: 0.6912754774093628 Accuracy: 0.5078125\n",
            "Epoch: 1\n",
            "Step:  0\n",
            "Loss: 0.6713583469390869 Accuracy: 0.53125\n",
            "Step:  50\n",
            "Loss: 0.6589031219482422 Accuracy: 0.59375\n",
            "Step:  100\n",
            "Loss: 0.6297928690910339 Accuracy: 0.6015625\n",
            "Step:  150\n",
            "Loss: 0.6451587677001953 Accuracy: 0.5625\n",
            "Epoch: 2\n",
            "Step:  0\n",
            "Loss: 0.6143935918807983 Accuracy: 0.625\n",
            "Step:  50\n",
            "Loss: 0.611807107925415 Accuracy: 0.6015625\n",
            "Step:  100\n",
            "Loss: 0.6169560551643372 Accuracy: 0.484375\n",
            "Step:  150\n",
            "Loss: 0.604966402053833 Accuracy: 0.6484375\n",
            "Epoch: 3\n",
            "Step:  0\n",
            "Loss: 0.6075243353843689 Accuracy: 0.5703125\n",
            "Step:  50\n",
            "Loss: 0.5892278552055359 Accuracy: 0.59375\n",
            "Step:  100\n",
            "Loss: 0.5346689224243164 Accuracy: 0.6875\n",
            "Step:  150\n",
            "Loss: 0.5922883749008179 Accuracy: 0.609375\n",
            "Epoch: 4\n",
            "Step:  0\n",
            "Loss: 0.5623510479927063 Accuracy: 0.6796875\n",
            "Step:  50\n",
            "Loss: 0.5378588438034058 Accuracy: 0.625\n",
            "Step:  100\n",
            "Loss: 0.5270135998725891 Accuracy: 0.6484375\n",
            "Step:  150\n",
            "Loss: 0.5401636362075806 Accuracy: 0.6015625\n",
            "Epoch: 5\n",
            "Step:  0\n",
            "Loss: 0.5226103663444519 Accuracy: 0.7109375\n",
            "Step:  50\n",
            "Loss: 0.4981992840766907 Accuracy: 0.671875\n",
            "Step:  100\n",
            "Loss: 0.5373651385307312 Accuracy: 0.640625\n",
            "Step:  150\n",
            "Loss: 0.5226879119873047 Accuracy: 0.609375\n",
            "Epoch: 6\n",
            "Step:  0\n",
            "Loss: 0.5176438093185425 Accuracy: 0.6484375\n",
            "Step:  50\n",
            "Loss: 0.5212116241455078 Accuracy: 0.6328125\n",
            "Step:  100\n",
            "Loss: 0.5435708165168762 Accuracy: 0.625\n",
            "Step:  150\n",
            "Loss: 0.5541906356811523 Accuracy: 0.5546875\n",
            "Epoch: 7\n",
            "Step:  0\n",
            "Loss: 0.5660772323608398 Accuracy: 0.6171875\n",
            "Step:  50\n",
            "Loss: 0.5406034588813782 Accuracy: 0.71875\n",
            "Step:  100\n",
            "Loss: 0.5401220917701721 Accuracy: 0.625\n",
            "Step:  150\n",
            "Loss: 0.5422041416168213 Accuracy: 0.6484375\n",
            "Epoch: 8\n",
            "Step:  0\n",
            "Loss: 0.5145259499549866 Accuracy: 0.640625\n",
            "Step:  50\n",
            "Loss: 0.5286413431167603 Accuracy: 0.6015625\n",
            "Step:  100\n",
            "Loss: 0.5495870113372803 Accuracy: 0.6015625\n",
            "Step:  150\n",
            "Loss: 0.5063948035240173 Accuracy: 0.6015625\n",
            "Epoch: 9\n",
            "Step:  0\n",
            "Loss: 0.4974472224712372 Accuracy: 0.640625\n",
            "Step:  50\n",
            "Loss: 0.5329474806785583 Accuracy: 0.6171875\n",
            "Step:  100\n",
            "Loss: 0.53254234790802 Accuracy: 0.5703125\n",
            "Step:  150\n",
            "Loss: 0.5509268045425415 Accuracy: 0.5859375\n",
            "-----------Model test-------------\n",
            "Final Test Accuracy: 0.5145009756088257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsat5SkASQPx"
      },
      "source": [
        "### **Model 2**\n",
        "##### - Hidden units size: 64\n",
        "##### - Output unit size: 1\n",
        "##### - Output activation function: Sigmoid\n",
        "##### - Epochs: 30\n",
        "##### - Vocab_size: 20000\n",
        "##### - Max words length: 300\n",
        "##### - Padding: post\n",
        "##### - One hot encoding for every batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYacbqRrchgf",
        "outputId": "7e1a5406-f098-4288-cfc8-282b71b4e800"
      },
      "source": [
        "model_2 = Model_2()\n",
        "U_x_h, W_h_h, b, V_h_o, c = start_model_train(train_data, model_2)\n",
        "start_model_test(test_data, model_2, U_x_h, W_h_h, b, V_h_o, c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------Model train-------------\n",
            "Epoch: 0\n",
            "Step:  0\n",
            "Loss: 0.6924635171890259 Accuracy: 0.4765625\n",
            "Step:  50\n",
            "Loss: 0.6921123266220093 Accuracy: 0.578125\n",
            "Step:  100\n",
            "Loss: 0.6881871223449707 Accuracy: 0.5\n",
            "Step:  150\n",
            "Loss: 0.6972715258598328 Accuracy: 0.4921875\n",
            "Epoch: 1\n",
            "Step:  0\n",
            "Loss: 0.6837220788002014 Accuracy: 0.453125\n",
            "Step:  50\n",
            "Loss: 0.6536791324615479 Accuracy: 0.484375\n",
            "Step:  100\n",
            "Loss: 0.6601487398147583 Accuracy: 0.5\n",
            "Step:  150\n",
            "Loss: 0.6848399639129639 Accuracy: 0.46875\n",
            "Epoch: 2\n",
            "Step:  0\n",
            "Loss: 0.6229733228683472 Accuracy: 0.46875\n",
            "Step:  50\n",
            "Loss: 0.6111183166503906 Accuracy: 0.484375\n",
            "Step:  100\n",
            "Loss: 0.6328283548355103 Accuracy: 0.453125\n",
            "Step:  150\n",
            "Loss: 0.5959903001785278 Accuracy: 0.4609375\n",
            "Epoch: 3\n",
            "Step:  0\n",
            "Loss: 0.5918513536453247 Accuracy: 0.4296875\n",
            "Step:  50\n",
            "Loss: 0.6068836450576782 Accuracy: 0.5234375\n",
            "Step:  100\n",
            "Loss: 0.6558130383491516 Accuracy: 0.5234375\n",
            "Step:  150\n",
            "Loss: 0.6062442064285278 Accuracy: 0.53125\n",
            "Epoch: 4\n",
            "Step:  0\n",
            "Loss: 0.5590101480484009 Accuracy: 0.53125\n",
            "Step:  50\n",
            "Loss: 0.5323888063430786 Accuracy: 0.46875\n",
            "Step:  100\n",
            "Loss: 0.5820302367210388 Accuracy: 0.5390625\n",
            "Step:  150\n",
            "Loss: 0.5279157757759094 Accuracy: 0.4609375\n",
            "Epoch: 5\n",
            "Step:  0\n",
            "Loss: 0.5466057658195496 Accuracy: 0.546875\n",
            "Step:  50\n",
            "Loss: 0.5648609399795532 Accuracy: 0.53125\n",
            "Step:  100\n",
            "Loss: 1.629441499710083 Accuracy: 0.5234375\n",
            "Step:  150\n",
            "Loss: 0.5847364664077759 Accuracy: 0.578125\n",
            "Epoch: 6\n",
            "Step:  0\n",
            "Loss: 0.5307761430740356 Accuracy: 0.5859375\n",
            "Step:  50\n",
            "Loss: 0.5511035919189453 Accuracy: 0.453125\n",
            "Step:  100\n",
            "Loss: 0.5515199303627014 Accuracy: 0.484375\n",
            "Step:  150\n",
            "Loss: 0.5654891133308411 Accuracy: 0.5234375\n",
            "Epoch: 7\n",
            "Step:  0\n",
            "Loss: 0.5196324586868286 Accuracy: 0.546875\n",
            "Step:  50\n",
            "Loss: 0.49173203110694885 Accuracy: 0.53125\n",
            "Step:  100\n",
            "Loss: 0.5014947652816772 Accuracy: 0.5625\n",
            "Step:  150\n",
            "Loss: 0.5536807775497437 Accuracy: 0.578125\n",
            "Epoch: 8\n",
            "Step:  0\n",
            "Loss: 0.5407851338386536 Accuracy: 0.4609375\n",
            "Step:  50\n",
            "Loss: 0.5680181980133057 Accuracy: 0.421875\n",
            "Step:  100\n",
            "Loss: 0.4943501949310303 Accuracy: 0.4921875\n",
            "Step:  150\n",
            "Loss: 0.5267218947410583 Accuracy: 0.4921875\n",
            "Epoch: 9\n",
            "Step:  0\n",
            "Loss: 0.5430405139923096 Accuracy: 0.4453125\n",
            "Step:  50\n",
            "Loss: 0.512816846370697 Accuracy: 0.5078125\n",
            "Step:  100\n",
            "Loss: 0.5657340884208679 Accuracy: 0.484375\n",
            "Step:  150\n",
            "Loss: 0.5476051568984985 Accuracy: 0.5\n",
            "Epoch: 10\n",
            "Step:  0\n",
            "Loss: 0.5248967409133911 Accuracy: 0.484375\n",
            "Step:  50\n",
            "Loss: 0.534853458404541 Accuracy: 0.4921875\n",
            "Step:  100\n",
            "Loss: 0.565336287021637 Accuracy: 0.4296875\n",
            "Step:  150\n",
            "Loss: 0.5670256614685059 Accuracy: 0.4921875\n",
            "Epoch: 11\n",
            "Step:  0\n",
            "Loss: 0.5263446569442749 Accuracy: 0.421875\n",
            "Step:  50\n",
            "Loss: 0.504483699798584 Accuracy: 0.4765625\n",
            "Step:  100\n",
            "Loss: 0.47177061438560486 Accuracy: 0.484375\n",
            "Step:  150\n",
            "Loss: 0.5654213428497314 Accuracy: 0.4921875\n",
            "Epoch: 12\n",
            "Step:  0\n",
            "Loss: 0.5046749114990234 Accuracy: 0.5234375\n",
            "Step:  50\n",
            "Loss: 0.5447728633880615 Accuracy: 0.5390625\n",
            "Step:  100\n",
            "Loss: 0.47163382172584534 Accuracy: 0.4765625\n",
            "Step:  150\n",
            "Loss: 0.5460460186004639 Accuracy: 0.4375\n",
            "Epoch: 13\n",
            "Step:  0\n",
            "Loss: 0.5164186954498291 Accuracy: 0.4921875\n",
            "Step:  50\n",
            "Loss: 0.49672800302505493 Accuracy: 0.5234375\n",
            "Step:  100\n",
            "Loss: 0.505154013633728 Accuracy: 0.5\n",
            "Step:  150\n",
            "Loss: 0.5489004254341125 Accuracy: 0.5\n",
            "Epoch: 14\n",
            "Step:  0\n",
            "Loss: 0.4851865768432617 Accuracy: 0.546875\n",
            "Step:  50\n",
            "Loss: 0.5104547142982483 Accuracy: 0.4453125\n",
            "Step:  100\n",
            "Loss: 0.516916036605835 Accuracy: 0.4609375\n",
            "Step:  150\n",
            "Loss: 0.5928465127944946 Accuracy: 0.4765625\n",
            "Epoch: 15\n",
            "Step:  0\n",
            "Loss: 0.469886839389801 Accuracy: 0.4921875\n",
            "Step:  50\n",
            "Loss: 0.49706095457077026 Accuracy: 0.484375\n",
            "Step:  100\n",
            "Loss: 0.4925573766231537 Accuracy: 0.4609375\n",
            "Step:  150\n",
            "Loss: 0.602941632270813 Accuracy: 0.4296875\n",
            "Epoch: 16\n",
            "Step:  0\n",
            "Loss: 0.5414274334907532 Accuracy: 0.4609375\n",
            "Step:  50\n",
            "Loss: 0.49931541085243225 Accuracy: 0.4765625\n",
            "Step:  100\n",
            "Loss: 0.5040978193283081 Accuracy: 0.4609375\n",
            "Step:  150\n",
            "Loss: 0.5305447578430176 Accuracy: 0.5078125\n",
            "Epoch: 17\n",
            "Step:  0\n",
            "Loss: 0.5013631582260132 Accuracy: 0.5703125\n",
            "Step:  50\n",
            "Loss: 0.4962398111820221 Accuracy: 0.4453125\n",
            "Step:  100\n",
            "Loss: 0.5076199769973755 Accuracy: 0.5\n",
            "Step:  150\n",
            "Loss: 0.5190792679786682 Accuracy: 0.5\n",
            "Epoch: 18\n",
            "Step:  0\n",
            "Loss: 0.5833267569541931 Accuracy: 0.46875\n",
            "Step:  50\n",
            "Loss: 0.5147725343704224 Accuracy: 0.53125\n",
            "Step:  100\n",
            "Loss: 0.5223361849784851 Accuracy: 0.546875\n",
            "Step:  150\n",
            "Loss: 0.5040830969810486 Accuracy: 0.53125\n",
            "Epoch: 19\n",
            "Step:  0\n",
            "Loss: 0.5053558945655823 Accuracy: 0.46875\n",
            "Step:  50\n",
            "Loss: 0.5162626504898071 Accuracy: 0.4453125\n",
            "Step:  100\n",
            "Loss: 0.514565646648407 Accuracy: 0.5625\n",
            "Step:  150\n",
            "Loss: 0.46009618043899536 Accuracy: 0.5546875\n",
            "Epoch: 20\n",
            "Step:  0\n",
            "Loss: 0.5850069522857666 Accuracy: 0.3984375\n",
            "Step:  50\n",
            "Loss: 0.5155037641525269 Accuracy: 0.5234375\n",
            "Step:  100\n",
            "Loss: 0.5383524894714355 Accuracy: 0.46875\n",
            "Step:  150\n",
            "Loss: 0.5618994832038879 Accuracy: 0.4765625\n",
            "Epoch: 21\n",
            "Step:  0\n",
            "Loss: 0.5143503546714783 Accuracy: 0.5\n",
            "Step:  50\n",
            "Loss: 0.4720340371131897 Accuracy: 0.5234375\n",
            "Step:  100\n",
            "Loss: 0.4661332368850708 Accuracy: 0.5390625\n",
            "Step:  150\n",
            "Loss: 0.5070241689682007 Accuracy: 0.453125\n",
            "Epoch: 22\n",
            "Step:  0\n",
            "Loss: 0.4964328110218048 Accuracy: 0.46875\n",
            "Step:  50\n",
            "Loss: 0.5219259262084961 Accuracy: 0.546875\n",
            "Step:  100\n",
            "Loss: 0.47425174713134766 Accuracy: 0.578125\n",
            "Step:  150\n",
            "Loss: 0.47428011894226074 Accuracy: 0.421875\n",
            "Epoch: 23\n",
            "Step:  0\n",
            "Loss: 0.5448287129402161 Accuracy: 0.5703125\n",
            "Step:  50\n",
            "Loss: 0.533855676651001 Accuracy: 0.4453125\n",
            "Step:  100\n",
            "Loss: 0.5471547245979309 Accuracy: 0.40625\n",
            "Step:  150\n",
            "Loss: 0.539353609085083 Accuracy: 0.484375\n",
            "Epoch: 24\n",
            "Step:  0\n",
            "Loss: 0.5152745246887207 Accuracy: 0.5546875\n",
            "Step:  50\n",
            "Loss: 0.5011721849441528 Accuracy: 0.5234375\n",
            "Step:  100\n",
            "Loss: 0.4966772496700287 Accuracy: 0.484375\n",
            "Step:  150\n",
            "Loss: 0.52278071641922 Accuracy: 0.4765625\n",
            "Epoch: 25\n",
            "Step:  0\n",
            "Loss: 0.49152207374572754 Accuracy: 0.5078125\n",
            "Step:  50\n",
            "Loss: 0.4832169711589813 Accuracy: 0.4765625\n",
            "Step:  100\n",
            "Loss: 0.4994884431362152 Accuracy: 0.5625\n",
            "Step:  150\n",
            "Loss: 0.5581983923912048 Accuracy: 0.4765625\n",
            "Epoch: 26\n",
            "Step:  0\n",
            "Loss: 0.5071106553077698 Accuracy: 0.4609375\n",
            "Step:  50\n",
            "Loss: 0.5115350484848022 Accuracy: 0.515625\n",
            "Step:  100\n",
            "Loss: 0.5449097156524658 Accuracy: 0.4609375\n",
            "Step:  150\n",
            "Loss: 0.5064964294433594 Accuracy: 0.5625\n",
            "Epoch: 27\n",
            "Step:  0\n",
            "Loss: 0.5222105979919434 Accuracy: 0.421875\n",
            "Step:  50\n",
            "Loss: 0.6451753377914429 Accuracy: 0.609375\n",
            "Step:  100\n",
            "Loss: 0.507439136505127 Accuracy: 0.46875\n",
            "Step:  150\n",
            "Loss: 0.4972434639930725 Accuracy: 0.484375\n",
            "Epoch: 28\n",
            "Step:  0\n",
            "Loss: 0.5423702001571655 Accuracy: 0.53125\n",
            "Step:  50\n",
            "Loss: 0.5210063457489014 Accuracy: 0.53125\n",
            "Step:  100\n",
            "Loss: 0.5239052772521973 Accuracy: 0.4609375\n",
            "Step:  150\n",
            "Loss: 0.5177950859069824 Accuracy: 0.5546875\n",
            "Epoch: 29\n",
            "Step:  0\n",
            "Loss: 0.4539172947406769 Accuracy: 0.4921875\n",
            "Step:  50\n",
            "Loss: 0.4959373474121094 Accuracy: 0.5078125\n",
            "Step:  100\n",
            "Loss: 0.5263689160346985 Accuracy: 0.4765625\n",
            "Step:  150\n",
            "Loss: 0.5155127644538879 Accuracy: 0.484375\n",
            "-----------Model test-------------\n",
            "Final Test Accuracy: 0.4997369349002838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbnFN2KTPyyu"
      },
      "source": [
        "### **Model 3**\n",
        "##### - Hidden units size: 128\n",
        "##### - Output unit size: 2\n",
        "##### - Output activation function: Softmax\n",
        "##### - Epochs: 20\n",
        "##### - Vocab_size: 40000\n",
        "##### - Max words length: 300\n",
        "##### - Padding: pre\n",
        "##### - One hot encoding for every step ( cannot be batch level due to OOM error for 40000 vocab size), comparitively slow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0GWnitKAaWz",
        "outputId": "13b7d268-a04b-47ef-a2ac-95bccffb8c28"
      },
      "source": [
        "model_3 = Model_3()\n",
        "U_x_h, W_h_h, b, V_h_o, c = start_model_train(train_data, model_3)\n",
        "start_model_test(test_data, model_3, U_x_h, W_h_h, b, V_h_o, c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------Model train-------------\n",
            "Epoch: 0\n",
            "Step:  0\n",
            "Loss: 0.693466067314148 Accuracy: 0.4765625\n",
            "Step:  50\n",
            "Loss: 0.6623430252075195 Accuracy: 0.6328125\n",
            "Step:  100\n",
            "Loss: 0.5894943475723267 Accuracy: 0.6953125\n",
            "Step:  150\n",
            "Loss: 0.5674940347671509 Accuracy: 0.6796875\n",
            "Epoch: 1\n",
            "Step:  0\n",
            "Loss: 0.5117217302322388 Accuracy: 0.78125\n",
            "Step:  50\n",
            "Loss: 0.3901035189628601 Accuracy: 0.8515625\n",
            "Step:  100\n",
            "Loss: 0.3111390173435211 Accuracy: 0.859375\n",
            "Step:  150\n",
            "Loss: 0.35048192739486694 Accuracy: 0.8515625\n",
            "Epoch: 2\n",
            "Step:  0\n",
            "Loss: 0.22653040289878845 Accuracy: 0.890625\n",
            "Step:  50\n",
            "Loss: 0.18994292616844177 Accuracy: 0.921875\n",
            "Step:  100\n",
            "Loss: 0.1192915067076683 Accuracy: 0.953125\n",
            "Step:  150\n",
            "Loss: 0.21812835335731506 Accuracy: 0.9140625\n",
            "Epoch: 3\n",
            "Step:  0\n",
            "Loss: 0.06736163794994354 Accuracy: 0.984375\n",
            "Step:  50\n",
            "Loss: 0.12817560136318207 Accuracy: 0.953125\n",
            "Step:  100\n",
            "Loss: 0.024618934839963913 Accuracy: 0.9921875\n",
            "Step:  150\n",
            "Loss: 0.22736451029777527 Accuracy: 0.921875\n",
            "Epoch: 4\n",
            "Step:  0\n",
            "Loss: 0.031235460191965103 Accuracy: 0.9921875\n",
            "Step:  50\n",
            "Loss: 0.017026184126734734 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.01025500800460577 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 0.0472177155315876 Accuracy: 0.984375\n",
            "Epoch: 5\n",
            "Step:  0\n",
            "Loss: 0.007061968557536602 Accuracy: 1.0\n",
            "Step:  50\n",
            "Loss: 0.0034290244802832603 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.04932970926165581 Accuracy: 0.984375\n",
            "Step:  150\n",
            "Loss: 0.005823432933539152 Accuracy: 1.0\n",
            "Epoch: 6\n",
            "Step:  0\n",
            "Loss: 0.022403022274374962 Accuracy: 0.9921875\n",
            "Step:  50\n",
            "Loss: 0.006775998510420322 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.0013470540288835764 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 0.03786003589630127 Accuracy: 0.9921875\n",
            "Epoch: 7\n",
            "Step:  0\n",
            "Loss: 0.04425109177827835 Accuracy: 0.9921875\n",
            "Step:  50\n",
            "Loss: 0.004663215484470129 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.006765596568584442 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 0.0047592893242836 Accuracy: 1.0\n",
            "Epoch: 8\n",
            "Step:  0\n",
            "Loss: 0.004661228973418474 Accuracy: 1.0\n",
            "Step:  50\n",
            "Loss: 0.00025327777257189155 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.0007226158631965518 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 0.0006656465120613575 Accuracy: 1.0\n",
            "Epoch: 9\n",
            "Step:  0\n",
            "Loss: 0.015626179054379463 Accuracy: 0.9921875\n",
            "Step:  50\n",
            "Loss: 0.09515060484409332 Accuracy: 0.9609375\n",
            "Step:  100\n",
            "Loss: 0.02011660858988762 Accuracy: 0.9921875\n",
            "Step:  150\n",
            "Loss: 0.0017766677774488926 Accuracy: 1.0\n",
            "Epoch: 10\n",
            "Step:  0\n",
            "Loss: 0.014148270711302757 Accuracy: 0.9921875\n",
            "Step:  50\n",
            "Loss: 0.004542081151157618 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.003610112238675356 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 0.0015199724584817886 Accuracy: 1.0\n",
            "Epoch: 11\n",
            "Step:  0\n",
            "Loss: 0.0006836705724708736 Accuracy: 1.0\n",
            "Step:  50\n",
            "Loss: 0.00044962711399421096 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.0005964422598481178 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 0.005948932375758886 Accuracy: 1.0\n",
            "Epoch: 12\n",
            "Step:  0\n",
            "Loss: 0.0035294326953589916 Accuracy: 1.0\n",
            "Step:  50\n",
            "Loss: 0.0004260065616108477 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.001196668017655611 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 0.07228529453277588 Accuracy: 0.9765625\n",
            "Epoch: 13\n",
            "Step:  0\n",
            "Loss: 0.006585113238543272 Accuracy: 1.0\n",
            "Step:  50\n",
            "Loss: 0.004529503174126148 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.008206810802221298 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 0.009985297918319702 Accuracy: 1.0\n",
            "Epoch: 14\n",
            "Step:  0\n",
            "Loss: 0.015631336718797684 Accuracy: 0.9921875\n",
            "Step:  50\n",
            "Loss: 0.007254943251609802 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.002673331880941987 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 0.005215157754719257 Accuracy: 1.0\n",
            "Epoch: 15\n",
            "Step:  0\n",
            "Loss: 0.0013254944933578372 Accuracy: 1.0\n",
            "Step:  50\n",
            "Loss: 0.0002664739149622619 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.0006669520516879857 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 0.00038640364073216915 Accuracy: 1.0\n",
            "Epoch: 16\n",
            "Step:  0\n",
            "Loss: 0.0002998697164002806 Accuracy: 1.0\n",
            "Step:  50\n",
            "Loss: 0.0003410878125578165 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.0001931995211634785 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 0.00013638066593557596 Accuracy: 1.0\n",
            "Epoch: 17\n",
            "Step:  0\n",
            "Loss: 8.701130718691275e-05 Accuracy: 1.0\n",
            "Step:  50\n",
            "Loss: 0.0001437044411431998 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.00011294242722215131 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 9.787618910195306e-05 Accuracy: 1.0\n",
            "Epoch: 18\n",
            "Step:  0\n",
            "Loss: 5.921951014897786e-05 Accuracy: 1.0\n",
            "Step:  50\n",
            "Loss: 8.043702837312594e-05 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 0.00011799589265137911 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 9.633525041863322e-05 Accuracy: 1.0\n",
            "Epoch: 19\n",
            "Step:  0\n",
            "Loss: 3.0320632504299283e-05 Accuracy: 1.0\n",
            "Step:  50\n",
            "Loss: 9.446535841561854e-05 Accuracy: 1.0\n",
            "Step:  100\n",
            "Loss: 5.7630961237009615e-05 Accuracy: 1.0\n",
            "Step:  150\n",
            "Loss: 3.562847996363416e-05 Accuracy: 1.0\n",
            "-----------Model test-------------\n",
            "Final Test Accuracy: 0.8026865720748901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOSXSjdyd2wp"
      },
      "source": [
        "## Questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgmNqqq0W85K"
      },
      "source": [
        "#### **Q1) Why is this wasteful? Can you think of a smarter padding scheme that is more efficient? Consider the fact that RNNs can work on arbitrary sequence lengths, and that training minibatches are pretty much independent of each other**\n",
        "\n",
        "1.   Because as per the histogram plot in the above plots, we can see that most of the sequence lengths are between 0-500 and if we pad to the max length then most of the neurons are initialised with dummy values which is wasteful for computation.\n",
        "\n",
        "2.   Alternative way is to pad at batch level by taking the average length of sequences in that particular batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QAVsIljYMWN"
      },
      "source": [
        "#### **Q2) Between truncating long sequences and removing them, which option do you think is better? Why?**\n",
        "\n",
        "1.   Post trunctaing could be better, but here tried different configs with both pre and post truncating but the results are same.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWUHJIN8ZjPU"
      },
      "source": [
        "#### **Q3) Can you think of a way to avoid the one-hot vectors completely? Even if you cannot implement it, a conceptual idea is fine.**\n",
        "\n",
        "1.   Word Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGaZiFh1afEn"
      },
      "source": [
        "#### **Q4) How can it be that we can choose how many outputs we have, i.e. how can both be correct? Are there differences between both choices as well as (dis)advantages relative to each other?**\n",
        "\n",
        "1. Use softmax if the output neurons size is 2 and whichever wins will be the target\n",
        "2. Use sigmoid if the output neuron size is 1 and get a value, based on the value classify it as 0 or 1 ( by threshold of 0.5)\n",
        "3. Both almost gave same results. Using softmax accuracy was 2 percent more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14QPaOeVbPSS"
      },
      "source": [
        "#### **Q5) All sequences start with the same special beginning of sequence token (coded by index 1). Given this fact, is there a point in learning an initial state? Why (not)?**\n",
        "\n",
        "1. Same information might be propagated from the initial state to the next states for every sequence which might not be helpful for classification purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a5iXzNHevdk"
      },
      "source": [
        "#### **Q6) All sequences start with the same special beginning of sequence token (coded by index 1). Given this fact, is there a point in learning an initial state? Why (not)?**\n",
        "\n",
        "1. Same information might be propagated from the initial state to the next states for every sequence which might not be helpful for classification purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dwP3J90feLu"
      },
      "source": [
        "#### **Q6) Pad_sequences allows for pre or post padding. Try both to see the difference. Which option do you think is better? Recall that we use the final time step output from our model.**\n",
        "\n",
        "1. Post padding is recommended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-_Onpizgefo"
      },
      "source": [
        "#### **Q7) Can you think of a way to prevent the RNN from computing new states on padded time steps? One idea might be to pass through the previous state in case the current time step is padding. Note that, within a batch, some sequences might be padded for a given time step while others are not.**\n",
        "\n",
        "1. May be instead of setting max lenth sequence at global level for the entire dataset. We can set the max length of the sequence at batch level since RNN can accept variable input size. By taking the average length in the current batch sequences and then apply padding. But here we need to compute the tensors at batch level which will be time consuming.\n",
        "\n",
        "2. The above description might not prevent padding at all, we can go sequence by sequence which can avoid padding. It is time consuming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgNMAfrKhimu"
      },
      "source": [
        "#### **Q8) What could be the advantage of using methods like the above? What are disadvantages? Can you think of other methods to incorporate the full output sequence instead of just the final step?**\n",
        "\n",
        "1. We can have output units at each time step and before sending it to the loss function take average of all the output units and send them."
      ]
    }
  ]
}