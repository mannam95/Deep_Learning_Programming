{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1_MLP.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "GW9A-uDZx9wq",
        "kERRSBuXp5JT",
        "wfDVrX2VunFA",
        "Vy7lrKcQt0dp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vn94RPJ4oTU"
      },
      "source": [
        "#Team Assignment\n",
        "\n",
        "\n",
        "1.   Venkata Srinath Mannam\n",
        "2.   Meghana Rao Bangalore Narasimha Prasad\n",
        "3.   Govind\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW9A-uDZx9wq"
      },
      "source": [
        "#Change the directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wam6g9cNx9HU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxBOM49SaFl7"
      },
      "source": [
        "import os\n",
        "working_directory = '/content/drive/My Drive/Colab Notebooks/OVGU/Deep_Learning/First_Assignment'\n",
        "if os.getcwd() !=  working_directory:\n",
        "  os.chdir(working_directory)\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBYC4kM2ppSs"
      },
      "source": [
        "#import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJaGiGnXpsAP"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import MNISTDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kERRSBuXp5JT"
      },
      "source": [
        "##Check tensorflow version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3D4m_0Op4iZ"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLtYs70nqz3b"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zegh4TeIq28p"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn8ONaMC3gYG"
      },
      "source": [
        "#Re-shape the dataset and then pass it to the function which creates data into batches\n",
        "def reset_Data():\n",
        "  return MNISTDataset(train_images.reshape([-1, 784]), train_labels, \n",
        "                    test_images.reshape([-1, 784]), test_labels,\n",
        "                    batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8ioSRg6AYcU"
      },
      "source": [
        "print(train_images.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_images.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfDVrX2VunFA"
      },
      "source": [
        "## Visualize some images on some training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X_MvhoZumPn"
      },
      "source": [
        "print(train_labels[3])\n",
        "plt.imshow(train_images[3], cmap=\"Greys_r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpaNU36_vXzy"
      },
      "source": [
        "print(test_labels[121])\n",
        "plt.imshow(test_images[121], cmap=\"Greys_r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hai0NIyaN2go"
      },
      "source": [
        "#Define a Generic function for training which uses relu activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqTYHVmVN06T"
      },
      "source": [
        "def model_Train(_modelConfig, data):\n",
        "  for step in range(_modelConfig[\"train_steps\"]):\n",
        "      img_batch, lbl_batch = data.next_batch()\n",
        "      with tf.GradientTape() as tape:\n",
        "          #loop that will goes forward for every hidden layer\n",
        "          for ind in range(len(_modelConfig[\"_weights\"])-1):\n",
        "            if ind == 0:\n",
        "              hPrevLogits = tf.matmul(img_batch, _modelConfig[\"_weights\"][ind]) + _modelConfig[\"_biases\"][ind]\n",
        "              hPrevOutput = tf.nn.relu(hPrevLogits)\n",
        "            else :\n",
        "              hcurrLogits = tf.matmul(hPrevOutput, _modelConfig[\"_weights\"][ind]) + _modelConfig[\"_biases\"][ind]\n",
        "              hcurrOutput = tf.nn.relu(hcurrLogits)\n",
        "              hPrevOutput = hcurrOutput\n",
        "\n",
        "          lastInd = len(_modelConfig[\"_weights\"])-1\n",
        "\n",
        "          #final output layer\n",
        "          logits = tf.matmul(hcurrOutput, _modelConfig[\"_weights\"][lastInd]) + _modelConfig[\"_biases\"][lastInd]\n",
        "          xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "              logits=logits, labels=lbl_batch))\n",
        "          \n",
        "      grads = tape.gradient(xent, [*_modelConfig[\"_weights\"], *_modelConfig[\"_biases\"]])\n",
        "\n",
        "      #update weights\n",
        "      for ind in range(len(_modelConfig[\"_weights\"])):\n",
        "        _modelConfig[\"_weights\"][ind].assign_sub(_modelConfig[\"learning_rate\"] * grads[ind])\n",
        "      \n",
        "      #update biases\n",
        "      for ind in range(len(_modelConfig[\"_biases\"])):\n",
        "        _modelConfig[\"_biases\"][ind].assign_sub(_modelConfig[\"learning_rate\"] * grads[ind + len(_modelConfig[\"_biases\"])])\n",
        "\n",
        "      \n",
        "      if not step % 100:\n",
        "          print(step)\n",
        "          preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "          acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n",
        "                              tf.float32))\n",
        "          print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "\n",
        "  #Predict for Test Dataset\n",
        "  for ind in range(len(_modelConfig[\"_weights\"])-1):\n",
        "      if ind == 0:\n",
        "        hPrevLogits = tf.matmul(data.test_data, _modelConfig[\"_weights\"][ind]) + _modelConfig[\"_biases\"][ind]\n",
        "        hPrevOutput = tf.nn.relu(hPrevLogits)\n",
        "      else :\n",
        "        hcurrLogits = tf.matmul(hPrevOutput, _modelConfig[\"_weights\"][ind]) + _modelConfig[\"_biases\"][ind]\n",
        "        hcurrOutput = tf.nn.relu(hcurrLogits)\n",
        "        hPrevOutput = hcurrOutput\n",
        "\n",
        "  lastInd = len(_modelConfig[\"_weights\"])-1\n",
        "  test_preds = tf.argmax(tf.matmul(hcurrOutput, _modelConfig[\"_weights\"][lastInd]) + _modelConfig[\"_biases\"][lastInd], axis=1,\n",
        "                       output_type=tf.int32)\n",
        "  acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                              tf.float32))\n",
        "  print(\"Test Accuracy\") \n",
        "  print(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InzTeW3xdRAU"
      },
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy7lrKcQt0dp"
      },
      "source": [
        "##Define params Model1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cuohb9MMt9Ws"
      },
      "source": [
        "modelConfig1 = {\n",
        "    \"train_steps\" : 1000,\n",
        "    \"learning_rate\" : 0.1,\n",
        "\n",
        "    \"_weights\" : [\n",
        "        tf.Variable(tf.random.uniform([784, 512], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "        tf.Variable(tf.random.uniform([512, 256], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "        tf.Variable(tf.random.uniform([256, 10], minval=-0.1, maxval=0.1, dtype=np.float32))\n",
        "    ],\n",
        "\n",
        "    \"_biases\" : [\n",
        "        tf.Variable(np.zeros(512, dtype=np.float32)),\n",
        "        tf.Variable(np.zeros(256, dtype=np.float32)),\n",
        "        tf.Variable(np.zeros(10, dtype=np.float32))\n",
        "    ],\n",
        "\n",
        "    \"w_b_list\" : [\"W1\", \"W2\", \"W3\", \"b1\", \"b2\", \"b3\"]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8ZoZ1gMddXx"
      },
      "source": [
        "##Define params Model2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJL0X9UvddXx"
      },
      "source": [
        "modelConfig2 = {\n",
        "    \"train_steps\" : 2000,\n",
        "    \"learning_rate\" : 0.01,\n",
        "\n",
        "    \"_weights\" : [\n",
        "        tf.Variable(tf.random.uniform([784, 512], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "        tf.Variable(tf.random.uniform([512, 256], minval=-0.1, maxval=0.1, dtype=np.float32)),\n",
        "        tf.Variable(tf.random.uniform([256, 10], minval=-0.1, maxval=0.1, dtype=np.float32))\n",
        "    ],\n",
        "\n",
        "    \"_biases\" : [\n",
        "        tf.Variable(np.zeros(512, dtype=np.float32)),\n",
        "        tf.Variable(np.zeros(256, dtype=np.float32)),\n",
        "        tf.Variable(np.zeros(10, dtype=np.float32))\n",
        "    ],\n",
        "\n",
        "    \"w_b_list\" : [\"W1\", \"W2\", \"W3\", \"b1\", \"b2\", \"b3\"]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiPEmOEtdMWc"
      },
      "source": [
        "#Models Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhaPfa4jYOTK"
      },
      "source": [
        "print(\"Model1 Start\")\n",
        "model_Train(modelConfig1, reset_Data())\n",
        "print(\"Model1 End\")\n",
        "print(\"\\n\")\n",
        "print(\"Model2 Start\")\n",
        "model_Train(modelConfig2, reset_Data())\n",
        "print(\"Model2 End\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}