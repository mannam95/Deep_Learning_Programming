{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_7_NMT_Srinath.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9b1ec548",
        "0c34550c",
        "sHbpSCdw8qnW",
        "yh0bPt7wkCHt",
        "JEIAwPgokXk6",
        "S6LD6RwikuOw",
        "6UKOUGEX0Z75",
        "J1JQ2_On36HO",
        "0LiDrt7y3-lW",
        "VQujLhNg4BrF",
        "3HCh9PfH4oZK",
        "7reyM4fc4rmJ",
        "ijN0V-aI46-x",
        "XE7o0uJU4rh2",
        "XQI6bl3P65rr",
        "15SEZQ5062pY",
        "_JM4SVkfDE1C"
      ],
      "machine_shape": "hm",
      "mount_file_id": "156VNP0l29aIE4p0WHewhNZn95t-7XKvN",
      "authorship_tag": "ABX9TyPzrQB3UfOdR8mSs0LSFdZl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mannam95/Deep_Learning_Programming/blob/main/Assignment7/Assignment_7_NMT_Srinath.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6aef94d"
      },
      "source": [
        "# Team Assignment\n",
        "\n",
        "\n",
        "1.   Srinath Mannam (229750)\n",
        "2.   Meghana Rao (234907)\n",
        "3.   Govind Shukla (235192)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b1ec548"
      },
      "source": [
        "# import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQCgf3jKQ8L3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee86798e-7e71-452f-a9bd-150af60b5f47"
      },
      "source": [
        "pip install tensorflow_text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (12.0.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.22.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.13.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.37.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.42.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.1.1)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85eb7211"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.preprocessing.sequence as sequence\n",
        "import tensorflow_text as tf_text\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c34550c"
      },
      "source": [
        "# Change the directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94b2626e"
      },
      "source": [
        "working_directory = '/content/drive/My Drive/Colab Notebooks/OVGU/Deep_Learning/07_Assignment'\n",
        "def colabDrive():\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "    if os.getcwd() !=  working_directory:\n",
        "      os.chdir(working_directory)\n",
        "    print(os.getcwd())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8540ed4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beafcc58-2487-4282-aa96-1086d403e083"
      },
      "source": [
        "colabDrive()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Colab Notebooks/OVGU/Deep_Learning/07_Assignment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjeunnuphktY"
      },
      "source": [
        "# Preamble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHbpSCdw8qnW"
      },
      "source": [
        "## Helper function which can be used to check the shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M15xw1ve8teI"
      },
      "source": [
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh0bPt7wkCHt"
      },
      "source": [
        "## A function that will read the data and return"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvTtHPvjhqBC"
      },
      "source": [
        "def load_data(path):\n",
        "  # text = path.read_text(encoding='utf-8')\n",
        "  with open(\"tel-eng/tel.txt\", \"r\", encoding='utf-8') as tel:\n",
        "     telugu=tel.read()\n",
        "\n",
        "  lines = telugu.splitlines()\n",
        "  pairs = [line.split('\\t')[:-1] for line in lines]\n",
        "\n",
        "  inp = [inp for targ, inp in pairs]\n",
        "  targ = [targ for targ, inp in pairs]\n",
        "\n",
        "  return targ, inp"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cOKg7mXhtw5"
      },
      "source": [
        "targ, inp = load_data(\"tel-eng/tel.txt\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoZqzF-_kMe-",
        "outputId": "08670340-c58d-42e4-e68f-f8d7405def07"
      },
      "source": [
        "print(inp[-1])\n",
        "print(targ[-1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "టామ్ తనకు తాను చేయటానికి అనుమతించకపోవచ్చని అనుకున్నాడు.\n",
            "Tom thought he might not be permitted to do that by himself.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEIAwPgokXk6"
      },
      "source": [
        "## Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvwzLTtIkU--"
      },
      "source": [
        "BUFFER_SIZE = len(inp)\n",
        "BATCH_SIZE = 60\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "test_dataset = dataset.take(20) \n",
        "train_dataset = dataset.skip(20)\n",
        "\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.batch(20).shuffle(10, seed=45)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrxWy_-3kbCL",
        "outputId": "07b6cd76-c47e-4c8a-966e-8d088ddad444"
      },
      "source": [
        "for example_input_batch, example_target_batch in train_dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\\xe0\\xb0\\xa8\\xe0\\xb1\\x81\\xe0\\xb0\\xb5\\xe0\\xb1\\x8d\\xe0\\xb0\\xb5\\xe0\\xb1\\x81 \\xe0\\xb0\\x8e\\xe0\\xb0\\x95\\xe0\\xb1\\x8d\\xe0\\xb0\\x95\\xe0\\xb0\\xa1 \\xe0\\xb0\\x89\\xe0\\xb0\\xa8\\xe0\\xb1\\x8d\\xe0\\xb0\\xa8\\xe0\\xb0\\xb5\\xe0\\xb1\\x8b \\xe0\\xb0\\x85\\xe0\\xb0\\x95\\xe0\\xb1\\x8d\\xe0\\xb0\\x95\\xe0\\xb0\\xa1\\xe0\\xb1\\x87 \\xe0\\xb0\\x89\\xe0\\xb0\\x82\\xe0\\xb0\\xa1\\xe0\\xb1\\x81'\n",
            " b'\\xe0\\xb0\\xa8\\xe0\\xb1\\x80 \\xe0\\xb0\\x95\\xe0\\xb0\\xb2\\xe0\\xb0\\x82 \\xe0\\xb0\\x8f\\xe0\\xb0\\xa6\\xe0\\xb0\\xbf ?'\n",
            " b'\\xe0\\xb0\\xa8\\xe0\\xb0\\xbf\\xe0\\xb0\\xa8\\xe0\\xb1\\x8d\\xe0\\xb0\\xa8 \\xe0\\xb0\\xb0\\xe0\\xb0\\xbe\\xe0\\xb0\\xa4\\xe0\\xb1\\x8d\\xe0\\xb0\\xb0\\xe0\\xb0\\xbf\\xe0\\xb0\\x95\\xe0\\xb0\\xbf \\xe0\\xb0\\xa8\\xe0\\xb0\\xa8\\xe0\\xb1\\x8d\\xe0\\xb0\\xa8\\xe0\\xb1\\x81 \\xe0\\xb0\\xa8\\xe0\\xb0\\xbf\\xe0\\xb0\\x9c\\xe0\\xb0\\x82\\xe0\\xb0\\x97\\xe0\\xb0\\xbe \\xe0\\xb0\\x95\\xe0\\xb1\\x8d\\xe0\\xb0\\xb7\\xe0\\xb0\\xae\\xe0\\xb0\\xbf\\xe0\\xb0\\x82\\xe0\\xb0\\x9a\\xe0\\xb1\\x81'\n",
            " b'\\xe0\\xb0\\xa4\\xe0\\xb0\\xa8\\xe0\\xb1\\x81 \\xe0\\xb0\\xb8\\xe0\\xb1\\x8d\\xe0\\xb0\\x95\\xe0\\xb1\\x82\\xe0\\xb0\\xb2\\xe0\\xb1\\x81\\xe0\\xb0\\x95\\xe0\\xb1\\x81 \\xe0\\xb0\\xb5\\xe0\\xb1\\x86\\xe0\\xb0\\xb3\\xe0\\xb1\\x8d\\xe0\\xb0\\xb3\\xe0\\xb1\\x87 \\xe0\\xb0\\xa6\\xe0\\xb0\\xbe\\xe0\\xb0\\xb0\\xe0\\xb0\\xbf\\xe0\\xb0\\xb2\\xe0\\xb1\\x8b \\xe0\\xb0\\xb5\\xe0\\xb1\\x81\\xe0\\xb0\\x82\\xe0\\xb0\\xa6\\xe0\\xb0\\xbf'\n",
            " b'\\xe0\\xb0\\x87\\xe0\\xb0\\xa6\\xe0\\xb0\\xbf \\xe0\\xb0\\x92\\xe0\\xb0\\x95 \\xe0\\xb0\\xb5\\xe0\\xb0\\xbe\\xe0\\xb0\\x95\\xe0\\xb1\\x8d\\xe0\\xb0\\xaf\\xe0\\xb0\\x82 \\xe0\\xb0\\x95\\xe0\\xb0\\xbe\\xe0\\xb0\\xa6\\xe0\\xb1\\x81.'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b'You must stay where you are.' b'Which is your pen?'\n",
            " b\"I'm really sorry about last night.\" b'She was on her way to school.'\n",
            " b'This is not a sentence.'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6LD6RwikuOw"
      },
      "source": [
        "## Unicode Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpgbPhcfkw4V"
      },
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accecented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, ఀ-౿(telugu text) and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z$ఀ-౿.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UKOUGEX0Z75"
      },
      "source": [
        "## Text Vectorization\n",
        "\n",
        "Creating a word to index dictionary and an index to word dictionary for all unique source and target words in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1JQ2_On36HO"
      },
      "source": [
        "### For Telugu language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnStPmuy0ci5",
        "outputId": "b529e2b0-a577-44bb-fc1d-a046d659c967"
      },
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "input_text_processor.adapt(inp)\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "input_text_processor.get_vocabulary()[:10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '?', '.', 'నువ్వు', 'నేను', 'అది', 'నాకు']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LiDrt7y3-lW"
      },
      "source": [
        "### For English language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2n7U_B00ebs",
        "outputId": "0149d65d-544d-4a27-c8a8-7a18e95317a5"
      },
      "source": [
        "output_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(targ)\n",
        "output_text_processor.get_vocabulary()[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'you', '?', 'to', 'i', 'the']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQujLhNg4BrF"
      },
      "source": [
        "### See some tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DubharMb1QAt",
        "outputId": "15038179-0b98-433f-cf1c-1a7745622cea"
      },
      "source": [
        "example_inp_tokens = input_text_processor('నువ్వు అది చూసావా?')\n",
        "example_inp_tokens"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6,), dtype=int64, numpy=array([  2,   6,   8, 311,   4,   3])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef80Q88-1aEa",
        "outputId": "85457ede-43a6-4715-827f-c8fd59eec823"
      },
      "source": [
        "example_tar_tokens = output_text_processor('Can you see that?')\n",
        "example_tar_tokens"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(7,), dtype=int64, numpy=array([  2,  30,   5, 199,  10,   6,   3])>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BWNVkXh74dU"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z-Q4E8A78xB"
      },
      "source": [
        "embedding_dim = 256\n",
        "units = 1024"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HCh9PfH4oZK"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfVSM8z87_ok"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(tokens, ('batch', 's'))\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    #    output shape: (batch, s, enc_units)\n",
        "    #    state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
        "    shape_checker(state, ('batch', 'enc_units'))\n",
        "\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaAwgen08g35",
        "outputId": "5e1b9282-7c23-4bd5-f353-bb9e0dd52f91"
      },
      "source": [
        "# Convert the input text to tokens.\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "\n",
        "# Encode the input sequence.\n",
        "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)\n",
        "example_enc_output, example_enc_state = encoder(example_tokens)\n",
        "\n",
        "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
        "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
        "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch, shape (batch): (60,)\n",
            "Input batch tokens, shape (batch, s): (60, 12)\n",
            "Encoder output, shape (batch, s, units): (60, 12, 1024)\n",
            "Encoder state, shape (batch, units): (60, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7reyM4fc4rmJ"
      },
      "source": [
        "# Attention Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijN0V-aI46-x"
      },
      "source": [
        "## Bahdanau Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nMlsrbr4ud7"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(query, ('batch', 't', 'query_units'))\n",
        "    shape_checker(value, ('batch', 's', 'value_units'))\n",
        "    shape_checker(mask, ('batch', 's'))\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwXBo95d5Puv"
      },
      "source": [
        "attention_layer = BahdanauAttention(units)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az3SlShz5caC",
        "outputId": "de7dfb5f-9a8e-4685-a2c2-a3ff0402d32b"
      },
      "source": [
        "# Later, the decoder will generate this attention query\n",
        "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
        "\n",
        "# Attend to the encoded tokens\n",
        "\n",
        "context_vector, attention_weights = attention_layer(\n",
        "    query=example_attention_query,\n",
        "    value=example_enc_output,\n",
        "    mask=(example_tokens != 0))\n",
        "\n",
        "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
        "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch_size, query_seq_length, units):           (60, 2, 1024)\n",
            "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (60, 2, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE7o0uJU4rh2"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2LzXKtk6JtR"
      },
      "source": [
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq5z9qVo50Wt"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # For Step 1. The embedding layer convets token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # For step 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    # For step 5. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
        "\n",
        "  def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(inputs.new_tokens, ('batch', 't'))\n",
        "    shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
        "    shape_checker(inputs.mask, ('batch', 's'))\n",
        "\n",
        "    if state is not None:\n",
        "      shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "    # Step 1. Lookup the embeddings\n",
        "    vectors = self.embedding(inputs.new_tokens)\n",
        "    shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
        "\n",
        "    # Step 2. Process one step with the RNN\n",
        "    rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "    shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
        "    shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "    # Step 3. Use the RNN output as the query for the attention over the\n",
        "    # encoder output.\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "    shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "    #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "    # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "    attention_vector = self.Wc(context_and_rnn_output)\n",
        "    shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
        "\n",
        "    # Step 5. Generate logit predictions:\n",
        "    logits = self.fc(attention_vector)\n",
        "    shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
        "\n",
        "    return DecoderOutput(logits, attention_weights), state"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwSvsa4R56GQ"
      },
      "source": [
        "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQpJ5Xej607m"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQI6bl3P65rr"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "Summary:\n",
        "\n",
        "\n",
        "*   Sparse Categorical crossentropy is the loss function\n",
        "*   Not computing the loss where padding is applied\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOPAFtbb637r"
      },
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(y_true, ('batch', 't'))\n",
        "    shape_checker(y_pred, ('batch', 't', 'logits'))\n",
        "\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    shape_checker(loss, ('batch', 't'))\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    shape_checker(mask, ('batch', 't'))\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcnrzfLV6_BN"
      },
      "source": [
        "## Train Model\n",
        "\n",
        "Summary\n",
        "\n",
        "\n",
        "1.   A constructor which will initialise Encoder, Decoder etc\n",
        "2.   A **train_step** function which will choose which function to run whether the train step function which is wrapped with tf or the normal train step function.\n",
        "3.   A  **_preprocess** function which will preprocess the data.\n",
        "4.   A  **_train_step** function.\n",
        "5.   A  **_tf_train_step** function wrapped like a tf function.\n",
        "6.   A  **_loop_step** function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjNgJCTe7Bb_"
      },
      "source": [
        "class TrainTranslator(tf.keras.Model):\n",
        "  ## --------------------------------------------------Start of the Constructor----------------------------------------------------------\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor, \n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "    self.shape_checker = ShapeChecker()\n",
        "  ## --------------------------------------------------End of the Constructor----------------------------------------------------------\n",
        "\n",
        "  ## --------------------------------------------------Start of the Call----------------------------------------------------------\n",
        "  def call(self, inputs):\n",
        "    return self._test_step(inputs)\n",
        "  ## --------------------------------------------------End of the Call----------------------------------------------------------\n",
        "\n",
        "  ## --------------------------------------------------Start of the train step----------------------------------------------------------\n",
        "  #Choose which train step to run\n",
        "  def train_step(self, inputs):\n",
        "    self.shape_checker = ShapeChecker()\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)\n",
        "  ## --------------------------------------------------End of the train step----------------------------------------------------------\n",
        "\n",
        "  ## --------------------------------------------------Start of the Preprocess data function----------------------------------------------------------\n",
        "  def _preprocess(self, input_text, target_text):\n",
        "    self.shape_checker(input_text, ('batch',))\n",
        "    self.shape_checker(target_text, ('batch',))\n",
        "\n",
        "    # Convert the text to token IDs\n",
        "    input_tokens = self.input_text_processor(input_text)\n",
        "    target_tokens = self.output_text_processor(target_text)\n",
        "    self.shape_checker(input_tokens, ('batch', 's'))\n",
        "    self.shape_checker(target_tokens, ('batch', 't'))\n",
        "\n",
        "    # Convert IDs to masks.\n",
        "    input_mask = input_tokens != 0\n",
        "    self.shape_checker(input_mask, ('batch', 's'))\n",
        "\n",
        "    target_mask = target_tokens != 0\n",
        "    self.shape_checker(target_mask, ('batch', 't'))\n",
        "\n",
        "    return input_tokens, input_mask, target_tokens, target_mask\n",
        "  ## --------------------------------------------------End of the Preprocess data function----------------------------------------------------------\n",
        "\n",
        "  ## --------------------------------------------------Start of the train step function definition----------------------------------------------------------\n",
        "  def _train_step(self, inputs):\n",
        "    input_text, target_text = inputs  \n",
        "\n",
        "    (input_tokens, input_mask,\n",
        "    target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "    max_target_length = tf.shape(target_tokens)[1]\n",
        "    \n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Encode the input\n",
        "      enc_output, enc_state = self.encoder(input_tokens)\n",
        "      self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
        "      self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "      # Initialize the decoder's state to the encoder's final state.\n",
        "      # This only works if the encoder and decoder have the same number of\n",
        "      # units.\n",
        "      dec_state = enc_state\n",
        "      loss = tf.constant(0.0)\n",
        "\n",
        "      for t in tf.range(max_target_length-1):\n",
        "        # Pass in two tokens from the target sequence:\n",
        "        # 1. The current input to the decoder.\n",
        "        # 2. The target for the decoder's next prediction.\n",
        "        new_tokens = target_tokens[:, t:t+2]\n",
        "        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                              enc_output, dec_state)\n",
        "        loss = loss + step_loss\n",
        "\n",
        "      # Average the loss over all non padding tokens.\n",
        "      average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables \n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "  ## --------------------------------------------------End of the train step function definition----------------------------------------------------------\n",
        "\n",
        "\n",
        "  ## --------------------------------------------------Start of the test step evaluate function definition----------------------------------------------------------\n",
        "  @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "  def _test_step(self, inputs):\n",
        "    input_text, target_text = inputs  \n",
        "    \n",
        "    (input_tokens, input_mask,\n",
        "    target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "    max_target_length = tf.shape(target_tokens)[1]\n",
        "    \n",
        "    # Encode the input\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "    self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
        "    self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "    # Initialize the decoder's state to the encoder's final state.\n",
        "    # This only works if the encoder and decoder have the same number of\n",
        "    # units.\n",
        "    dec_state = enc_state\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    for t in tf.range(max_target_length-1):\n",
        "      # Pass in two tokens from the target sequence:\n",
        "      # 1. The current input to the decoder.\n",
        "      # 2. The target for the decoder's next prediction.\n",
        "      new_tokens = target_tokens[:, t:t+2]\n",
        "      step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                            enc_output, dec_state)\n",
        "      loss = loss + step_loss\n",
        "\n",
        "    # Average the loss over all non padding tokens.\n",
        "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "  ## --------------------------------------------------End of the test step evaluate function definition----------------------------------------------------------\n",
        "\n",
        "  ## --------------------------------------------------Start of the tf train step function ----------------------------------------------------------\n",
        "  # wrap train step into tf function to speed up the process\n",
        "  @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "  def _tf_train_step(self, inputs):\n",
        "    return self._train_step(inputs)\n",
        "  ## --------------------------------------------------Start of the tf train step function ----------------------------------------------------------\n",
        "\n",
        "\n",
        "  ## --------------------------------------------------Start of the train loop function----------------------------------------------------------\n",
        "  #The _loop_step method, added below, executes the decoder and calculates the incremental loss and new decoder state (dec_state).\n",
        "  def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "    # Run the decoder one step.\n",
        "    decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                                enc_output=enc_output,\n",
        "                                mask=input_mask)\n",
        "\n",
        "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "    self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
        "    self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
        "    self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
        "\n",
        "    # `self.loss` returns the total for non-padded tokens\n",
        "    y = target_token\n",
        "    y_pred = dec_result.logits\n",
        "    step_loss = self.loss(y, y_pred)\n",
        "\n",
        "    return step_loss, dec_state\n",
        "   ## --------------------------------------------------End of the train loop function----------------------------------------------------------"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIWxhEMq-hxK"
      },
      "source": [
        "## Test the Model to see whether it works or not for a single batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PBoLFRi7k-P"
      },
      "source": [
        "translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=True)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "923fPXrO9Gz9",
        "outputId": "1176b8bd-4110-4c95-e5f7-c43d59ef08d3"
      },
      "source": [
        "losses = []\n",
        "for n in range(100):\n",
        "  print('.', end='')\n",
        "  logs = translator.train_step([example_input_batch, example_target_batch])\n",
        "  losses.append(logs['batch_loss'].numpy())\n",
        "\n",
        "print()\n",
        "plt.plot(losses)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "....................................................................................................\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0d3ae3ef10>]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXiV5Z3/8ff3bNk3khAg7KuAskhUFFDrvqMttthWq2MHa7Wi9teZcaa/6TLT6bTjdEqttUVxq45bi0KRYluXiiBo2BQJlH0NENYEsif3/JETDBAkIedwznnyeV3XuZKz5vtcT65P7nyf+3luc84hIiLxyxfrAkRE5LMpqEVE4pyCWkQkzimoRUTinIJaRCTOBaLxoXl5ea5v377R+GgREU9asmTJHudcfmvPRSWo+/btS3FxcTQ+WkTEk8xs84meU+tDRCTOKahFROLcSYPazIaY2fIWt3Izu/90FCciIm3oUTvn1gCjAMzMD2wHXo1yXSIiEtbe1selwHrn3Amb3iIiElntDerJwAutPWFmU8ys2MyKy8rKOl6ZiIgA7QhqMwsBNwCvtPa8c266c67IOVeUn9/qVEARETkF7RlRXw0sdc7tilYxHbF572Hmr9VIXkS8pz1BfQsnaHvEg8fnb+D+F5fHugwRkYhrU1CbWRpwOTAzuuWcukPV9RyqqY91GSIiEdemU8idc4eB3CjX0iFVdQ3U1DfS2Ojw+SzW5YiIRIxnzkysqmsEoLq+IcaViIhElmeCurq2KaCrahXUIuItngnqqrqGo76KiHiF54K6WkEtIh7jnaA+0vpojHElIiKR5ZmgrlbrQ0Q8yjNBrR61iHiVJ4LaOfdpUGvWh4h4jCeCuqa+EeeavtfBRBHxGk8EdctRdKVG1CLiMd4I6hajaPWoRcRrPBfUan2IiNd4I6hbtDt0MFFEvMYTQV2t1oeIeJgnglo9ahHxMm8EdYt2R7VaHyLiMd4I6vAoOuAzjahFxHM8EdTNPeqctJCCWkQ8xxNB3dz66JIa0qwPEfEcbwR1eBmunLSg5lGLiOe0dRXybDP7nZmtNrMSMzs/2oW1R1Vt0+rj2SkhnUIuIp7TplXIgWnAPOfcJDMLAalRrKndquoaSAn6SQ351aMWEc85aVCbWRZwIXA7gHOuFqiNblntU1XXQErIT3LIr9aHiHhOW1of/YAy4CkzW2ZmT5hZ2rEvMrMpZlZsZsVlZWURL/SzVNU2khL0kxL062CiiHhOW4I6AJwNPOacGw0cBv7p2Bc556Y754qcc0X5+fkRLvOzVdc1kBz0NQV1XQOu+eLUIiIe0Jag3gZsc84tDt//HU3BHTeaWx8pIT+NDmobtMCtiHjHSYPaObcT2GpmQ8IPXQqsimpV7VRV20BqMEBK0A9AtVYiFxEPaeusj28Bz4dnfGwA7oheSe1XVddAZkqQlJD/yP0sgjGuSkQkMtoU1M655UBRlGs5ZdV1DRRkJh0ZUWuKnoh4iUfOTGyaR53cHNSa+SEiHuKNoK799GAiaEQtIt7imaBODs+jbr4vIuIV3gjqcOtDPWoR8aKED+q6hkbqG11TUIeaNkdBLSJekvBB3RzKKaFPDyZqOS4R8ZKED+rmUE5W60NEPCrhg/rIiDqoWR8i4k2eCerUkJ/kgGZ9iIj3JH5QN7c+Qn58PiM56NM1qUXEUxI/qFu0Ppq/qvUhIl6S8EFd3VpQq/UhIh6S8EFdFb6kafOBxOSQn0qNqEXEQxI+qCvDK5C3HFFrHrWIeEnCB3Vz6yNZPWoR8ai4CWrnHN95ZQWzlm9v15qHLc9MbP6qoBYRL4mboC6vrmdVaTlTX1zOFx5byLIt+9v0vuYedXKgaVOSdTBRRDwmboI6KyXI7HvH89NJI9i6v4qbfrWQ/5hbctLRdVVdAyG/j4C/aVNSgn7NoxYRT2nrmomnhd9nfLGoF9ec1Z0fvV7C9Hc30NDo+O61QzGzVt9TXddAcvDTvzfqUYuI17QpqM1sE1ABNAD1zrmorp+YnhTgP246k6SAjxnvbcRn8M/XtB7WVbUNpIY+3YyUkFofIuIt7RlRf845tydqlRzDzPje9cNoaHQ8Pn8jOWkhvnnxwONeV1XXcORAIjQFdXVd4+kqU0Qk6uKmR90aM+OHE4dz6Rld+c1fN7Tae66qazgyNQ+aWh+1DY3UNyisRcQb2hrUDviTmS0xsynRLOhYZsad4/txsKqOP64sPe756roGUo7pUQNU1yuoRcQb2hrU451zZwNXA/eY2YXHvsDMpphZsZkVl5WVRbTIsf1z6ZubyguLtx73XPMK5M2Sw983n7EoIpLo2hTUzrnt4a+7gVeBc1t5zXTnXJFzrig/Pz+yRfqMW87tzQeb9rF2V8VRz1XWNhwZRUOLEXWtRtQi4g0nDWozSzOzjObvgSuAldEu7FhfGNOToN944YOjR9XVrfSoQau8iIh3tGVEXQC8Z2YrgA+A151z86Jb1vHy0pO4Yng3fr9021EHFavqjhlRayVyEfGYkwa1c26Dc25k+DbcOfej01FYa75ybu/jDioeOz2veXStudQi4hVxPT3vWM0HFWcu3X7ksaoT9ag1ohYRj0iooPb5jPMH5PLx9oM452hsdNTUNx7do9ZK5CLiMQkV1ACDCzI4UFlHWUUN1fWfrkDeLDXYdLKlWh8i4hUJF9RDumUAsHpnxZEwPnoetQ4mioi3JFxQn9EtE4A1OyuOhHFr0/PUoxYRr0i4oO6SFiI/I4nVOyuOW4EcNOtDRLwn4YIa4IxuGazZVf7pCuQtgjro9xH0m1YiFxHPSMigHlKQwdpdh6ioqQOO7lGDluMSEW+JqxVe2mpItwxq6htZs7Ppuh8te9TQtuW45q0sZcZ7G0kNBcjPSKJ7VjJfOqcXPXNSo1a3iMipSNigBli+9QBwdOsDPnsl8v2Ha/ne7E+YvWIH/fPSqE1uZO2uCnZV1PCbdzfw9fH9uPviAWQkB6O7ESIibZSQQT2oawZmLYI6dPyIurXWx7rdFdzy+GL2H67lwcsHc/fFAwiGF8XdcaCKh99Yw6/eWc/LxVuZNnk04wbmRX9jREROIiF71CkhP31z09i8t7LpfrCVHnUrI+ppb66juraBWfeO475LBx0JaYAe2Sn87EujmH3vOHLTkrjjqQ+Z18pCBSIip1tCBjU0HVBsdlzro5Ue9fYDVcz9uJRbzuvN8B5ZJ/zcET2zefmu8zmrZxbffH4pL324JbKFi4i0U+IGdbcWQX1s66OVHvUzCzcB8LUL+p70s7NSg/z2znOZMCiff/z9xwprEYmphA3qM8JB7fcZQb8d9VxK6OgedUV1HS8s3sI1Z3WnMDulTZ+fGgrw+G1FXDg4n+++tpIlm/dFrngRkXZI2KAeHA7qlKAfs2OCOuinuu7TpbheLt5GRU09d47v166fEQr4eGTyaHpkp/CN55ayq7y644WLiLRTwgZ139w0kgK+4+ZQQ3jWR7j1Ud/QyFMLNnJO3xxG9cpu98/JSg3y+G1FHK6p567fLqGmXifSiMjplbBB7fcZgwrSjyy91VJKyE95VR33/O9SPv/YQrbtr+LO8f1P+WcNLsjgZ18cyfKtB/jx3NUdKVtEpN0SNqgBrj6zO+f1yz3u8eE9MklLClBSWk5aKMDdFw/g8mEFHfpZV53Zndsv6MvTCzexaMPeDn2WiEh7mHMu4h9aVFTkiouLI/65sVZZW8/V0+bjHMy7fwKpoYQ8X0hE4pCZLXHOFbX2XJtH1GbmN7NlZjYncqUlltRQgJ9+YQRb9lXykz+qBSIip0d7Wh9TgZJoFZIozuufy+0X9OWZ9zfz/nq1QEQk+toU1GbWE7gWeCK65SSGf7hqCH1yU3lo5kdaSUZEoq6tI+qfA/8ANJ7oBWY2xcyKzay4rKwsIsXFq9RQgB/deBab9lby6NvrYl2OiHjcSYPazK4DdjvnlnzW65xz051zRc65ovz8/IgVGK/GD8rjptGF/Pqv61m7qyLW5YiIh7VlRD0OuMHMNgEvApeY2XNRrSpBfPfaoaQlBXho5sc0NkZ+9oyICLQhqJ1zDznnejrn+gKTgbecc1+NemUJIDc9iX++ZijFm/fzgi7cJCJRktAnvMSDm8f05IIBufzo9RI27z0c63JExIPaFdTOuXecc9dFq5hEZGY8fPNI/D7jgZeWU99wwuOtIiKnRCPqCOiRncK/33gmS7cc4LF31se6HBHxGAV1hEwcVcj1I3sw7c21fLTtQKzLEREPUVBH0L9PPJP8jCSmPLuE9WWHYl2OiHiEgjqCslKDPHXHOdQ1NPKl3yzib5pfLSIRoKCOsDO6ZfLSXWPxGUyevohlW/bHuiQRSXC6zGmUbNpzmC8/vogdB6sZXJDOVcO7cU6/LqSG/CQF/HTNSKJrZnKsyxSROPFZlzlVUEfR3kM1zF6xg3krd/Lhpn0ce/LimYWZXDmsG1ef1Y2BXTNa/xAR6RQU1HFgz6Ea1u0+RE19IzV1DawvO8yfV+1k6ZamGSKXDe3KvZcMOqV1HUUk8Smo49ju8mpe/HArTy7YyIHKOi4cnM8PbhhOv7y0WJcmIqeRgjoBHKqp57lFm/nV2+uoqW/k/10xhL8b3w+/z2JdmoicBhFZikuiKz0pwDcuGsCfH7yICYPy+dHcEr7w2EK27quMdWkiEmMK6jhTkJnM47eN4Re3jGZ92SEmPrpAS36JdHIK6jhkZtwwsgez7hlHTmqQr85YzDMLNxGNNpWIxD8FdRzrn5/Oq/eM46LB+Xxv9ifc9uQHbNCp6SKdjoI6zmUmB3nitiK+f/0wlm85wJU/f5efzlvNrvLqWJcmIqeJZn0kkLKKGn78xxJmLt0OwOje2Vw5vBtj++cyrHsmoYD+7ookKk3P85h1uyv448c7eWPVTlZuLwcg5PcxrEcm5/brwtj+XTinbxcykoMxrlRE2kpB7WE7D1azbMt+lm09wLIt+1mx9SC1DY34DIr6dOHKM7tx5fACeuakxrpUEfkMCupOpKq2gWVb9rNw/V7+UrKL1TubLrU6smcWE0cVct3I7nTN0MWgROKNgroT27TnMPM+2cns5TtYVVqOz2DcwDyuG9GdK4d3Izs1FOsSRYQOBrWZJQPvAklAAPidc+57n/UeBXV8WrurgteWb2fOR6Vs3ltJ0G9cMCCPy4Z25ZKhBRRmp8S6RJFOq6NBbUCac+6QmQWB94CpzrlFJ3qPgjq+OedYub2cOR/t4E+rdrFxz2EAzuiWwaVDu3LJGQWM6pWt64yInEYRa32YWSpNQX23c27xiV6noE4s68sO8WbJLt4s2U3x5v00NDq6ZiQxaUxPJp/Tm965OhApEm0dDmoz8wNLgIHAo865f2zlNVOAKQC9e/ces3nz5g4VLbFxsLKOd/62m9nLd/D2mt00OpgwKI+plw6iqG+XWJcn4lmRHFFnA68C33LOrTzR6zSi9obSg1W8UryNZ9/fzJ5DNUwYlMcDlw/m7N45sS5NxHMiOuvDzP4VqHTOPXyi1yiovaWqtoHnFm3msb+uZ9/hWsYPzOPeSwZyXr8uNB3CEJGO6ujBxHygzjl3wMxSgD8BP3HOzTnRexTU3nS4pp7nF29m+rsb2XOohrN7ZzNpTC+uOrMbXdI0zU+kIzoa1COAZwA/TRdxetk598PPeo+C2tuq6xp4uXgrTy/YxIY9h/H7jAsG5HLFsAI+d0ZXnQUpcgp0wotEhXOOVaXlzPmolLkfN83NhqZpfl8+rzeTxvQkNRSIcZUiiUFBLVHnnGPDnsO8VbKbOR/tYMW2g2SlBPnKeb35+oT+ao2InISCWk67JZv38fi7G3lj1U4ykgJMvWwwt47to0uxipyAglpi5m+7Kvi3OauYv3YP/fPS+M6VQ7hyeDd8OutR5ChahVxiZnBBBs/+3bk8dfs5mMHdzy/lukfe48+rdmkNSJE20ohaTpuGRsfsFduZ9pe1bNpbyeCCdG47vy83jS4kLUkHHaVzU+tD4kp9QyOzlu/gqYUbWbm9nIykAF8Z24cpF+qgo3ReCmqJS845lm45wJMLNjL341JSg35uH9eXKRMGkJWqZcSkc/msoNb/mxIzZsaYPjmM6ZPD2l0V/PzNtfzqnfW8+MFW/vX6YdwwsodOURdBBxMlTgwqyODRL5/NnG+Np2eXVKa+uJw7nv6QbfsrY12aSMwpqCWuDO+Rxcy7L+B71w/jg437uOJ/3uXJ9zbS0KgZItJ5Kagl7vh9xh3j+vHnBy/ivH5d+OGcVXz+sYWUlJbHujSRmFBQS9wqzE7hydvPYdrkUWzbV8l1j7zHD/+wivLquliXJnJaKaglrpkZE0cV8pcHL+JL5/TiqYUbueThv/Lasu06YUY6DQW1JISctBD/cdNZzLpnHIU5Kdz/0nL+/tlidpdXx7o0kahTUEtCGdEzm5l3X8B3rx3K/LV7uPx/3mXW8u2xLkskqhTUknD8PuPrE/ozd+oEBuSnMfXF5XznlRVU1tbHujSRqFBQS8IakJ/Oy3edz32XDOR3S7cx8ZcL+NuuiliXJRJxCmpJaAG/jwevGMJv/+489lfWcv0j7zHjvY00at61eIiCWjxh/KA85t43gXED8/i3OauYPH0Rm/cejnVZIhGhoBbP6JqZzIyvFfFfk0ZQUlrOVT+fz7Pvb9LoWhLeSYPazHqZ2dtmtsrMPjGzqaejMJFTYWbcXNSLPz14Ief068K/zvqEr85YzNZ9umaIJK62jKjrgW8754YBY4F7zGxYdMsS6ZjuWSk8c8c5/Ofnz+KjbQe5etp85n5cGuuyRE7JSYPaOVfqnFsa/r4CKAEKo12YSEeZGZPP7c28+ycwuCCdbz6/lB/PLaG+oTHWpYm0S7t61GbWFxgNLG7luSlmVmxmxWVlZZGpTiQCeuak8uKU87l1bB9+8+4GbnvyA8oqamJdlkibtTmozSwd+D1wv3PuuMuYOeemO+eKnHNF+fn5kaxRpMNCAR//duOZ/NekESzZvJ9rfjGfBev2xLoskTZpU1CbWZCmkH7eOTczuiWJRM/NRb2Yde84MpMDfHXGYh5+Y41aIRL32jLrw4AZQIlz7mfRL0kkus7olskfvjWeSWf35Jdvr+PWGWqFSHxry4h6HHArcImZLQ/frolyXSJRlRoK8F83j+Thm0eydMt+rntkPsWb9sW6LJFWtWXWx3vOOXPOjXDOjQrf5p6O4kSibdKYnrz6zXEkB/1Mnr6Ipxds1HWuJe7ozETp9Ib1yGT2veO5eEg+3//DKr79ygqq6xpiXZbIEQpqESArJcj0W4t44LLBzFy6nUm/XqizGSVuKKhFwnw+Y+plg5jxtSI2763kmmnztSiBxAUFtcgxLh1awNz7JjCkWwZTX1zOAy8tp0IL6koMKahFWtGrSyovThnL/ZcNYtby7dz46AI27dFlUyU2FNQiJxDw+7j/ssE8//Wx7D1cy42/WsDC9TqbUU4/BbXISZw/IJdZ94wjLz2J22Z8wDMLN2kKn5xWCmqRNuiTm8bMb17AhYPz+d7sT/jGc0s4UFkb67Kkk1BQi7RRZnKQJ24r4rvXDuWt1bu5Ztp8lmzeH+uypBNQUIu0g89nfH1Cf35/9wUEAz5umb6ImUu3xbos8TgFtcgpGNEzm1n3jGNMnxwefHkFD7+xRmszStQoqEVOUXZqiGfvPJfJ5/Til2+v4+7nl3CwSvOtJfIU1CIdEPT7+PHnz+L/XzeMN0t2c90j81mx9UCsyxKPUVCLdJCZcef4frx01/k0NsKkXy/kyfd0FT6JHAW1SISM6ZPD6/eN56LBXfnhnFU88NJyXYVPIkJBLRJB2akhpt86hm9fPphZK3bwhccWsm2/rsInHaOgFokwn8/41qVNV+HbsreS6x95j7fX7I51WZLAFNQiUXLJGQXM/tZ4CjKTueOpD/nJvNVaSFdOiYJaJIr65aXx2j3juOXc3jz2znomT1/EjgNVsS5LEoyCWiTKkoN+fvz5s5g2eRQlpeVc84v5/HnVrliXJQnkpEFtZk+a2W4zW3k6ChLxqomjCplz3wQKs1P4+2eL+cEfPqG2Xq0QObm2jKifBq6Kch0inUK/vKar8N1+QV+eWrCJWx5fxK7y6liXJXHupEHtnHsX2HcaahHpFJICfr5/w3B++eXRlJSWc+0v3mPxhr2xLkviWMR61GY2xcyKzay4rKwsUh8r4lnXjejBrHvGkZkc4MtPLObhN9ZQU68TZOR4EQtq59x051yRc64oPz8/Uh8r4mmDCjKYde84bhpdyC/fXscNjyxg5faDsS5L4oxmfYjEWEZykIdvHsmMrxWxv7KWiY8u4CfzVuv0czlCQS0SJy4dWsCfHriQG0cV8tg767l62nzeX6/etbRtet4LwPvAEDPbZmZ3Rr8skc4pOzXEf39xJM/deR71jY3c8vgi/uXVjzlUUx/r0iSGLBqXYiwqKnLFxcUR/1yRzqSqtoH//tMaZizYSGF2Cj+dNIILBuTFuiyJEjNb4pwrau05tT5E4lRKyM93rxvGK3edT8BnfPnxxXznlRXsOVQT69LkNFNQi8S5or5d+OPUC7nrwv68umw7n3v4HZ5esFEXeOpEFNQiCSAl5Oeha4Yy7/4LGdkzm+//YRVX/PxdXv+oVIvqdgIKapEEMrBrOr+981ym3zoGvxn3/O9Srv9l0/WutfSXdymoRRKMmXHF8G7Mu/9CfvbFkZRX13HHUx9y86/f13Q+j9KsD5EEV1vfyMvFW3nkrbXsKq9h3MBcHrx8CGP65MS6NGmHz5r1oaAW8YjqugaeW7SZX/91PXsO1XLR4HymXjaIs3srsBOBglqkE6msrefZ9zfzm7+uZ39lHWP7d+Huiwdy4aA8zCzW5ckJKKhFOqHDNfW88MEWnpi/kZ3l1ZzRLYOvju3DjaMLSU8KxLo8OYaCWqQTq61v5LXl23lm4SY+2VFOelKAiaN68MWiXozomaVRdpxQUIsIzjmWbz3Ab9/fzOsfl1JT38jggnQmjenJxFGFFGQmx7rETk1BLSJHKa+uY86KUl5ZspVlWw7gMxg3MI+bRhdy+bACMpKDsS6x01FQi8gJrS87xGvLtjNz6Xa2H6giFPDxuSH5XDuiBxcNzicrRaF9OiioReSkGhsdS7fsZ85Hpbz+cSllFTX4fcaYPjlcPCSf8QPzGN4jC79PPe1oUFCLSLs0NDqWb93PW6t389bqMkpKywHITA4wtn8u5w/IZWz/XIYUZOBTcEeEglpEOmR3eTXvb9jLwnV7WbhhD1v3VQGQnRrk7N45jO6VzejeOZxVmEVWqlolp+KzglqTKUXkpLpmJjNxVCETRxUCsG1/JYs37GPxxr0s23KAt1bvPvLanjkpnNkji2E9MhnaPZOh3TMozE7RNMAOUFCLSLv1zEml55hUvjCmJwAHq+pYsfUAn+woZ+WOg6zaUc4bq3bS/A97WsjPwK7pDOiazoD8dPrkptI3N43eualkaobJSan1ISJRcbimntU7K1i9s5y1uw6xbvch1u6uYFf50SvU5KQG6dUllV45qRTmpFCYnUL3rGS6ZSVTkJlMXnpSpziA2eHWh5ldBUwD/MATzrn/jGB9IuJBaUkBxvTJOe4qfodr6tmyr5JNew6zZV/lkVtJaTl/KdlFTf3RK9f4DHLTk+ia0XTLTU8iLz2JvPQQ2akhuqQFyU4NkZMaIjslSGZK0HPBftKgNjM/8ChwObAN+NDMZjvnVkW7OBHxnrSkQLh3nXncc8459hyqZefBanaWV7MrfNtdXsPuimp2V9RQUlrB3sM11DWcuBuQkRQgMyVIRnIgfAuSnhQgLSlAWshPWlKA1JCf1JCflFCAlKCflJCP5KC/6RbwkxT0kRTwkRT+PuRvusVilktbRtTnAuuccxsAzOxFYCKgoBaRiDIz8jOSyM9I4iyyTvg65xzl1fUcqKxlf2Ud+w/XcqCqlgOVdRyorKO8uo7yqnrKq+s4VF1PWUUNG8oOcbi2gcM19VTWNpxyjQGfEfT7CPqNUMBHwOcj4G96LD89iZe/cf4pf/YJf2YbXlMIbG1xfxtwXsQrERFpIzMjKyVIVkqQPrntf39jo6OqroHK2gaqahuoqmsI36+npr6RmrpGauobPv1a30hNfSN1DY3Uhr/WNbjw10bqGxx1jY70JH/kN5YIzvowsynAFIDevXtH6mNFRCLO57OmNkiCXO61LWsmbgd6tbjfM/zYUZxz051zRc65ovz8/EjVJyLS6bUlqD8EBplZPzMLAZOB2dEtS0REmp103O+cqzeze4E3aJqe96Rz7pOoVyYiIkAbe9TOubnA3CjXIiIirWhL60NERGJIQS0iEucU1CIicU5BLSIS56Jy9TwzKwM2n+Lb84A9ESwnEXTGbYbOud2dcZuhc253e7e5j3Ou1ZNQohLUHWFmxSe61J9XdcZths653Z1xm6Fzbnckt1mtDxGROKegFhGJc/EY1NNjXUAMdMZths653Z1xm6FzbnfEtjnuetQiInK0eBxRi4hICwpqEZE4FzdBbWZXmdkaM1tnZv8U63qixcx6mdnbZrbKzD4xs6nhx7uY2Z/NbG34a87JPivRmJnfzJaZ2Zzw/X5mtji8z18KX0bXU8ws28x+Z2arzazEzM73+r42swfCv9srzewFM0v24r42syfNbLeZrWzxWKv71pr8Irz9H5nZ2e35WXER1C0W0L0aGAbcYmbDYltV1NQD33bODQPGAveEt/WfgDedc4OAN8P3vWYqUNLi/k+A/3HODQT2A3fGpKromgbMc86dAYykafs9u6/NrBC4Dyhyzp1J06WRJ+PNff00cNUxj51o314NDArfpgCPtesnOedifgPOB95ocf8h4KFY13Watn0WTSu8rwG6hx/rDqyJdW0R3s6e4V/cS4A5gNF01lagtd8BL9yALGAj4YP2LR737L7m0zVWu9B0GeU5wJVe3ddAX2DlyfYt8BvgltZe15ZbXIyoaX0B3cIY1XLamFlfYDSwGChwzpWGn9oJFMSorGj5OfAPQGP4fi5wwDlXH77vxX3eDygDngq3fJ4wszQ8vK+dc9uBh4EtQClwEFiC9/d1sxPt2w5lXN/19AYAAAG1SURBVLwEdadjZunA74H7nXPlLZ9zTX9yPTNv0syuA3Y755bEupbTLACcDTzmnBsNHOaYNocH93UOMJGmP1I9gDSObw90CpHct/ES1G1aQNcrzCxIU0g/75ybGX54l5l1Dz/fHdgdq/qiYBxwg5ltAl6kqf0xDcg2s+ZVhry4z7cB25xzi8P3f0dTcHt5X18GbHTOlTnn6oCZNO1/r+/rZifatx3KuHgJ6k6zgK6ZGTADKHHO/azFU7OBr4W//xpNvWtPcM495Jzr6ZzrS9O+fcs59xXgbWBS+GWe2mYA59xOYKuZDQk/dCmwCg/va5paHmPNLDX8u968zZ7e1y2caN/OBm4Lz/4YCxxs0SI5uVg341s0168B/gasB/4l1vVEcTvH0/Tv0EfA8vDtGpp6tm8Ca4G/AF1iXWuUtv9iYE74+/7AB8A64BUgKdb1RWF7RwHF4f39GpDj9X0N/ABYDawEfgskeXFfAy/Q1Ievo+m/pztPtG9pOnj+aDjfPqZpVkybf5ZOIRcRiXPx0voQEZETUFCLiMQ5BbWISJxTUIuIxDkFtYhInFNQi4jEOQW1iEic+z8zak/ZtDkKDgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_InTuIU-lTG"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIJF1cdi-pbe"
      },
      "source": [
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=True)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuAmK76A_BX0"
      },
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pliFuhCm_DP7",
        "outputId": "34873cb6-8a35-43fb-cf72-5f9ca9c730bf"
      },
      "source": [
        "train_translator.fit(train_dataset, epochs=100, callbacks=[batch_loss])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 6s 136ms/step - batch_loss: 5.1815\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 127ms/step - batch_loss: 4.8788\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 164ms/step - batch_loss: 5.8877\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 122ms/step - batch_loss: 4.2929\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 155ms/step - batch_loss: 4.5872\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 130ms/step - batch_loss: 4.5188\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 161ms/step - batch_loss: 4.3331\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 161ms/step - batch_loss: 4.0390\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 129ms/step - batch_loss: 3.8866\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 168ms/step - batch_loss: 3.9502\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 165ms/step - batch_loss: 3.8632\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 176ms/step - batch_loss: 3.7007\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 142ms/step - batch_loss: 3.6371\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 164ms/step - batch_loss: 3.6204\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 123ms/step - batch_loss: 3.5310\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 160ms/step - batch_loss: 3.5487\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 166ms/step - batch_loss: 3.4044\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 141ms/step - batch_loss: 3.3982\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 167ms/step - batch_loss: 3.3495\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 153ms/step - batch_loss: 3.2924\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 121ms/step - batch_loss: 3.2355\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 117ms/step - batch_loss: 3.1792\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 126ms/step - batch_loss: 3.0803\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 144ms/step - batch_loss: 3.0368\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 136ms/step - batch_loss: 3.0315\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 146ms/step - batch_loss: 2.9375\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 149ms/step - batch_loss: 2.8652\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 174ms/step - batch_loss: 2.7792\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 135ms/step - batch_loss: 2.7129\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 123ms/step - batch_loss: 2.5779\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 142ms/step - batch_loss: 2.5335\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 176ms/step - batch_loss: 2.4634\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 151ms/step - batch_loss: 2.3912\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 146ms/step - batch_loss: 2.3231\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 160ms/step - batch_loss: 2.2649\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 126ms/step - batch_loss: 2.1294\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 122ms/step - batch_loss: 2.0443\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 124ms/step - batch_loss: 1.9776\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 156ms/step - batch_loss: 1.8331\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 164ms/step - batch_loss: 1.7761\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 150ms/step - batch_loss: 1.6256\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 174ms/step - batch_loss: 1.5642\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 160ms/step - batch_loss: 1.4408\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 166ms/step - batch_loss: 1.3322\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 140ms/step - batch_loss: 1.2739\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 0s 130ms/step - batch_loss: 1.1602\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 0s 171ms/step - batch_loss: 1.0864\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 0s 179ms/step - batch_loss: 0.9883\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 0s 177ms/step - batch_loss: 0.9223\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 0s 178ms/step - batch_loss: 0.8210\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 0s 175ms/step - batch_loss: 0.7599\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 0s 157ms/step - batch_loss: 0.6790\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 150ms/step - batch_loss: 0.5983\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 154ms/step - batch_loss: 0.5680\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 125ms/step - batch_loss: 0.5152\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 120ms/step - batch_loss: 0.4470\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 151ms/step - batch_loss: 0.3784\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 165ms/step - batch_loss: 0.3386\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 157ms/step - batch_loss: 0.3358\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 153ms/step - batch_loss: 0.2805\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 157ms/step - batch_loss: 0.2634\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 148ms/step - batch_loss: 0.2166\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 145ms/step - batch_loss: 0.1967\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 167ms/step - batch_loss: 0.1685\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 168ms/step - batch_loss: 0.1485\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 170ms/step - batch_loss: 0.1310\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 161ms/step - batch_loss: 0.1222\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 162ms/step - batch_loss: 0.1067\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 147ms/step - batch_loss: 0.0899\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 137ms/step - batch_loss: 0.0785\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 169ms/step - batch_loss: 0.0722\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 0s 146ms/step - batch_loss: 0.0645\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 0s 129ms/step - batch_loss: 0.0532\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 0s 180ms/step - batch_loss: 0.0574\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 0s 162ms/step - batch_loss: 0.0500\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 0s 170ms/step - batch_loss: 0.0431\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 0s 167ms/step - batch_loss: 0.0375\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 0s 165ms/step - batch_loss: 0.0353\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 0s 166ms/step - batch_loss: 0.0316\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 0s 163ms/step - batch_loss: 0.0288\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 0s 118ms/step - batch_loss: 0.0275\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 0s 124ms/step - batch_loss: 0.0247\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 0s 166ms/step - batch_loss: 0.0216\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 0s 169ms/step - batch_loss: 0.0207\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 0s 124ms/step - batch_loss: 0.0193\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 0s 128ms/step - batch_loss: 0.0173\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 0s 142ms/step - batch_loss: 0.0174\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 0s 154ms/step - batch_loss: 0.0157\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 0s 137ms/step - batch_loss: 0.0169\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 0s 161ms/step - batch_loss: 0.0149\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 0s 148ms/step - batch_loss: 0.0141\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 0s 159ms/step - batch_loss: 0.0130\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 0s 129ms/step - batch_loss: 0.0128\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 0s 156ms/step - batch_loss: 0.0125\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 0s 171ms/step - batch_loss: 0.0118\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 0s 163ms/step - batch_loss: 0.0113\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 0s 127ms/step - batch_loss: 0.0112\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 0s 166ms/step - batch_loss: 0.0112\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 0s 169ms/step - batch_loss: 0.0103\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 0s 127ms/step - batch_loss: 0.0101\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee406c4f90>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFCAm2xqAdx4",
        "outputId": "f6ad8440-15c7-465d-d5d9-35d5eee2e731"
      },
      "source": [
        "losses = []\n",
        "for test_input_batch, test_target_batch in test_dataset:\n",
        "  logs = train_translator._test_step([test_input_batch, test_target_batch])\n",
        "  losses.append(logs['batch_loss'].numpy())\n",
        "print(\"Test loss: \",losses)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss:  [0.0051593324]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15SEZQ5062pY"
      },
      "source": [
        "# Create a Translater"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pboiGcpcDPww"
      },
      "source": [
        "class Translator(tf.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('[START]'))\n",
        "    self.end_token = index_from_string(tf.constant('[END]'))\n",
        "\n",
        "  def tokens_to_text(self, result_tokens):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(result_tokens, ('batch', 't'))\n",
        "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "    shape_checker(result_text_tokens, ('batch', 't'))\n",
        "\n",
        "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                        axis=1, separator=' ')\n",
        "    shape_checker(result_text, ('batch'))\n",
        "\n",
        "    result_text = tf.strings.strip(result_text)\n",
        "    shape_checker(result_text, ('batch',))\n",
        "    return result_text\n",
        "\n",
        "  def sample(self, logits, temperature):\n",
        "    shape_checker = ShapeChecker()\n",
        "    # 't' is usually 1 here.\n",
        "    shape_checker(logits, ('batch', 't', 'vocab'))\n",
        "    shape_checker(self.token_mask, ('vocab',))\n",
        "\n",
        "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "    shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
        "\n",
        "    # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "    if temperature == 0.0:\n",
        "      new_tokens = tf.argmax(logits, axis=-1)\n",
        "    else: \n",
        "      logits = tf.squeeze(logits, axis=1)\n",
        "      new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                          num_samples=1)\n",
        "\n",
        "    shape_checker(new_tokens, ('batch', 't'))\n",
        "\n",
        "    return new_tokens\n",
        "\n",
        "  def translate_unrolled(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "    batch_size = tf.shape(input_text)[0]\n",
        "    input_tokens = self.input_text_processor(input_text)\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "    dec_state = enc_state\n",
        "    new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "    result_tokens = []\n",
        "    attention = []\n",
        "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "      dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                              enc_output=enc_output,\n",
        "                              mask=(input_tokens!=0))\n",
        "\n",
        "      dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "      attention.append(dec_result.attention_weights)\n",
        "\n",
        "      new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "      # If a sequence produces an `end_token`, set it `done`\n",
        "      done = done | (new_tokens == self.end_token)\n",
        "      # Once a sequence is done it only produces 0-padding.\n",
        "      new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "      # Collect the generated tokens\n",
        "      result_tokens.append(new_tokens)\n",
        "\n",
        "      if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "        break\n",
        "\n",
        "    # Convert the list of generates token ids to a list of strings.\n",
        "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "    result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "    if return_attention:\n",
        "      attention_stack = tf.concat(attention, axis=1)\n",
        "      return {'text': result_text, 'attention': attention_stack}\n",
        "    else:\n",
        "      return {'text': result_text}"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4UwhwiNDRU6"
      },
      "source": [
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5-Ln_ZcQDgI"
      },
      "source": [
        "tel1, tel2, tel3, tel4, tel5, eng1, eng2, eng3, eng4, eng5 = \"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n",
        "\n",
        "t1 = 'నా పేరు శ్రీనాథ్'\n",
        "for test_input_batch, test_target_batch in test_dataset:\n",
        "  tel1 = test_input_batch[0].numpy().decode()\n",
        "  eng1 = test_target_batch[0].numpy().decode()\n",
        "  tel2 = test_input_batch[1].numpy().decode()\n",
        "  eng2 = test_target_batch[1].numpy().decode()\n",
        "  tel3 = test_input_batch[6].numpy().decode()\n",
        "  eng3 = test_target_batch[6].numpy().decode()\n",
        "  tel4 = test_input_batch[8].numpy().decode()\n",
        "  eng4 = test_target_batch[8].numpy().decode()\n",
        "  tel5 = test_input_batch[13].numpy().decode()\n",
        "  eng5 = test_target_batch[13].numpy().decode()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcmMwMfkDbAR",
        "outputId": "3d6dc534-e717-4bd0-ad55-7d3e09f8f0a1"
      },
      "source": [
        "%%time\n",
        "input_text = tf.constant([tel1, tel2, tel3, tel4, tel5, t1])\n",
        "\n",
        "result = translator.translate_unrolled(\n",
        "    input_text = input_text)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 246 ms, sys: 778 µs, total: 247 ms\n",
            "Wall time: 240 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yq5nBjmU585"
      },
      "source": [
        "## Correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIRuKnDW9fsu",
        "outputId": "6b3fca6a-a6b5-4086-b51b-6c5fa4571ae2"
      },
      "source": [
        "print(tel1)\n",
        "print(result['text'][0].numpy().decode())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "నన్ను సహాయం అడగడానికి ఏం సందేహ పడొద్దు\n",
            "dont hesitate to ask me for help .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVkHgKum9mh-",
        "outputId": "7c712ffa-7ba9-41af-80fc-da849d11f77a"
      },
      "source": [
        "print(tel2)\n",
        "print(result['text'][1].numpy().decode())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "నేనూ తప్పులు చేస్తాను\n",
            "i do make mistakes .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL81cHuIUONC",
        "outputId": "ef9b1bac-ab0d-4b8e-8612-a8ee448105dc"
      },
      "source": [
        "print(tel3)\n",
        "print(result['text'][2].numpy().decode())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "అది నిజంగా అంత చెడ్డదా ?\n",
            "was it really that bad ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4auQIPsYVZJf"
      },
      "source": [
        "## InCorrect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANHafcabUnz1",
        "outputId": "7433f872-0321-4dd9-de1f-71fd18a3e42e"
      },
      "source": [
        "print(tel4)\n",
        "print(result['text'][3].numpy().decode())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "నేను కాఫీ తాగాను\n",
            "i drank coffee .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfgEBV13UoA6",
        "outputId": "5f3e218d-6a8f-4dff-f264-ec5ec0556c00"
      },
      "source": [
        "print(tel5)\n",
        "print(result['text'][4].numpy().decode())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "చలనచిత్రం మొదలు అవ్వబోతుంది\n",
            "the movies about to start .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57C_mtgkiy8V",
        "outputId": "aa26de16-10dc-4ff7-b6c4-d272d70d0108"
      },
      "source": [
        "print(t1)\n",
        "print(result['text'][5].numpy().decode())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "నా పేరు శ్రీనాథ్\n",
            "i dont speak japanese .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JM4SVkfDE1C"
      },
      "source": [
        "# H1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwObt7u4EI0c"
      },
      "source": [
        "a = result['attention'][0]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "pYh7LdveEKAv",
        "outputId": "d40aa2b9-d89a-411d-a9ad-3c40ea3e99c1"
      },
      "source": [
        "_ = plt.bar(range(len(a[0, :])), a[0, :])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1YUpD10ERuj"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  sentence = tf_lower_and_split_punct(sentence).numpy().decode().split()\n",
        "  predicted_sentence = predicted_sentence.numpy().decode().split() + ['[END]']\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  attention = attention[:len(predicted_sentence), :len(sentence)]\n",
        "\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')\n",
        "  plt.suptitle('Attention weights')"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pfcb0ZEfES7i",
        "outputId": "f0623f39-a6d1-4325-c3bb-20ad2419b1cd"
      },
      "source": [
        "i=1\n",
        "plot_attention(result['attention'][i], input_text[i].numpy().decode(), result['text'][i])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3112 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3135 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3149 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3137 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3079 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3093 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3105 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3074 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3134 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3086 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3125 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3128 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3118 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3119 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3098 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3122 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3143 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3112 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3135 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3149 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3137 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3079 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3093 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3105 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3074 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3134 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3086 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3125 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3128 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3118 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3119 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3098 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3122 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3143 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1lBga5tbDy4"
      },
      "source": [
        "# Questions & Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJlNcedYbJFi"
      },
      "source": [
        "Q1-)Which parts of the sentence are used as a token? Each character, each word, or are some words split up?\n",
        "\n",
        "Ans-) Each word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSFabVhgb6NA"
      },
      "source": [
        "Q-2) Do the same tokens in different language have the same ID?\n",
        "e.g. Would the same token index map to the German word die and to the English word die?\n",
        "\n",
        "\n",
        "Ans-) No. \n",
        "\n",
        "As example we can see below:\n",
        "\n",
        "*  inp = 'die hallo morgen guten tag'\n",
        "*  tar = 'Someone might die because of the situation'\n",
        "*  inp_voc = [0 1 2 3 4 5]\n",
        "*  tar_voc = [0 1 2 3 4 5 6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43vetYtVdNt0"
      },
      "source": [
        "Q-3)What is the relation between the encoder output and the encoder hidden state which is used to initialize the decoder hidden state (for the architecture used in the tutorial)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtXk0eDdfIgC"
      },
      "source": [
        "Q-4) Is the decoder attending to all previous positions, including the previous decoder predictions?\n",
        "\n",
        "Ans-) No it will attend to just the previous decoder state and predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiRXSZ3zfxpc"
      },
      "source": [
        "Q-5) Does the encoder output change in different decoding steps?\n",
        "\n",
        "Ans-) Yes it computes attention weights dynamically for every decoder step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgrMHrqvgNxN"
      },
      "source": [
        "Q-6) Does the context vector change in different decoding steps?\n",
        "\n",
        "Ans-) Yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "molLOQfNgStB"
      },
      "source": [
        "Q-7) The decoder uses teacher forcing. Does this mean the time steps can be computed in parallel?\n",
        "\n",
        "Ans-) No because even the previous decoder state is also connected to the current decoder state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnFce_9CgeTR"
      },
      "source": [
        "Q-8) Why is a mask applied to the loss function?\n",
        "\n",
        "Ans-) To skip the zero padded cells in the sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5KlAkttgwos"
      },
      "source": [
        "Q-9) When translating the same sentence multiple times, do you get the same result? Why (not)? If not, what changes need to be made to get the same result each time?\n",
        "\n",
        "Ans-) The output is not consistent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGd5aFjjm4zZ"
      },
      "source": [
        "# References\n",
        "\n",
        "\n",
        "\n",
        "1.   https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "2.   https://ovgu-ailab.github.io/idl2021/ass7.html\n",
        "\n"
      ]
    }
  ]
}