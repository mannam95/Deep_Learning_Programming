{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_7_NMT_Srinath.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9b1ec548",
        "0c34550c",
        "sHbpSCdw8qnW",
        "yh0bPt7wkCHt",
        "JEIAwPgokXk6",
        "S6LD6RwikuOw",
        "6UKOUGEX0Z75",
        "J1JQ2_On36HO",
        "0LiDrt7y3-lW",
        "VQujLhNg4BrF",
        "3HCh9PfH4oZK",
        "7reyM4fc4rmJ",
        "ijN0V-aI46-x",
        "XE7o0uJU4rh2",
        "XQI6bl3P65rr",
        "15SEZQ5062pY",
        "_JM4SVkfDE1C"
      ],
      "machine_shape": "hm",
      "mount_file_id": "156VNP0l29aIE4p0WHewhNZn95t-7XKvN",
      "authorship_tag": "ABX9TyMgyNfqLybrwJkK/i5TXGkX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mannam95/Deep_Learning_Programming/blob/main/Assignment7/Assignment_7_NMT_Srinath.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6aef94d"
      },
      "source": [
        "# Team Assignment\n",
        "\n",
        "\n",
        "1.   Srinath Mannam (229750)\n",
        "2.   Meghana Rao (234907)\n",
        "3.   Govind Shukla (235192)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b1ec548"
      },
      "source": [
        "# import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQCgf3jKQ8L3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee86798e-7e71-452f-a9bd-150af60b5f47"
      },
      "source": [
        "pip install tensorflow_text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (12.0.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.22.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.13.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.37.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.42.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.1.1)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85eb7211"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.preprocessing.sequence as sequence\n",
        "import tensorflow_text as tf_text\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c34550c"
      },
      "source": [
        "# Change the directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94b2626e"
      },
      "source": [
        "working_directory = '/content/drive/My Drive/Colab Notebooks/OVGU/Deep_Learning/07_Assignment'\n",
        "def colabDrive():\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "    if os.getcwd() !=  working_directory:\n",
        "      os.chdir(working_directory)\n",
        "    print(os.getcwd())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8540ed4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beafcc58-2487-4282-aa96-1086d403e083"
      },
      "source": [
        "colabDrive()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Colab Notebooks/OVGU/Deep_Learning/07_Assignment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjeunnuphktY"
      },
      "source": [
        "# Preamble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHbpSCdw8qnW"
      },
      "source": [
        "## Helper function which can be used to check the shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M15xw1ve8teI"
      },
      "source": [
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh0bPt7wkCHt"
      },
      "source": [
        "## A function that will read the data and return"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvTtHPvjhqBC"
      },
      "source": [
        "def load_data(path):\n",
        "  # text = path.read_text(encoding='utf-8')\n",
        "  with open(\"tel-eng/tel.txt\", \"r\", encoding='utf-8') as tel:\n",
        "     telugu=tel.read()\n",
        "\n",
        "  lines = telugu.splitlines()\n",
        "  pairs = [line.split('\\t')[:-1] for line in lines]\n",
        "\n",
        "  inp = [inp for targ, inp in pairs]\n",
        "  targ = [targ for targ, inp in pairs]\n",
        "\n",
        "  return targ, inp"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cOKg7mXhtw5"
      },
      "source": [
        "targ, inp = load_data(\"tel-eng/tel.txt\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoZqzF-_kMe-",
        "outputId": "08670340-c58d-42e4-e68f-f8d7405def07"
      },
      "source": [
        "print(inp[-1])\n",
        "print(targ[-1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "టామ్ తనకు తాను చేయటానికి అనుమతించకపోవచ్చని అనుకున్నాడు.\n",
            "Tom thought he might not be permitted to do that by himself.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEIAwPgokXk6"
      },
      "source": [
        "## Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvwzLTtIkU--"
      },
      "source": [
        "BUFFER_SIZE = len(inp)\n",
        "BATCH_SIZE = 60\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "test_dataset = dataset.take(20) \n",
        "train_dataset = dataset.skip(20)\n",
        "\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.batch(20).shuffle(10, seed=45)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrxWy_-3kbCL",
        "outputId": "07b6cd76-c47e-4c8a-966e-8d088ddad444"
      },
      "source": [
        "for example_input_batch, example_target_batch in train_dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\\xe0\\xb0\\xa8\\xe0\\xb1\\x81\\xe0\\xb0\\xb5\\xe0\\xb1\\x8d\\xe0\\xb0\\xb5\\xe0\\xb1\\x81 \\xe0\\xb0\\x8e\\xe0\\xb0\\x95\\xe0\\xb1\\x8d\\xe0\\xb0\\x95\\xe0\\xb0\\xa1 \\xe0\\xb0\\x89\\xe0\\xb0\\xa8\\xe0\\xb1\\x8d\\xe0\\xb0\\xa8\\xe0\\xb0\\xb5\\xe0\\xb1\\x8b \\xe0\\xb0\\x85\\xe0\\xb0\\x95\\xe0\\xb1\\x8d\\xe0\\xb0\\x95\\xe0\\xb0\\xa1\\xe0\\xb1\\x87 \\xe0\\xb0\\x89\\xe0\\xb0\\x82\\xe0\\xb0\\xa1\\xe0\\xb1\\x81'\n",
            " b'\\xe0\\xb0\\xa8\\xe0\\xb1\\x80 \\xe0\\xb0\\x95\\xe0\\xb0\\xb2\\xe0\\xb0\\x82 \\xe0\\xb0\\x8f\\xe0\\xb0\\xa6\\xe0\\xb0\\xbf ?'\n",
            " b'\\xe0\\xb0\\xa8\\xe0\\xb0\\xbf\\xe0\\xb0\\xa8\\xe0\\xb1\\x8d\\xe0\\xb0\\xa8 \\xe0\\xb0\\xb0\\xe0\\xb0\\xbe\\xe0\\xb0\\xa4\\xe0\\xb1\\x8d\\xe0\\xb0\\xb0\\xe0\\xb0\\xbf\\xe0\\xb0\\x95\\xe0\\xb0\\xbf \\xe0\\xb0\\xa8\\xe0\\xb0\\xa8\\xe0\\xb1\\x8d\\xe0\\xb0\\xa8\\xe0\\xb1\\x81 \\xe0\\xb0\\xa8\\xe0\\xb0\\xbf\\xe0\\xb0\\x9c\\xe0\\xb0\\x82\\xe0\\xb0\\x97\\xe0\\xb0\\xbe \\xe0\\xb0\\x95\\xe0\\xb1\\x8d\\xe0\\xb0\\xb7\\xe0\\xb0\\xae\\xe0\\xb0\\xbf\\xe0\\xb0\\x82\\xe0\\xb0\\x9a\\xe0\\xb1\\x81'\n",
            " b'\\xe0\\xb0\\xa4\\xe0\\xb0\\xa8\\xe0\\xb1\\x81 \\xe0\\xb0\\xb8\\xe0\\xb1\\x8d\\xe0\\xb0\\x95\\xe0\\xb1\\x82\\xe0\\xb0\\xb2\\xe0\\xb1\\x81\\xe0\\xb0\\x95\\xe0\\xb1\\x81 \\xe0\\xb0\\xb5\\xe0\\xb1\\x86\\xe0\\xb0\\xb3\\xe0\\xb1\\x8d\\xe0\\xb0\\xb3\\xe0\\xb1\\x87 \\xe0\\xb0\\xa6\\xe0\\xb0\\xbe\\xe0\\xb0\\xb0\\xe0\\xb0\\xbf\\xe0\\xb0\\xb2\\xe0\\xb1\\x8b \\xe0\\xb0\\xb5\\xe0\\xb1\\x81\\xe0\\xb0\\x82\\xe0\\xb0\\xa6\\xe0\\xb0\\xbf'\n",
            " b'\\xe0\\xb0\\x87\\xe0\\xb0\\xa6\\xe0\\xb0\\xbf \\xe0\\xb0\\x92\\xe0\\xb0\\x95 \\xe0\\xb0\\xb5\\xe0\\xb0\\xbe\\xe0\\xb0\\x95\\xe0\\xb1\\x8d\\xe0\\xb0\\xaf\\xe0\\xb0\\x82 \\xe0\\xb0\\x95\\xe0\\xb0\\xbe\\xe0\\xb0\\xa6\\xe0\\xb1\\x81.'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b'You must stay where you are.' b'Which is your pen?'\n",
            " b\"I'm really sorry about last night.\" b'She was on her way to school.'\n",
            " b'This is not a sentence.'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6LD6RwikuOw"
      },
      "source": [
        "## Unicode Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpgbPhcfkw4V"
      },
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accecented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, ఀ-౿(telugu text) and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z$ఀ-౿.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UKOUGEX0Z75"
      },
      "source": [
        "## Text Vectorization\n",
        "\n",
        "Creating a word to index dictionary and an index to word dictionary for all unique source and target words in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1JQ2_On36HO"
      },
      "source": [
        "### For Telugu language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnStPmuy0ci5",
        "outputId": "b529e2b0-a577-44bb-fc1d-a046d659c967"
      },
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "input_text_processor.adapt(inp)\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "input_text_processor.get_vocabulary()[:10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '?', '.', 'నువ్వు', 'నేను', 'అది', 'నాకు']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LiDrt7y3-lW"
      },
      "source": [
        "### For English language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2n7U_B00ebs",
        "outputId": "0149d65d-544d-4a27-c8a8-7a18e95317a5"
      },
      "source": [
        "output_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(targ)\n",
        "output_text_processor.get_vocabulary()[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'you', '?', 'to', 'i', 'the']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQujLhNg4BrF"
      },
      "source": [
        "### See some tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DubharMb1QAt",
        "outputId": "15038179-0b98-433f-cf1c-1a7745622cea"
      },
      "source": [
        "example_inp_tokens = input_text_processor('నువ్వు అది చూసావా?')\n",
        "example_inp_tokens"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6,), dtype=int64, numpy=array([  2,   6,   8, 311,   4,   3])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef80Q88-1aEa",
        "outputId": "85457ede-43a6-4715-827f-c8fd59eec823"
      },
      "source": [
        "example_tar_tokens = output_text_processor('Can you see that?')\n",
        "example_tar_tokens"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(7,), dtype=int64, numpy=array([  2,  30,   5, 199,  10,   6,   3])>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BWNVkXh74dU"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z-Q4E8A78xB"
      },
      "source": [
        "embedding_dim = 256\n",
        "units = 1024"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HCh9PfH4oZK"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfVSM8z87_ok"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(tokens, ('batch', 's'))\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    #    output shape: (batch, s, enc_units)\n",
        "    #    state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
        "    shape_checker(state, ('batch', 'enc_units'))\n",
        "\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaAwgen08g35",
        "outputId": "5e1b9282-7c23-4bd5-f353-bb9e0dd52f91"
      },
      "source": [
        "# Convert the input text to tokens.\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "\n",
        "# Encode the input sequence.\n",
        "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)\n",
        "example_enc_output, example_enc_state = encoder(example_tokens)\n",
        "\n",
        "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
        "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
        "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch, shape (batch): (60,)\n",
            "Input batch tokens, shape (batch, s): (60, 12)\n",
            "Encoder output, shape (batch, s, units): (60, 12, 1024)\n",
            "Encoder state, shape (batch, units): (60, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7reyM4fc4rmJ"
      },
      "source": [
        "# Attention Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijN0V-aI46-x"
      },
      "source": [
        "## Bahdanau Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nMlsrbr4ud7"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(query, ('batch', 't', 'query_units'))\n",
        "    shape_checker(value, ('batch', 's', 'value_units'))\n",
        "    shape_checker(mask, ('batch', 's'))\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwXBo95d5Puv"
      },
      "source": [
        "attention_layer = BahdanauAttention(units)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az3SlShz5caC",
        "outputId": "de7dfb5f-9a8e-4685-a2c2-a3ff0402d32b"
      },
      "source": [
        "# Later, the decoder will generate this attention query\n",
        "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
        "\n",
        "# Attend to the encoded tokens\n",
        "\n",
        "context_vector, attention_weights = attention_layer(\n",
        "    query=example_attention_query,\n",
        "    value=example_enc_output,\n",
        "    mask=(example_tokens != 0))\n",
        "\n",
        "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
        "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch_size, query_seq_length, units):           (60, 2, 1024)\n",
            "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (60, 2, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE7o0uJU4rh2"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2LzXKtk6JtR"
      },
      "source": [
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq5z9qVo50Wt"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # For Step 1. The embedding layer convets token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # For step 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    # For step 5. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
        "\n",
        "  def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(inputs.new_tokens, ('batch', 't'))\n",
        "    shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
        "    shape_checker(inputs.mask, ('batch', 's'))\n",
        "\n",
        "    if state is not None:\n",
        "      shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "    # Step 1. Lookup the embeddings\n",
        "    vectors = self.embedding(inputs.new_tokens)\n",
        "    shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
        "\n",
        "    # Step 2. Process one step with the RNN\n",
        "    rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "    shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
        "    shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "    # Step 3. Use the RNN output as the query for the attention over the\n",
        "    # encoder output.\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "    shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "    #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "    # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "    attention_vector = self.Wc(context_and_rnn_output)\n",
        "    shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
        "\n",
        "    # Step 5. Generate logit predictions:\n",
        "    logits = self.fc(attention_vector)\n",
        "    shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
        "\n",
        "    return DecoderOutput(logits, attention_weights), state"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwSvsa4R56GQ"
      },
      "source": [
        "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQpJ5Xej607m"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQI6bl3P65rr"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "Summary:\n",
        "\n",
        "\n",
        "*   Sparse Categorical crossentropy is the loss function\n",
        "*   Not computing the loss where padding is applied\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOPAFtbb637r"
      },
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(y_true, ('batch', 't'))\n",
        "    shape_checker(y_pred, ('batch', 't', 'logits'))\n",
        "\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    shape_checker(loss, ('batch', 't'))\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    shape_checker(mask, ('batch', 't'))\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcnrzfLV6_BN"
      },
      "source": [
        "## Train Model\n",
        "\n",
        "Summary\n",
        "\n",
        "\n",
        "1.   A constructor which will initialise Encoder, Decoder etc\n",
        "2.   A **train_step** function which will choose which function to run whether the train step function which is wrapped with tf or the normal train step function.\n",
        "3.   A  **_preprocess** function which will preprocess the data.\n",
        "4.   A  **_train_step** function.\n",
        "5.   A  **_tf_train_step** function wrapped like a tf function.\n",
        "6.   A  **_loop_step** function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjNgJCTe7Bb_"
      },
      "source": [
        "class TrainTranslator(tf.keras.Model):\n",
        "  ## --------------------------------------------------Start of the Constructor----------------------------------------------------------\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor, \n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "    self.shape_checker = ShapeChecker()\n",
        "  ## --------------------------------------------------End of the Constructor----------------------------------------------------------\n",
        "\n",
        "  ## --------------------------------------------------Start of the Call----------------------------------------------------------\n",
        "  def call(self, inputs):\n",
        "    return self._test_step(inputs)\n",
        "  ## --------------------------------------------------End of the Call----------------------------------------------------------\n",
        "\n",
        "  ## --------------------------------------------------Start of the train step----------------------------------------------------------\n",
        "  #Choose which train step to run\n",
        "  def train_step(self, inputs):\n",
        "    self.shape_checker = ShapeChecker()\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)\n",
        "  ## --------------------------------------------------End of the train step----------------------------------------------------------\n",
        "\n",
        "  ## --------------------------------------------------Start of the Preprocess data function----------------------------------------------------------\n",
        "  def _preprocess(self, input_text, target_text):\n",
        "    self.shape_checker(input_text, ('batch',))\n",
        "    self.shape_checker(target_text, ('batch',))\n",
        "\n",
        "    # Convert the text to token IDs\n",
        "    input_tokens = self.input_text_processor(input_text)\n",
        "    target_tokens = self.output_text_processor(target_text)\n",
        "    self.shape_checker(input_tokens, ('batch', 's'))\n",
        "    self.shape_checker(target_tokens, ('batch', 't'))\n",
        "\n",
        "    # Convert IDs to masks.\n",
        "    input_mask = input_tokens != 0\n",
        "    self.shape_checker(input_mask, ('batch', 's'))\n",
        "\n",
        "    target_mask = target_tokens != 0\n",
        "    self.shape_checker(target_mask, ('batch', 't'))\n",
        "\n",
        "    return input_tokens, input_mask, target_tokens, target_mask\n",
        "  ## --------------------------------------------------End of the Preprocess data function----------------------------------------------------------\n",
        "\n",
        "  ## --------------------------------------------------Start of the train step function definition----------------------------------------------------------\n",
        "  def _train_step(self, inputs):\n",
        "    input_text, target_text = inputs  \n",
        "\n",
        "    (input_tokens, input_mask,\n",
        "    target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "    max_target_length = tf.shape(target_tokens)[1]\n",
        "    \n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Encode the input\n",
        "      enc_output, enc_state = self.encoder(input_tokens)\n",
        "      self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
        "      self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "      # Initialize the decoder's state to the encoder's final state.\n",
        "      # This only works if the encoder and decoder have the same number of\n",
        "      # units.\n",
        "      dec_state = enc_state\n",
        "      loss = tf.constant(0.0)\n",
        "\n",
        "      for t in tf.range(max_target_length-1):\n",
        "        # Pass in two tokens from the target sequence:\n",
        "        # 1. The current input to the decoder.\n",
        "        # 2. The target for the decoder's next prediction.\n",
        "        new_tokens = target_tokens[:, t:t+2]\n",
        "        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                              enc_output, dec_state)\n",
        "        loss = loss + step_loss\n",
        "\n",
        "      # Average the loss over all non padding tokens.\n",
        "      average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables \n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "  ## --------------------------------------------------End of the train step function definition----------------------------------------------------------\n",
        "\n",
        "\n",
        "  ## --------------------------------------------------Start of the test step evaluate function definition----------------------------------------------------------\n",
        "  @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "  def _test_step(self, inputs):\n",
        "    input_text, target_text = inputs  \n",
        "    \n",
        "    (input_tokens, input_mask,\n",
        "    target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "    max_target_length = tf.shape(target_tokens)[1]\n",
        "    \n",
        "    # Encode the input\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "    self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
        "    self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "    # Initialize the decoder's state to the encoder's final state.\n",
        "    # This only works if the encoder and decoder have the same number of\n",
        "    # units.\n",
        "    dec_state = enc_state\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    for t in tf.range(max_target_length-1):\n",
        "      # Pass in two tokens from the target sequence:\n",
        "      # 1. The current input to the decoder.\n",
        "      # 2. The target for the decoder's next prediction.\n",
        "      new_tokens = target_tokens[:, t:t+2]\n",
        "      step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                            enc_output, dec_state)\n",
        "      loss = loss + step_loss\n",
        "\n",
        "    # Average the loss over all non padding tokens.\n",
        "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "  ## --------------------------------------------------End of the test step evaluate function definition----------------------------------------------------------\n",
        "\n",
        "  ## --------------------------------------------------Start of the tf train step function ----------------------------------------------------------\n",
        "  # wrap train step into tf function to speed up the process\n",
        "  @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "  def _tf_train_step(self, inputs):\n",
        "    return self._train_step(inputs)\n",
        "  ## --------------------------------------------------Start of the tf train step function ----------------------------------------------------------\n",
        "\n",
        "\n",
        "  ## --------------------------------------------------Start of the train loop function----------------------------------------------------------\n",
        "  #The _loop_step method, added below, executes the decoder and calculates the incremental loss and new decoder state (dec_state).\n",
        "  def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "    # Run the decoder one step.\n",
        "    decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                                enc_output=enc_output,\n",
        "                                mask=input_mask)\n",
        "\n",
        "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "    self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
        "    self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
        "    self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
        "\n",
        "    # `self.loss` returns the total for non-padded tokens\n",
        "    y = target_token\n",
        "    y_pred = dec_result.logits\n",
        "    step_loss = self.loss(y, y_pred)\n",
        "\n",
        "    return step_loss, dec_state\n",
        "   ## --------------------------------------------------End of the train loop function----------------------------------------------------------"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIWxhEMq-hxK"
      },
      "source": [
        "## Test the Model to see whether it works or not for a single batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PBoLFRi7k-P"
      },
      "source": [
        "translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=True)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "923fPXrO9Gz9",
        "outputId": "1176b8bd-4110-4c95-e5f7-c43d59ef08d3"
      },
      "source": [
        "losses = []\n",
        "for n in range(100):\n",
        "  print('.', end='')\n",
        "  logs = translator.train_step([example_input_batch, example_target_batch])\n",
        "  losses.append(logs['batch_loss'].numpy())\n",
        "\n",
        "print()\n",
        "plt.plot(losses)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "....................................................................................................\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0d3ae3ef10>]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_InTuIU-lTG"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIJF1cdi-pbe"
      },
      "source": [
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=True)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuAmK76A_BX0"
      },
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pliFuhCm_DP7",
        "outputId": "34873cb6-8a35-43fb-cf72-5f9ca9c730bf"
      },
      "source": [
        "train_translator.fit(train_dataset, epochs=100, callbacks=[batch_loss])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 6s 136ms/step - batch_loss: 5.1815\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 127ms/step - batch_loss: 4.8788\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 164ms/step - batch_loss: 5.8877\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 122ms/step - batch_loss: 4.2929\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 155ms/step - batch_loss: 4.5872\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 130ms/step - batch_loss: 4.5188\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 161ms/step - batch_loss: 4.3331\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 161ms/step - batch_loss: 4.0390\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 129ms/step - batch_loss: 3.8866\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 168ms/step - batch_loss: 3.9502\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 165ms/step - batch_loss: 3.8632\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 176ms/step - batch_loss: 3.7007\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 142ms/step - batch_loss: 3.6371\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 164ms/step - batch_loss: 3.6204\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 123ms/step - batch_loss: 3.5310\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 160ms/step - batch_loss: 3.5487\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 166ms/step - batch_loss: 3.4044\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 141ms/step - batch_loss: 3.3982\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 167ms/step - batch_loss: 3.3495\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 153ms/step - batch_loss: 3.2924\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 121ms/step - batch_loss: 3.2355\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 117ms/step - batch_loss: 3.1792\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 126ms/step - batch_loss: 3.0803\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 144ms/step - batch_loss: 3.0368\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 136ms/step - batch_loss: 3.0315\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 146ms/step - batch_loss: 2.9375\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 149ms/step - batch_loss: 2.8652\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 174ms/step - batch_loss: 2.7792\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 135ms/step - batch_loss: 2.7129\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 123ms/step - batch_loss: 2.5779\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 142ms/step - batch_loss: 2.5335\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 176ms/step - batch_loss: 2.4634\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 151ms/step - batch_loss: 2.3912\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 146ms/step - batch_loss: 2.3231\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 160ms/step - batch_loss: 2.2649\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 126ms/step - batch_loss: 2.1294\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 122ms/step - batch_loss: 2.0443\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 124ms/step - batch_loss: 1.9776\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 156ms/step - batch_loss: 1.8331\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 164ms/step - batch_loss: 1.7761\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 150ms/step - batch_loss: 1.6256\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 174ms/step - batch_loss: 1.5642\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 160ms/step - batch_loss: 1.4408\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 166ms/step - batch_loss: 1.3322\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 140ms/step - batch_loss: 1.2739\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 0s 130ms/step - batch_loss: 1.1602\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 0s 171ms/step - batch_loss: 1.0864\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 0s 179ms/step - batch_loss: 0.9883\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 0s 177ms/step - batch_loss: 0.9223\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 0s 178ms/step - batch_loss: 0.8210\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 0s 175ms/step - batch_loss: 0.7599\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 0s 157ms/step - batch_loss: 0.6790\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 150ms/step - batch_loss: 0.5983\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 154ms/step - batch_loss: 0.5680\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 125ms/step - batch_loss: 0.5152\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 120ms/step - batch_loss: 0.4470\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 151ms/step - batch_loss: 0.3784\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 165ms/step - batch_loss: 0.3386\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 157ms/step - batch_loss: 0.3358\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 153ms/step - batch_loss: 0.2805\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 157ms/step - batch_loss: 0.2634\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 148ms/step - batch_loss: 0.2166\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 145ms/step - batch_loss: 0.1967\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 167ms/step - batch_loss: 0.1685\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 168ms/step - batch_loss: 0.1485\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 170ms/step - batch_loss: 0.1310\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 161ms/step - batch_loss: 0.1222\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 162ms/step - batch_loss: 0.1067\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 147ms/step - batch_loss: 0.0899\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 137ms/step - batch_loss: 0.0785\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 169ms/step - batch_loss: 0.0722\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 0s 146ms/step - batch_loss: 0.0645\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 0s 129ms/step - batch_loss: 0.0532\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 0s 180ms/step - batch_loss: 0.0574\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 0s 162ms/step - batch_loss: 0.0500\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 0s 170ms/step - batch_loss: 0.0431\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 0s 167ms/step - batch_loss: 0.0375\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 0s 165ms/step - batch_loss: 0.0353\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 0s 166ms/step - batch_loss: 0.0316\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 0s 163ms/step - batch_loss: 0.0288\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 0s 118ms/step - batch_loss: 0.0275\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 0s 124ms/step - batch_loss: 0.0247\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 0s 166ms/step - batch_loss: 0.0216\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 0s 169ms/step - batch_loss: 0.0207\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 0s 124ms/step - batch_loss: 0.0193\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 0s 128ms/step - batch_loss: 0.0173\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 0s 142ms/step - batch_loss: 0.0174\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 0s 154ms/step - batch_loss: 0.0157\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 0s 137ms/step - batch_loss: 0.0169\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 0s 161ms/step - batch_loss: 0.0149\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 0s 148ms/step - batch_loss: 0.0141\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 0s 159ms/step - batch_loss: 0.0130\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 0s 129ms/step - batch_loss: 0.0128\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 0s 156ms/step - batch_loss: 0.0125\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 0s 171ms/step - batch_loss: 0.0118\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 0s 163ms/step - batch_loss: 0.0113\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 0s 127ms/step - batch_loss: 0.0112\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 0s 166ms/step - batch_loss: 0.0112\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 0s 169ms/step - batch_loss: 0.0103\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 0s 127ms/step - batch_loss: 0.0101\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee406c4f90>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFCAm2xqAdx4",
        "outputId": "f6ad8440-15c7-465d-d5d9-35d5eee2e731"
      },
      "source": [
        "losses = []\n",
        "for test_input_batch, test_target_batch in test_dataset:\n",
        "  logs = train_translator._test_step([test_input_batch, test_target_batch])\n",
        "  losses.append(logs['batch_loss'].numpy())\n",
        "print(\"Test loss: \",losses)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss:  [0.0051593324]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15SEZQ5062pY"
      },
      "source": [
        "# Create a Translater"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pboiGcpcDPww"
      },
      "source": [
        "class Translator(tf.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('[START]'))\n",
        "    self.end_token = index_from_string(tf.constant('[END]'))\n",
        "\n",
        "  def tokens_to_text(self, result_tokens):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(result_tokens, ('batch', 't'))\n",
        "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "    shape_checker(result_text_tokens, ('batch', 't'))\n",
        "\n",
        "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                        axis=1, separator=' ')\n",
        "    shape_checker(result_text, ('batch'))\n",
        "\n",
        "    result_text = tf.strings.strip(result_text)\n",
        "    shape_checker(result_text, ('batch',))\n",
        "    return result_text\n",
        "\n",
        "  def sample(self, logits, temperature):\n",
        "    shape_checker = ShapeChecker()\n",
        "    # 't' is usually 1 here.\n",
        "    shape_checker(logits, ('batch', 't', 'vocab'))\n",
        "    shape_checker(self.token_mask, ('vocab',))\n",
        "\n",
        "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "    shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
        "\n",
        "    # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "    if temperature == 0.0:\n",
        "      new_tokens = tf.argmax(logits, axis=-1)\n",
        "    else: \n",
        "      logits = tf.squeeze(logits, axis=1)\n",
        "      new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                          num_samples=1)\n",
        "\n",
        "    shape_checker(new_tokens, ('batch', 't'))\n",
        "\n",
        "    return new_tokens\n",
        "\n",
        "  def translate_unrolled(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "    batch_size = tf.shape(input_text)[0]\n",
        "    input_tokens = self.input_text_processor(input_text)\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "    dec_state = enc_state\n",
        "    new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "    result_tokens = []\n",
        "    attention = []\n",
        "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "      dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                              enc_output=enc_output,\n",
        "                              mask=(input_tokens!=0))\n",
        "\n",
        "      dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "      attention.append(dec_result.attention_weights)\n",
        "\n",
        "      new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "      # If a sequence produces an `end_token`, set it `done`\n",
        "      done = done | (new_tokens == self.end_token)\n",
        "      # Once a sequence is done it only produces 0-padding.\n",
        "      new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "      # Collect the generated tokens\n",
        "      result_tokens.append(new_tokens)\n",
        "\n",
        "      if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "        break\n",
        "\n",
        "    # Convert the list of generates token ids to a list of strings.\n",
        "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "    result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "    if return_attention:\n",
        "      attention_stack = tf.concat(attention, axis=1)\n",
        "      return {'text': result_text, 'attention': attention_stack}\n",
        "    else:\n",
        "      return {'text': result_text}"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4UwhwiNDRU6"
      },
      "source": [
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5-Ln_ZcQDgI"
      },
      "source": [
        "tel1, tel2, tel3, tel4, tel5, eng1, eng2, eng3, eng4, eng5 = \"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n",
        "\n",
        "t1 = 'నా పేరు శ్రీనాథ్'\n",
        "for test_input_batch, test_target_batch in test_dataset:\n",
        "  tel1 = test_input_batch[0].numpy().decode()\n",
        "  eng1 = test_target_batch[0].numpy().decode()\n",
        "  tel2 = test_input_batch[1].numpy().decode()\n",
        "  eng2 = test_target_batch[1].numpy().decode()\n",
        "  tel3 = test_input_batch[6].numpy().decode()\n",
        "  eng3 = test_target_batch[6].numpy().decode()\n",
        "  tel4 = test_input_batch[8].numpy().decode()\n",
        "  eng4 = test_target_batch[8].numpy().decode()\n",
        "  tel5 = test_input_batch[13].numpy().decode()\n",
        "  eng5 = test_target_batch[13].numpy().decode()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcmMwMfkDbAR",
        "outputId": "3d6dc534-e717-4bd0-ad55-7d3e09f8f0a1"
      },
      "source": [
        "%%time\n",
        "input_text = tf.constant([tel1, tel2, tel3, tel4, tel5, t1])\n",
        "\n",
        "result = translator.translate_unrolled(\n",
        "    input_text = input_text)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 246 ms, sys: 778 µs, total: 247 ms\n",
            "Wall time: 240 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yq5nBjmU585"
      },
      "source": [
        "## Correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIRuKnDW9fsu",
        "outputId": "6b3fca6a-a6b5-4086-b51b-6c5fa4571ae2"
      },
      "source": [
        "print(tel1)\n",
        "print(result['text'][0].numpy().decode())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "నన్ను సహాయం అడగడానికి ఏం సందేహ పడొద్దు\n",
            "dont hesitate to ask me for help .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVkHgKum9mh-",
        "outputId": "7c712ffa-7ba9-41af-80fc-da849d11f77a"
      },
      "source": [
        "print(tel2)\n",
        "print(result['text'][1].numpy().decode())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "నేనూ తప్పులు చేస్తాను\n",
            "i do make mistakes .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL81cHuIUONC",
        "outputId": "ef9b1bac-ab0d-4b8e-8612-a8ee448105dc"
      },
      "source": [
        "print(tel3)\n",
        "print(result['text'][2].numpy().decode())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "అది నిజంగా అంత చెడ్డదా ?\n",
            "was it really that bad ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4auQIPsYVZJf"
      },
      "source": [
        "## InCorrect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANHafcabUnz1",
        "outputId": "7433f872-0321-4dd9-de1f-71fd18a3e42e"
      },
      "source": [
        "print(tel4)\n",
        "print(result['text'][3].numpy().decode())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "నేను కాఫీ తాగాను\n",
            "i drank coffee .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfgEBV13UoA6",
        "outputId": "5f3e218d-6a8f-4dff-f264-ec5ec0556c00"
      },
      "source": [
        "print(tel5)\n",
        "print(result['text'][4].numpy().decode())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "చలనచిత్రం మొదలు అవ్వబోతుంది\n",
            "the movies about to start .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57C_mtgkiy8V",
        "outputId": "aa26de16-10dc-4ff7-b6c4-d272d70d0108"
      },
      "source": [
        "print(t1)\n",
        "print(result['text'][5].numpy().decode())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "నా పేరు శ్రీనాథ్\n",
            "i dont speak japanese .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JM4SVkfDE1C"
      },
      "source": [
        "# Weights Analyse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwObt7u4EI0c"
      },
      "source": [
        "a = result['attention'][0]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "pYh7LdveEKAv",
        "outputId": "d40aa2b9-d89a-411d-a9ad-3c40ea3e99c1"
      },
      "source": [
        "_ = plt.bar(range(len(a[0, :])), a[0, :])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANeElEQVR4nO3df6zd9V3H8eeLljplDBJ7NaQtaxO7xWYxQm66GcwkMkzLltZEY9oEfyxk9Y+xsLBoOjVM8R/nkmlM6rQCjs0N7JgzjatW4zBTI9hbYGxt1+Vamb112juGTFy0om//uN/i4XJvz+nltN/bT5+P5Ibz/X4/OeedG/Lke7/fcw6pKiRJl74r+h5AkjQeBl2SGmHQJakRBl2SGmHQJakRK/t64dWrV9f69ev7enlJuiQdPnz461U1sdCx3oK+fv16pqam+np5SbokJfnqYse85CJJjTDoktQIgy5JjTDoktQIgy5JjRga9CQPJDmd5EuLHE+S30oyneTpJDeOf0xJ0jCjnKF/FNhyjuNbgY3dzy7gI69+LEnS+Roa9Kr6PPCNcyzZDnys5jwGXJvkunENKEkazTiuoa8BTg5sz3T7JEkX0UX9pGiSXcxdluH666+/mC8tLdn63Z/t9fWf+bW39/r6unSM4wz9FLBuYHttt+8VqmpvVU1W1eTExIJfRSBJWqJxBH0/8FPdu13eAjxfVV8bw/NKks7D0EsuSR4CbgZWJ5kBPgBcCVBVvwMcAG4DpoFvAe+8UMNKkhY3NOhVtXPI8QLePbaJJElL4idFJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRIwU9yZYkx5NMJ9m9wPHrkzya5MkkTye5bfyjSpLOZWjQk6wA9gBbgU3AziSb5i37JWBfVd0A7AB+e9yDSpLObZQz9M3AdFWdqKozwMPA9nlrCnhd9/ga4J/HN6IkaRSjBH0NcHJge6bbN+iXgduTzAAHgPcs9ERJdiWZSjI1Ozu7hHElSYsZ103RncBHq2otcBvw8SSveO6q2ltVk1U1OTExMaaXliTBaEE/Bawb2F7b7Rt0B7APoKr+DngNsHocA0qSRjNK0A8BG5NsSLKKuZue++et+SfgFoAk38tc0L2mIkkX0dCgV9WLwJ3AQeAYc+9mOZLk3iTbumXvA96V5AvAQ8DPVFVdqKElSa+0cpRFVXWAuZudg/vuGXh8FLhpvKNJks6HnxSVpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqxEhBT7IlyfEk00l2L7LmJ5IcTXIkySfHO6YkaZiVwxYkWQHsAW4FZoBDSfZX1dGBNRuB9wM3VdVzSb7rQg0sSVrYKGfom4HpqjpRVWeAh4Ht89a8C9hTVc8BVNXp8Y4pSRpmlKCvAU4ObM90+wa9AXhDkr9N8liSLQs9UZJdSaaSTM3Ozi5tYknSgsZ1U3QlsBG4GdgJ/F6Sa+cvqqq9VTVZVZMTExNjemlJEowW9FPAuoHttd2+QTPA/qr676r6R+ArzAVeknSRjBL0Q8DGJBuSrAJ2APvnrflj5s7OSbKauUswJ8Y4pyRpiKFBr6oXgTuBg8AxYF9VHUlyb5Jt3bKDwLNJjgKPAj9XVc9eqKElSa809G2LAFV1ADgwb989A48LuLv7kST1wE+KSlIjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjRgp6ki1JjieZTrL7HOt+LEklmRzfiJKkUQwNepIVwB5gK7AJ2Jlk0wLrrgbuAh4f95CSpOFGOUPfDExX1YmqOgM8DGxfYN2vAh8E/nOM80mSRjRK0NcAJwe2Z7p9L0lyI7Cuqj57ridKsivJVJKp2dnZ8x5WkrS4V31TNMkVwIeB9w1bW1V7q2qyqiYnJiZe7UtLkgaMEvRTwLqB7bXdvrOuBt4E/FWSZ4C3APu9MSpJF9coQT8EbEyyIckqYAew/+zBqnq+qlZX1fqqWg88BmyrqqkLMrEkaUFDg15VLwJ3AgeBY8C+qjqS5N4k2y70gJKk0awcZVFVHQAOzNt3zyJrb371Y0mSzpefFJWkRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWrESEFPsiXJ8STTSXYvcPzuJEeTPJ3kL5O8fvyjSpLOZWjQk6wA9gBbgU3AziSb5i17Episqu8DHgF+fdyDSpLObZQz9M3AdFWdqKozwMPA9sEFVfVoVX2r23wMWDveMSVJw4wS9DXAyYHtmW7fYu4A/nShA0l2JZlKMjU7Ozv6lJKkocZ6UzTJ7cAk8KGFjlfV3qqarKrJiYmJcb60JF32Vo6w5hSwbmB7bbfvZZK8DfhF4Ieq6r/GM54kaVSjnKEfAjYm2ZBkFbAD2D+4IMkNwO8C26rq9PjHlCQNMzToVfUicCdwEDgG7KuqI0nuTbKtW/Yh4LXAp5I8lWT/Ik8nSbpARrnkQlUdAA7M23fPwOO3jXkuSdJ58pOiktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktSIkYKeZEuS40mmk+xe4Pi3JfnD7vjjSdaPe1BJ0rkNDXqSFcAeYCuwCdiZZNO8ZXcAz1XV9wC/AXxw3INKks5tlDP0zcB0VZ2oqjPAw8D2eWu2Aw92jx8BbkmS8Y0pSRpm5Qhr1gAnB7ZngDcvtqaqXkzyPPCdwNcHFyXZBezqNl9IcnwpQ4/BaubNtow429I0O1su7N+7zf7eLrA+Z3v9YgdGCfrYVNVeYO/FfM2FJJmqqsm+51iIsy2Nsy2Nsy3Ncp1tlEsup4B1A9tru30LrkmyErgGeHYcA0qSRjNK0A8BG5NsSLIK2AHsn7dmP/DT3eMfBz5XVTW+MSVJwwy95NJdE78TOAisAB6oqiNJ7gWmqmo/cD/w8STTwDeYi/5y1vtln3NwtqVxtqVxtqVZlrPFE2lJaoOfFJWkRhh0SWrEZRX0YV9h0KckDyQ5neRLfc8yX5J1SR5NcjTJkSR39T3TWUlek+Tvk3yhm+1X+p5pUJIVSZ5M8id9zzJfkmeSfDHJU0mm+p5nUJJrkzyS5MtJjiX5gb5nAkjyxu73dfbnm0ne2/dcZ10219C7rzD4CnArcx+OOgTsrKqjvQ7WSfJW4AXgY1X1pr7nGZTkOuC6qnoiydXAYeBHl8PvrvtE8lVV9UKSK4G/Ae6qqsd6Hg2AJHcDk8Drquodfc8zKMkzwGRVLbsP7yR5EPjrqrqve3fdd1TVv/U916CuKaeAN1fVV/ueBy6vM/RRvsKgN1X1eebeIbTsVNXXquqJ7vG/A8eY+3Rw72rOC93mld3PsjhLSbIWeDtwX9+zXEqSXAO8lbl3z1FVZ5ZbzDu3AP+wXGIOl1fQF/oKg2URpUtJ902aNwCP9zvJ/+suazwFnAb+oqqWy2y/Cfw88L99D7KIAv48yeHuazmWiw3ALPD73eWq+5Jc1fdQC9gBPNT3EIMup6DrVUryWuDTwHur6pt9z3NWVf1PVX0/c59i3pyk90tWSd4BnK6qw33Pcg4/WFU3MvdNqu/uLvstByuBG4GPVNUNwH8Ay+2e1ypgG/CpvmcZdDkFfZSvMNAiuuvTnwY+UVV/1Pc8C+n+LH8U2NL3LMBNwLbuOvXDwA8n+YN+R3q5qjrV/fM08BnmLksuBzPAzMBfWo8wF/jlZCvwRFX9a9+DDLqcgj7KVxhoAd2Nx/uBY1X14b7nGZRkIsm13eNvZ+6m95f7nQqq6v1Vtbaq1jP379rnqur2nsd6SZKruhvcdJczfgRYFu+wqqp/AU4meWO36xag9xvw8+xkmV1ugYv8bYt9WuwrDHoe6yVJHgJuBlYnmQE+UFX39zvVS24CfhL4YnetGuAXqupAjzOddR3wYPeOgyuAfVW17N4iuAx9N/CZ7n9bsBL4ZFX9Wb8jvcx7gE90J18ngHf2PM9Luv8A3gr8bN+zzHfZvG1Rklp3OV1ykaSmGXRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RG/B87iFNbXLbL6gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1YUpD10ERuj"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  sentence = tf_lower_and_split_punct(sentence).numpy().decode().split()\n",
        "  predicted_sentence = predicted_sentence.numpy().decode().split() + ['[END]']\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  attention = attention[:len(predicted_sentence), :len(sentence)]\n",
        "\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0, vmax=1)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')\n",
        "  plt.suptitle('Attention weights')"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pfcb0ZEfES7i",
        "outputId": "e2362433-ac52-4fc1-b463-92274bed4947"
      },
      "source": [
        "i=3\n",
        "plot_attention(result['attention'][i], input_text[i], result['text'][i])\n",
        "print(input_text[i].numpy().decode())"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "నేను కాఫీ తాగాను\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3112 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3143 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3137 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3093 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3134 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3115 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3136 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3108 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3095 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3112 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3143 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3137 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3093 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3134 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3115 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3136 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3108 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3095 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1lBga5tbDy4"
      },
      "source": [
        "# Questions & Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJlNcedYbJFi"
      },
      "source": [
        "Q1-)Which parts of the sentence are used as a token? Each character, each word, or are some words split up?\n",
        "\n",
        "Ans-) Each word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSFabVhgb6NA"
      },
      "source": [
        "Q-2) Do the same tokens in different language have the same ID?\n",
        "e.g. Would the same token index map to the German word die and to the English word die?\n",
        "\n",
        "\n",
        "Ans-) No. \n",
        "\n",
        "As example we can see below:\n",
        "\n",
        "*  inp = 'die hallo morgen guten tag'\n",
        "*  tar = 'Someone might die because of the situation'\n",
        "*  inp_voc = [0 1 2 3 4 5]\n",
        "*  tar_voc = [0 1 2 3 4 5 6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43vetYtVdNt0"
      },
      "source": [
        "Q-3)What is the relation between the encoder output and the encoder hidden state which is used to initialize the decoder hidden state (for the architecture used in the tutorial)?\n",
        "\n",
        "Ans-) Encoder ouput is dependent on encoder hidden state whereas vice-versa isn't the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtXk0eDdfIgC"
      },
      "source": [
        "Q-4) Is the decoder attending to all previous positions, including the previous decoder predictions?\n",
        "\n",
        "Ans-) No it will attend to just the previous decoder state and predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiRXSZ3zfxpc"
      },
      "source": [
        "Q-5) Does the encoder output change in different decoding steps?\n",
        "\n",
        "Ans-) Yes it computes attention weights dynamically for every decoder step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgrMHrqvgNxN"
      },
      "source": [
        "Q-6) Does the context vector change in different decoding steps?\n",
        "\n",
        "Ans-) Yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "molLOQfNgStB"
      },
      "source": [
        "Q-7) The decoder uses teacher forcing. Does this mean the time steps can be computed in parallel?\n",
        "\n",
        "Ans-) No because even the previous decoder state is also connected to the current decoder state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnFce_9CgeTR"
      },
      "source": [
        "Q-8) Why is a mask applied to the loss function?\n",
        "\n",
        "Ans-) To skip the zero padded cells in the sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5KlAkttgwos"
      },
      "source": [
        "Q-9) When translating the same sentence multiple times, do you get the same result? Why (not)? If not, what changes need to be made to get the same result each time?\n",
        "\n",
        "Ans-) The output is not consistent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGd5aFjjm4zZ"
      },
      "source": [
        "# References\n",
        "\n",
        "\n",
        "\n",
        "1.   https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "2.   https://ovgu-ailab.github.io/idl2021/ass7.html\n",
        "\n"
      ]
    }
  ]
}