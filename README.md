# Deep_Learning Programming Tasks (OVGU)

## OVGU Class URL - [https://ovgu-ailab.github.io/idl2021/](https://ovgu-ailab.github.io/idl2021/)


### Contributors (Assignment Team)
* **Venkata Srinath Mannam**
* **Meghana Rao**
* **Govind Shukla**

### Details
* Completing the tasks are mandatory as part of the course completion.
* 80% of the programs should be solved and submitted in order to participate the course exam (Furthe details can be found in the above mentioned url).
* All the code is developed in [google colab](https://colab.research.google.com/).
* All the tasks are written in Python with [TensorFlow](https://www.tensorflow.org/), [Keras](https://keras.io/) framework.
* Each assignment is there in the respected folder

## Assignments

### Assignment 1 - Multi-Layered-Perceptrons(MLP)
#### Submission Details
* Building A Deep Model
* Hand in working code that defines, trains and evaluates an MLP on MNIST!
* Hand in any additional experiments you tried!
* Further details related to the task can be found in the below task URL.
* Task URL : [https://ovgu-ailab.github.io/idl2021/ass1.html](https://ovgu-ailab.github.io/idl2021/ass1.html)

### Assignment 2 - MLP with TensorBoard
#### Submission Details
* An MLP training script using tf.data
* A description of what the six shuffle/batch/repeat orderings do (on a conceptual level) and which one you think is the most sensible for training neural networks.
* For each “failed” script above, a description of the problem as well as how to fix it (there may be multiple ways). You can just write some text here (markdown cells!), but feel free to reinforce your ideas with some code snippets.
* Further details related to the task can be found in the below task URL.
* Task URL : [https://ovgu-ailab.github.io/idl2021/ass2.html](https://ovgu-ailab.github.io/idl2021/ass2.html)

### Assignment 3 - Convolutional-Neural-Nets(CNN's)
#### Submission Details
* A CNN (built with Keras) trained on MNIST (or not, see below). Also use Keras losses, optimizers and metrics, but do still use a “custom” training loop (with GradientTape).
* You are highly encouraged to move past MNIST at this point. E.g. switching to CIFAR takes minimal effort since it can also be downloaded through Keras. You can still use MNIST as a “sanity check” that your model is working, but you can skip it for the submission.
* Really do play with the model parameters. As a silly example, you could try increasing your filter sizes up to the input image size – think about what kind of network you are ending up with if you do this! On the other extreme, what about 1x1 filters?
* Further details related to the task can be found in the below task URL.
* Task URL : [https://ovgu-ailab.github.io/idl2021/ass3.html](https://ovgu-ailab.github.io/idl2021/ass3.html)

### Assignment 4 - DenseNets, Inception, ResNets
#### Submission Details
* DenseNet. Thoroughly experiment with (hyper)parameters. Try to achieve the best performance you can on CIFAR10/100.
* For your model(s), compare performance with and without tf.function. You can also do this for non-DenseNet models. How does the impact depend on the size of the models?
* High-level Training Loops with Keras.
* Further details related to the task can be found in the below task URL.
* Task URL : [https://ovgu-ailab.github.io/idl2021/ass4.html](https://ovgu-ailab.github.io/idl2021/ass4.html)


